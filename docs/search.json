[
  {
    "objectID": "assets/body-header.html",
    "href": "assets/body-header.html",
    "title": "Andrew Zieffler",
    "section": "",
    "text": "./assets/body-header.qmd\n\n\n\n\n\n\nAndrew Zieffler\nAcademic. Data lover. Statistics enthusiast."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Academic. Data lover. Statistics enthusiast.\nSLEDS Lab is a research lab in the Department of Educational Psychology at the University of Minnesota. It sprung from the Social Lab, started by Geoff Maruyama."
  },
  {
    "objectID": "about.html#members",
    "href": "about.html#members",
    "title": "About",
    "section": "Members",
    "text": "Members"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andrew Zieffler",
    "section": "",
    "text": "Andrew Zieffler\nAcademic. Data lover. Statistics enthusiast.\n\n\n\n\n\n\n\n\n\n\nAbout Me\nI am a Professor of Teaching and researcher in the Quantitative Methods in Education program within the Department of Educational Psychology at the University of Minnesota. I currently teach undergraduate and graduate-level courses in statistics and train and supervise graduate students in statistics education. In a previous life I taught mathematics and A.P. Statistics at ROCORI High School in Cold Spring, MN.\nMy scholarship addresses statistics education writ large, and has recently been focused on teacher education and also on how data science is transforming the statistics curriculum. You can find out more about some of this work at the LaSER website. I also work on curriculum design projects.\nI have authored/co-authored several papers and book chapters related to statistics education, and have been a co-PI on many NSF-funded statistics education research projects. Additionally, I have co-authored two textbooks that serve as an introduction to modern statistical and computational methods for students in the educational and behavioral sciences.\nI have served as an AE for the Journal of Statistics Education and co-editor of the journal Technology Innovations in Statistics Education. In 2018, I was part of a study group formed by the National Academies of Sciences, Engineering, and Medicine that authored the report, Data Science for Undergraduates: Opportunities and Options.\n\n\n\nEducation\n\n\nPhD, Educational Psychology, Quantitative Methods in Education, 2006\n\nUniversity of Minnesota\n\n\n\nBS, Mathematics Education, 1998\n\nSt. Cloud State University"
  },
  {
    "objectID": "posts/2018-03-30-epsy-5271-part-02/2018-03-30-epsy-5271-part-02.html",
    "href": "posts/2018-03-30-epsy-5271-part-02/2018-03-30-epsy-5271-part-02.html",
    "title": "Q&A with Becoming a Teacher of Statistics Class: Part II",
    "section": "",
    "text": "./assets/body-header.qmd\nThis post is the second in a series of blogposts in which I respond to questions from the students in the Becoming a Teacher of Statistics course. In today’s posting I respond to questions related to several of the projects I have worked on over the years.\n\nCan you tell us a little about your AIMS project? I think it’s not always easy for everyone to apply guidelines in order to change their behavior (in any context), but your AIMS project seems to fill that gap for stats educators-that is, it explicitly creates educational materials aligned with GAISE. Can you also tell us about some of the difficult decision points in developing your materials?\nWow. You are making me think. I honestly haven’t thought about AIMS in a long time (I think we started writing these materials in 2005ish). At that time, the GAISE recommendations had just come out. Joan Garfield, my advisor and colleague, was the chair of the committee that wrote the GAISE college report. She was also the person that had transformed the curriculum in our undergraduate course (EPsy 3264) to what it was at that time.\nThere were two graduate students, myself and Sharon Lane-Getaz, who were teaching 3264 at that time. We actually would write the curriculum as we went, writing a lesson, teaching it and observing the other teach, and then meeting afterward in Joan’s office to re-vise the lesson and write the next one. It was a very invloved process, and, in those early iterations, a little rough.\nOne of the major decisions was to choose what topics would be included in the course and which wouldn’t. We also really switched the focus from calculation to conceptual understanding. The choice of software was also a decision point. Bill Finzer was one of the project’s advsiors because we were pretty convinced that Fathom was the software we wanted to use in the course. (Although, looking back on the actual lessons, I think the materials are pretty software agnostic.)\n\n\nHow much biology did you have to know to contribute meaningfully to the development of BioSQuaRE? Or, are you just an expert in assessment creation who applied his expertise to biology?\nThankfully, not much. My high school and undergraduate experiences in biology were only slightly better than those in chemistry (although I did really enjoy an environmental biology course I took). I primarily brought assessment development expertise to the project. The biology expertise (from others on the project) was primarily in ensuring that the items had a context related to biology, as the assessment itself was geared toward measuring students’ quantitative skills and knowledge.\n\n\nWhat’s next for the CATALST project? Is there hope to expand it to other universities or high schools outside Minnesota?\nGood question. In thinking about the current CATALST curriculum, we do not have explicit plans to expand to other institutions, but if there is a grass roots expansion, the more the merrier. My sense is that it would be a pretty good curriculum for high schools, but that many high school teachers are not aware of the its existence. (Perhaps we should do a workshop aimed at high school teachers at USCOTS next year…)\nOne way that we have thought about “growing” the curriculum is to write a Lab Manual for use with R. Jim Albert taught CATALST with R to his students at Bowling Green when we first started the project many moons ago. I think this could be used as a skeleton for writing such a Lab Manual, but updating the syntax to use mosaic and tidyverse functions. The one reason we have not done this yet is that I am unsure that R is the correct choice of software in an introductory statistics course for undergraduates. Inference is hard to learn. Inference and coding may be too much at that level.\nLately, there have been a few of us (loosely) thinking about what a CATALST-like course that emphasizes data science might look like. In the early days of CATALST, we had thought about more forward-thinking units such as prediction/classification, and visualization. These were ultimately scrapped, but in today’s climate might be good candidates for a data science-driven curriculum. We would keep the pedagogical approach used in the CATALST course (cooperative learning, teacher as guide-on-the-side, discussion) and update the content."
  },
  {
    "objectID": "posts/2020-07-21-spelling-lesson/2020-07-21-spelling-lesson.html",
    "href": "posts/2020-07-21-spelling-lesson/2020-07-21-spelling-lesson.html",
    "title": "Spelling Lesson",
    "section": "",
    "text": "./assets/body-header.qmdCame across this funny ditty by Ralph P. Boas in the June 1984 issue of The College Mathematics Journal. The complete citation is: Boas, R. P. (1984). Spelling lesson. The College Mathematics Journal, 15(3), 217.\nSpelling Lesson \nWeep for the mathematicians   Posterity acclaims: Although we know their theorems   We cannot spell their names.\nForget the rules you thought you knew Henri Lebesgue has got no Q.\nAlthough it almost rhymes with Birkhoff, Two H’s grace the name of Kirchhoff.\nThe Schwarz of inequality And lemma too, he has no T.\nThe “distribution” Schwartz, you see Is French, and so he has a T.\nIn Turing’s name—no German, he— An umlaut we should never see.\nHermann Grassmann—please try to Spell both his names with 2 N’s, too.\nIf you should ever have to quote A Harvard Peirce, be sure to note He has the E before the I; And so does Klein. Rules still apply To Wiener: I precedes the E; The same for Riemann, as you see. But Weierstrass, the lucky guy, Has it both ways, with EIE.\nFejer, Turan, Cesaro, Frechet— Let’s make the accents go that way,\nAnd as for Radon-Nikodým, Restore the accent, that’s my dream.\n“Stokes’s theorem” is O.K. “Stokes’ theorem” is an awkward way. “Stoke’s theorem” seems to be a joke, Or did you think his name was “Stoke?”\nBut there is one I leave to you, Whatever you may choose to do: Put letters in or leave them out, Garnish with accents round about, Finish the name with eff or off: There is no way to spell Чебышëв\n\n\n\nThe poem was accompanied by this lovely illustration. No image credit was given; perhaps it was also Boas."
  },
  {
    "objectID": "posts/2020-02-20-some-plots-i-made/2020-02-20-some-plots-i-made.html",
    "href": "posts/2020-02-20-some-plots-i-made/2020-02-20-some-plots-i-made.html",
    "title": "Some Plots I Made…",
    "section": "",
    "text": "./assets/body-header.qmdI am spring cleaning my computer and have a couple PNG files of plots I made that I want to adios. So, I thought to myself, “self…why don’t you put these on a blog post before you trash them forever.” It sounded like a good idea at the time, so here they are.\n\nThis second plot is a sunburst plot I created for the data visualization course. If I remember right, this was created using my annual TV watching data from 2017 and made using RawGraphs.\n\nHere is a plot from a set of plots I created years ago to show whether NFL players were being traded to teams within the same conference or elsewhere.\n\nThis is a micromap I created showing tuition changes over a three-year span. This was inspired by content in the book Visualizing Data Patterns with Micromaps, which I read a few years ago. I wanted to try some of what they were doing, so I ultimately created each map using R and then put them all together onto a single graphic with Keynote. Probably could do this now with Patchwork.\n\nFinally, here is a residual plot I created for a set of regression notes where we were talking about model assumptions right near Halloween."
  },
  {
    "objectID": "posts/2019-12-09-teaching-statistics-reading-discussion-group/2019-12-09-teaching-statistics-reading-discussion-group.html",
    "href": "posts/2019-12-09-teaching-statistics-reading-discussion-group/2019-12-09-teaching-statistics-reading-discussion-group.html",
    "title": "Teaching Statistics Reading/Discussion Group",
    "section": "",
    "text": "./assets/body-header.qmdLaura Le is organizing a reading/discussion group at the University of Minnesota in spring 2020 for anyone interested in teaching statistics. The group will meet Mondays from 4:00pm–5:00pm (location TBD). Here is the tentative plan for what this interest group will entail:\n\nPrior to the meeting, read/skim one article related to the topic of the week. During the meeting, discuss the topic/article for the first 30 minutes and video chat with a prominent statistics educator on that topic for the last 30 minutes.\n\nFeel free to come to some, or all, of the meetings. Email Laura (free0312@umn.edu) for more information."
  },
  {
    "objectID": "posts/2022-11-21-graduation-2022/index.html",
    "href": "posts/2022-11-21-graduation-2022/index.html",
    "title": "Graduation 2022 and New Students",
    "section": "",
    "text": "./assets/body-header.qmdHere are a couple of pictures from the University of Minnesota’s 2022 commencement. Two of our statistics education students walked and were hooded: Chelsey Legacy and Jonathan Brown!\n\n\n\nFrom Left: Mireya Smith, Yoo Jeong Jang, Chelsey Legacy, and Jonathan Brown.\n\n\nStatistics Education also had a good presence at the 2022 Student Orientation. Regina Lisinker, a second year MA student, and Pablo Vivas Corrales, a first year MA student from Costa Rica, were on hand to welcome in the new academic year. In addition, we also welcomed new QME faculty including Chelsey Legacy, who started as an Assistant Professor of Teaching in the QME program.\n\n\n\nQME students and faculty at the 2022 Fall Department of Educational Psychology new student orientation."
  },
  {
    "objectID": "posts/2018-08-27-inbox-zero/2018-08-27-inbox-zero.html",
    "href": "posts/2018-08-27-inbox-zero/2018-08-27-inbox-zero.html",
    "title": "Inbox Zero",
    "section": "",
    "text": "./assets/body-header.qmdOn Saturday August 25, 2018 at 8:08 PM I finally hit Inbox Zero!\n\nI did it by immediately copying the snippets of email I wanted into Evernote notes (my notetaking system). I also attended to to-dos more immediately, or added them to a note of “To-Dos”.\nI doubt I will stay at zero emails in my inbox, but I have been at fewer than 10 emails all summer. Hoping to keep things at this level as the semester gears up."
  },
  {
    "objectID": "posts/2021-08-31-books/index.html",
    "href": "posts/2021-08-31-books/index.html",
    "title": "Books I Bought",
    "section": "",
    "text": "./assets/body-header.qmd\nI went to the Arc today and bought several books. (It’s hard to beat a price tag of 2.99!) So, in the spirit of Nick Hornby’s column in the Guardian, I thought I would chronicle these books along with the reason I bought each.\n\nBOOKS BOUGHT:\nBruce—Peter Ames Carlin: A biography of Bruce Springsteen, bought because he is The Boss.\nGoing for a Beer and A Night at the Movies or, You Must Remember This—Robert Coover: I got into Coover’s work after reading a book on writing in which he was featured in a chapter. He has written some really great short stories and he also was a founder of the Electronic Literature Organization in 1999 which promotedthe publication of literature in the “new” electronic format.\nThe Black Death—Philip Ziegler: A book on the plague that swept through Europe in the mid 1300s wiping out one-third of the English population in three years. I have several books on pandemics; from Ebola to the flu. Maybe I will one day want to read about COVID as well.\nRoads Less Traveled: Dispatches from the Ends of the Earth—Catherine Watson: Watson was the first travel editor for the Minneapolis Star Tribune (so the back cover of the book tells me). This book of travel essays I bought on a lark and am hoping her perspective as a native Minnesotan will resonate.\nEnduring Love—Ian McEwan: I have most of McEwan’s oevre and have loved all of them.\nThe Keeper of Lost Causes—Jussi Adler-Olsen: This is a detective story that I picked up because I liked the title. Plus there is a bird cage on the cover. Who doesn’t want to read a book that has a bird cage on the cover. The front cover also tells me that the author won the Glass Key Award, so I am optimistic.\nOjibwe Waasa Inaabida: We Look in All Directions—Thomas Peacock and Marlene Wisuri: I have a large collection of books on the Native Americans of the plains and especially of Minnesota. Fun fact: When I was a youngster and we were asked what we wanted to be when we grew up, while most of my classmates said astronaut or baseball player, my answer was Crazy Horse.\nA Man Called Ove—Fredrik Backman: This has been on my Goodreads “to-read” list for awhile. My sister tells me it is great!\nThe Trial of Henry Kissinger—Christopher Hitchens: Hitchens is another author I read a lot of. He was a Republican columnist that I truly respected and a brilliant debater. His feud with Gore Vidal is legendary and the shade that they dished was top-notch. I miss really, really smart people arguing the merits of a thing rather than just trying to out-shout each other.\nLeonard Cohen: Hallelujah—Tim Footman: A biography of the late, great Leonard Cohen. Poet. Song writer. And a voice that gives you shivers.\nThe Art Spirit—Robert Henri: This is the writings that lay out the philosophy espoused by Henri on whta it means to be an artist. I feel a kindred spirit to artists, so this sounded good to me.\nStaying the Course: A Runner’s Toughest Race—Dick Beardsley and Maureen Anderson: In the 1982 Boston Marathon, Beardsley lost by 2 seconds to Alberto Salzar. This book tells that story and the aftermath of Beardsley’s life (farm accident, almost overdosing, addiction) from his point of view. I read Duel in the Sun a year or so ago (about this race) and loved every second. I hope this will be as good.\nJohn Dillinger Slept Here: A Crook’s Tour of Crime and Corruption in St. Paul, 1920-1936—Paul Maccabee: I also have several books on crime in Minnesota. Someday I might put together a map that pins locations of interest by culling the info in these books….in all my infinite spare time."
  },
  {
    "objectID": "posts/2019-05-19-uscots-2019/2019-05-19-uscots-2019.html",
    "href": "posts/2019-05-19-uscots-2019/2019-05-19-uscots-2019.html",
    "title": "USCOTS 2019",
    "section": "",
    "text": "./assets/body-header.qmdLast week I attended the United States Conference on Teaching Statistics. The biennial conference, which took place at Penn State, attracts statistics educators and statistics education researchers from across the world. It was a fantastic conference with keynotes from Jane Watson, Allen Schirm and Ron Wasserstein, John Kruschke, and Kari Lock Morgan.\nI cajoled four of my graduate students (Jonathan Brown, Mike Huberty, Chelsey Legacy, and Vimal Rao) to tag along, and it was fun to see them interacting with the people and ideas presented. Our alumni from the statistics education program were also well represented with posters and breakout sessions led by myself, Laura Le, Laura Ziegler, and Matt Beckman.\n\n\n\nJane Watson presents a keynote of her work with Austrailian school-level students.\n\n\n\n\n\nJonathan Brown presents a poster on his work on understanding how students approach simulation-based inferential problems.\n\n\n\n\n\nLaura Le talks to Douglas Whittaker about her work teaching biostatistics students in a literacy-based course.\n\n\n\n\n\nVimal Rao talks to a whole host of folks including Iddo Gal and Rob Gould about some of the work he did with George Washington students."
  },
  {
    "objectID": "posts/2019-04-26-slhs-prosem/2019-04-26-slhs-prosem.html",
    "href": "posts/2019-04-26-slhs-prosem/2019-04-26-slhs-prosem.html",
    "title": "Deprecating Statistical Significance: Toward Better Science",
    "section": "",
    "text": "./assets/body-header.qmdOn Friday April 26, 2019 I gave an invited talk to the Department of Speech-Language-Hearing Sciences in their Pro-Sem series. These are really neat as they are organized by the graduate students. This is the third one of these I have given over the years and each is a treat.\nIn this talk I spoke about the ASA’s recent call for the deprecation of “statistical significance” and all its related variants (e.g., “significantly different,” “p &lt; 0.05”, and “nonsignificant”, asterisks in a table). Citing articles in the recent volume of The American Statistician and drawing on the historical conflagration that is NHST I laid out why scientists and statisticians have an issue with “statistical significance” and also proposed some ideas for moving forward based on those proposed by Ronald L. Wasserstein, Allen L. Schirm & Nicole A. Lazar.\nThe slides for the talk are available here."
  },
  {
    "objectID": "posts/2019-05-30-change-time-and-effort/2019-05-30-change-time-and-effort.html",
    "href": "posts/2019-05-30-change-time-and-effort/2019-05-30-change-time-and-effort.html",
    "title": "Change: Time and Effort",
    "section": "",
    "text": "./assets/body-header.qmdIn the fall of 2008 (maybe 2007; my memory is fleeting) our department moved from Burton Hall to the, at the time, newly renovated Education Sciences Building. This building is beautiful from the outside; brick, overlooking the Mississippi River. The building was designed in such a way that (at least on my floor) there are two long parallel hallways with the faculty offices on the outside of these hallways (running the exterior wall of the building) and the interior filled with lab space (small offices) for research grants. Other than the claustrophobic closed-in linear feeling of this, the setup isn’t that terrible.\nThe space designed for students, however left a lot to be desired. The student space was clustered into two different open areas on the floor and was filled with study carrels, the thought being that each student who wasn’t placed in a research lab needed a designated carrel. The reality was that students didn’t use these at all and quit showing up to the building.\nOne of the faculty’s biggest laments is that students are never around. Perhaps it is the experiences we had as graduate students, but many of us believe that having community with other students and faculty alike is a large part of growing as a scholar. Of course the internet and online accessibility to resources has changed many aspects of human life, including graduate school. But, the choice to be around the building more often, let alone to work cooperatively (another of the faculty’s goals) is often less palettable when students are accustomed to the large tables and open areas associated with coffee shops and other study areas around campus.\nI couldn’t blame the students for not showing up. Around 2009 I started inquiring (along with some others) about whether we could change things. This was the “way it was” we were told and we couldn’t change things. As the years went on, I kept asking, even offering to dismantle all of the carrels myself, haul them to the university storage facility and bring in chairs, couches and tables. The answer was consistently “no”.\nLast year, after organizing some students and other faculty we finally got a “maybe”, which turned into “yes”. This spring the carrels came down and were replaced with large tables. We also put in several whiteboards. Although we didn’t get our couch, this is an improvement. The pictures below show the new space.\n\n\n\nEdSciB Student Space\n\n\n\n\nSince this space was renovated we have been seeing many more students working in the building. One of my colleagues sent out the following email:\n\nHi everyone,\nI have been working late pretty consistently over the past 2 weeks and have good news to report: the new student space on the 1st floor is a huge hit!\nEvery evening, I have seen clusters of students at the tables working on class projects, reading papers, talking about research, and eating and drinking and socializing together. I could not be happier.\n(It strongly reminds me of the camaraderie of my graduate school days, before the rise of cell phones and laptops and wireless networks emptied university labs and filled coffeeshops with people working alone.)\nLast night, for example, I drifted down around 6 PM and saw Carlos, Rina, and Ashley working on R programming (perhaps for 8251), Tayler at a separate table coding data, and Jesslyn eagerly preparing to analyze the post-test data from her first-year project. I checked in 2 hours later, as I was leaving. Jesslyn and Ashley had been replaced by Ozge and Tai. Who knows how late they all worked.\n\nI write this post to remind others (and myself) that change at a university takes time. A lot of time. It also takes a person to take the lead and keep asking. This took us almost 10 years to get changed. It was 8–9 years of asking and hearing “no” before we got a maybe. Many initiatives take a lot of time and one dedicated person who gets others excited as well. Keep asking; keep working; today’s “no” is tomorrow’s “yes”. Now if only we could get a couch and a cappuccino maker!"
  },
  {
    "objectID": "posts/2022-04-27-a-couple-interesting-reads/index.html",
    "href": "posts/2022-04-27-a-couple-interesting-reads/index.html",
    "title": "A Couple Interesting Reads",
    "section": "",
    "text": "./assets/body-header.qmd\nHere are a couple of things that I have read that I wanted to share.\n\nMath Phobia: An American Crisis\nThe first was a short piece in the Harvard Business Review called Americans Need to Get Over Their Fear of Math. The author, Sian Beilock, paints the thesis that math phobia is a liability for the U.S. given the importance of STEM skills and jobs to the current marketplace. Here is the quote that stood out to me:\n\nFinally, we need to reject the social acceptability of being bad at math. Think about it: You don’t hear highly intelligent people proclaiming that they can’t read, but you do hear many of these same individuals talking about “not being a math person.”\n\nHear, hear!\n\n\nStructured Procrastination\nIn the second piece called Structured Procrastination, John Perry, argues that procrastinators can do better prioritizing important projects. He argues that while procrastinators put off things they have to do, it does not mean doing absolutely nothing. There are some gems in here:\n\nProcrastinators often follow exactly the wrong tack. They try to minimize their commitments, assuming that if they have only a few things to do, they will quit procrastinating and get them done. But this goes contrary to the basic nature of the procrastinator and destroys his most important source of motivation. The few tasks on his list will be by definition the most important, and the only way to avoid doing them will be to do nothing. This is a way to become a couch potato, not an effective human being.\n\nAlso,\n\nTasks that seem most urgent and important are on top. But there are also worthwhile tasks to perform lower down on the list. Doing these tasks becomes a way of not doing the things higher up on the list…The trick is to pick the right sorts of projects for the top of the list. The ideal sorts of things have two characteristics, First, they seem to have clear deadlines (but really don’t). Second, they seem awfully important (but really aren’t). Luckily, life abounds with such tasks. In universities the vast majority of tasks fall into this category, and I’m sure the same is true for most other large institutions.\n\n\n\nThe Devil Teaches Thermodynamics\nThe last piece is a short poem that was published in Roald Hoffman’s book, [Chemistry Imagined: Reflections on Science](http://www.amazon.com/exec/obidos/ASIN/1560982144/braipick-20. I saw this on The Marginalian. Hoffman is a Nobel Prize winning chemist who is also a “literary artist”. You can watch Sean Ono Lennon recite the poem, which he read at the second annual Universe in Verse celebration."
  },
  {
    "objectID": "posts/2020-01-18-apa-tables-using-rmarkdown-part-5/2020-01-18-apa-tables-using-rmarkdown-part-5.html",
    "href": "posts/2020-01-18-apa-tables-using-rmarkdown-part-5/2020-01-18-apa-tables-using-rmarkdown-part-5.html",
    "title": "APA Tables using RMarkdown: Part 5",
    "section": "",
    "text": "./assets/body-header.qmd\nThis is the fifth part of a short blog series I am writing to re-create some of the sample tables found in the 7th edition APA Publication Manual. In this post I will attempt to show how to create a table to present the results of several fitted models. (This type of table is not among the example tables in the 7th edition APA Publication Manual.) To do so, I will incorporate many ideas that I covered in the first, second, third and fourth of these posts.\n\nMy Process\nI will again render to PDF and set up the YAML to import the caption package (LaTeX) and set up the APA caption formatting.\n\n---\ntitle: \"Untitled\"\nauthor: \"Andrew Zieffler\"\ndate: \"1/12/2020\"\noutput: pdf_document\nheader-includes:\n   - \\usepackage{caption}\n   - \\captionsetup[table]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=newline}\n---\n\nIn the RMarkdown body, I will load a few packages and also import a data set that I will use to fit the models.\n\n# Load libraries\nlibrary(broom)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(tidyverse)\n\n# Import data\nmn = read_csv(\"https://raw.githubusercontent.com/zief0002/rustic-penguin/main/data/mn-schools.csv\") |&gt;\n  mutate(public = if_else(sector == \"Public\", 1, 0))\n\n# View data\nhead(mn)\n\n# A tibble: 6 × 6\n  name                               grad sector    sat tuition public\n  &lt;chr&gt;                             &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 Augsburg College                   65.2 Private  1030    39.3      0\n2 Bethany Lutheran College           52.6 Private  1065    30.5      0\n3 Bethel University, Saint Paul, MN  73.3 Private  1145    39.4      0\n4 Carleton College                   92.6 Private  1400    54.3      0\n5 College of Saint Benedict          81.1 Private  1185    43.2      0\n6 Concordia College at Moorhead      69.4 Private  1145    36.6      0\n\n\nThe data in were collected from http://www.collegeresults.org and contain 2011 institutional data for n=33 Minnesota colleges and universities. The codebook is available here.\nWe need to compute the regression results from fitting an OLS model. Here I will predict variation in graduation rates using several potential competing models.\n\n# Fit models\nlm.1 = lm(grad ~ 1 + public + sat + tuition, data = mn)\nlm.2 = lm(grad ~ 1 + public + sat + tuition + public:sat, data = mn)\nlm.3 = lm(grad ~ 1 + public + sat + tuition + public:tuition, data = mn)\n\nThere are several packages that can be used to present the results of one or more fitted models. These include stargazer, texreg, and modelsummary among others. In this post I will focus on using the stargazer package, which I have found works well for fixed-effects models.\nAfter loading the stagazer package, the stargazer() function can be used to create a basic table of regression results. The type= argument defaults to latex, so if you are rendering to an HTML document, you need to change this to type=\"html\".\n\n# Load library\nlibrary(stargazer)\n\n# Output model results in a table\nstargazer(lm.1, lm.2, lm.3, type = \"latex\")\n\n\n% Table created by stargazer v.5.2.3 by Marek Hlavac, Social Policy Institute. E-mail: marek.hlavac at gmail.com\n% Date and time: Tue, Oct 31, 2023 - 18:00:09\n\\begin{table}[!htbp] \\centering \n  \\caption{} \n  \\label{} \n\\begin{tabular}{@{\\extracolsep{5pt}}lccc} \n\\\\[-1.8ex]\\hline \n\\hline \\\\[-1.8ex] \n & \\multicolumn{3}{c}{\\textit{Dependent variable:}} \\\\ \n\\cline{2-4} \n\\\\[-1.8ex] & \\multicolumn{3}{c}{grad} \\\\ \n\\\\[-1.8ex] & (1) & (2) & (3)\\\\ \n\\hline \\\\[-1.8ex] \n public & $-$0.647 & 9.132 & 8.554 \\\\ \n  & (4.716) & (32.466) & (19.945) \\\\ \n  & & & \\\\ \n sat & 0.104$^{***}$ & 0.106$^{***}$ & 0.106$^{***}$ \\\\ \n  & (0.016) & (0.018) & (0.017) \\\\ \n  & & & \\\\ \n tuition & 0.470$^{*}$ & 0.451$^{*}$ & 0.455$^{*}$ \\\\ \n  & (0.242) & (0.253) & (0.247) \\\\ \n  & & & \\\\ \n public:sat &  & $-$0.009 &  \\\\ \n  &  & (0.031) &  \\\\ \n  & & & \\\\ \n public:tuition &  &  & $-$0.487 \\\\ \n  &  &  & (1.025) \\\\ \n  & & & \\\\ \n Constant & $-$68.297$^{***}$ & $-$70.205$^{***}$ & $-$70.578$^{***}$ \\\\ \n  & (12.564) & (14.220) & (13.610) \\\\ \n  & & & \\\\ \n\\hline \\\\[-1.8ex] \nObservations & 33 & 33 & 33 \\\\ \nR$^{2}$ & 0.861 & 0.861 & 0.862 \\\\ \nAdjusted R$^{2}$ & 0.846 & 0.841 & 0.842 \\\\ \nResidual Std. Error & 6.562 (df = 29) & 6.667 (df = 28) & 6.651 (df = 28) \\\\ \nF Statistic & 59.732$^{***}$ (df = 3; 29) & 43.421$^{***}$ (df = 4; 28) & 43.660$^{***}$ (df = 4; 28) \\\\ \n\\hline \n\\hline \\\\[-1.8ex] \n\\textit{Note:}  & \\multicolumn{3}{r}{$^{*}$p$&lt;$0.1; $^{**}$p$&lt;$0.05; $^{***}$p$&lt;$0.01} \\\\ \n\\end{tabular} \n\\end{table} \n\n\nThe function outputs raw LaTeX (or HTML), so to get it to form into a table you need to include results='asis' in your Rmarkdown chunk.\n```{r message=FALSE, results='asis'} \n# Load library\nlibrary(stargazer)\n\n# Output model results in a table\nstargazer(lm.1, lm.2, lm.3, type = \"latex\")\n```\n\n\nCustomizing the Stargazer Output\nThere are several arguments in the stargazer() function to customize the table.\n\nstargazer(\n  lm.1, lm.2, lm.3,\n  type = \"latex\",\n  table.placement = \"H\",\n  title = \"Three Regression Models Predicting Variation in Six-Year Graduation Rates\",\n  column.labels = c(\"Model A\", \"Model B\", \"Model C\"),\n  colnames = FALSE,\n  model.numbers = FALSE,\n  dep.var.caption = \" \",\n  dep.var.labels = \"Graduation Rate\",\n  covariate.labels = c(\"Public$^a$\", \"Median SAT score\", \"Tuition\", \"Public$^a$ x Median SAT score\", \"Public$^a$ x Tuition\"),\n  star.cutoffs = NA,\n  keep.stat = c(\"rsq\", \"f\"),\n  notes = \"$^a$0 = Private institution and 1 = Public institution.\",\n  notes.align = \"l\"\n  )\n\n\nThis is close, but there are a few things I am not happy with. Namely,\n\nThe table should be aligned with the caption rather than centered.\nThe double lines produced at the top and bottom of the table.\nThe label for the dependent variable, “Graduation Rate”, is redundant with the caption so could be omitted, as could the border produced above it.\nThe note is separated from the Note: text and uses a colon rather than a period in this text. It also has the text “NA” since we set the star.cutoff= argument (which omits the dreaded significance stars) to NA.\n\nTo fix these things we need to edit the LaTeX syntax that is produced by the stargazer() function. To do this, run the syntax in the console and then copy-and-paste the raw LaTeX into your RMarkdown document (outside of a code chunk). The LaTeX syntax from our table is:\n\n\\begin{table}[H] \\centering \n  \\caption{Three Regression Models Predicting Variation in Six-Year Graduation Rates} \n  \\label{} \n\\begin{tabular}{@{\\extracolsep{5pt}}lccc} \n\\\\[-1.8ex]\\hline \n\\hline \\\\[-1.8ex] \n & \\multicolumn{3}{c}{ } \\\\ \n\\cline{2-4} \n\\\\[-1.8ex] & \\multicolumn{3}{c}{Graduation Rate} \\\\ \n & Model A & Model B & Model C \\\\ \n\\hline \\\\[-1.8ex] \n Public$^a$ & $-$0.647 & 9.132 & 8.554 \\\\ \n  & (4.716) & (32.466) & (19.945) \\\\ \n  & & & \\\\ \n Median SAT score & 0.104 & 0.106 & 0.106 \\\\ \n  & (0.016) & (0.018) & (0.017) \\\\ \n  & & & \\\\ \n Tuition & 0.470 & 0.451 & 0.455 \\\\ \n  & (0.242) & (0.253) & (0.247) \\\\ \n  & & & \\\\ \n Public$^a$ x Median SAT score &  & $-$0.009 &  \\\\ \n  &  & (0.031) &  \\\\ \n  & & & \\\\ \n Public$^a$ x Tuition &  &  & $-$0.487 \\\\ \n  &  &  & (1.025) \\\\ \n  & & & \\\\ \n Constant & $-$68.297 & $-$70.205 & $-$70.578 \\\\ \n  & (12.564) & (14.220) & (13.610) \\\\ \n  & & & \\\\ \n\\hline \\\\[-1.8ex] \nR$^{2}$ & 0.861 & 0.861 & 0.862 \\\\ \nF Statistic & 59.732 (df = 3; 29) & 43.421 (df = 4; 28) & 43.660 (df = 4; 28) \\\\ \n\\hline \n\\hline \\\\[-1.8ex] \n\\textit{Note:}  & \\multicolumn{3}{l}{NA} \\\\ \n & \\multicolumn{3}{l}{$^a$0 = Private institution and 1 = Public institution.} \\\\ \n\\end{tabular} \n\\end{table}\n\nThe \\centering synatax in the first line is what is causing the table itself to be centered. Delete this. The double lines at the top and bottom of the table are produced by the two consecutive hline commands. Remove one at the beginning and end of the table. In a similar way the syntax that corresponds to the “Graduation Rate” label and the \\cline{2-4} preceeding it can be omitted.\nChanging the table note is a little more complicated. The primary syntax for this is the following:\n\n\\textit{Note:}  & \\multicolumn{3}{l}{NA} \\\\ \n & \\multicolumn{3}{l}{$^a$0 = Private institution and 1 = Public institution.} \\\\ \n\nIn the syntax that creates the LaTeX table, the ampersand (&) indicates a new column and the double slash (\\\\) indicates the end of the row. So the italicized text Note: is put in the first cell of the row and then the text “NA” is placed in the next cell of the same row. The multicolumn{3}{l}{} actually combines the last 3 columns into a single cell and aligns the text (which is placed in the third set of curly braces) to the left. To make this all line up better, we are going to replace the note syntax with the following:\n\n\\multicolumn{4}{l}{\\textit{Note.} $^a$0 = Private institution and 1 = Public institution.} \\\\ \n\nThis will combine all four columns of the table into a single cell and will align the text to the left of this cell. The resulting table will fix our issues. This is the syntax that is remaining:\n\\begin{table}[H] \n  \\caption{Three Regression Models Predicting Variation in Six-Year Graduation Rates} \n  \\label{} \n\\begin{tabular}{@{\\extracolsep{5pt}}lccc} \n\\\\[-1.8ex]\n\\hline \\\\[-1.8ex] \n & \\multicolumn{3}{c}{ } \\\\ \n & Model A & Model B & Model C \\\\ \n\\hline \\\\[-1.8ex] \n Public$^a$ & $-$0.647 & 9.132 & 8.554 \\\\ \n  & (4.716) & (32.466) & (19.945) \\\\ \n  & & & \\\\ \n Median SAT score & 0.104 & 0.106 & 0.106 \\\\ \n  & (0.016) & (0.018) & (0.017) \\\\ \n  & & & \\\\ \n Tuition & 0.470 & 0.451 & 0.455 \\\\ \n  & (0.242) & (0.253) & (0.247) \\\\ \n  & & & \\\\ \n Public$^a$ x Median SAT score &  & $-$0.009 &  \\\\ \n  &  & (0.031) &  \\\\ \n  & & & \\\\ \n Public$^a$ x Tuition &  &  & $-$0.487 \\\\ \n  &  &  & (1.025) \\\\ \n  & & & \\\\ \n Constant & $-$68.297 & $-$70.205 & $-$70.578 \\\\ \n  & (12.564) & (14.220) & (13.610) \\\\ \n  & & & \\\\ \n\\hline \\\\[-1.8ex] \nR$^{2}$ & 0.861 & 0.861 & 0.862 \\\\ \nF Statistic & 59.732 (df = 3; 29) & 43.421 (df = 4; 28) & 43.660 (df = 4; 28) \\\\ \n\\hline \\\\[-1.8ex] \n\\multicolumn{4}{l}{\\textit{Note.} $^a$0 = Private institution and 1 = Public institution.} \\\\ \n\\end{tabular} \n\\end{table}\nThe resulting table looks like the following.\n\n\n\nAdding Additional Model-Level Statistics into the Table\nYou may also wish to add other model-level summaries into the table. For example, below I will compute and add the corrected AIC into the table. To do this, use the add.lines= argument in the stargazer() function. This argument takes a list that gives the contents of each cell in the added row. In our example this would be a list including the text to produce in the first cell and then a corrected AIC value for each respective model in the remaining three cells. I also omit the \\(F\\)-statistic in the keep.stat= argument.\n\n# Load library\nlibrary(AICcmodavg)\n\n# Create table\nstargazer(\n  lm.1, lm.2, lm.3,\n  type = \"latex\",\n  table.placement = \"H\",\n  title = \"Three Regression Models Predicting Variation in Six-Year Graduation Rates\",\n  column.labels = c(\"Model A\", \"Model B\", \"Model C\"),\n  colnames = FALSE,\n  model.numbers = FALSE,\n  dep.var.caption = \" \",\n  dep.var.labels = \"Graduation Rate\",\n  covariate.labels = c(\"Public$^a$\", \"Median SAT score\", \"Tuition\", \"Public$^a$ x Median SAT score\", \"Public$^a$ x Tuition\"),\n  star.cutoffs = NA,\n  keep.stat = c(\"rsq\"),\n  notes = \"$^a$0 = Private institution and 1 = Public institution.\",\n  notes.align = \"l\",\n  add.lines = list(c(\"Corrected AIC\", round(AICc(lm.1), 1), round(AICc(lm.2), 1), round(AICc(lm.3), 1)))\n  )\n\nAfter editing the LaTeX syntax to make the same changes as before, we get the following table:"
  },
  {
    "objectID": "posts/2019-05-09-congratulations-ethan/2019-05-09-congratulations-ethan.html",
    "href": "posts/2019-05-09-congratulations-ethan/2019-05-09-congratulations-ethan.html",
    "title": "Congratulations Ethan!",
    "section": "",
    "text": "./assets/body-header.qmdEthan Brown, one of our Statistics Education students successfully defended his dissertation today! His dissertation research looked at how a sequence of structured activities impact students’ understanding about the Empirical Law of Large Numbers and sampling uncertainty. Ethan presented some of his preliminary analyses of this work at ICOTS 10 in Kyoto, Japan last year and received a commendation for that work [read his paper here.\n\n\n\nEthan at the public part of his Final Oral Examination.\n\n\nThe abstract for Ethan’s work is below:\n\nExtensive research has documented students’ difficulty understanding and applying the Empirical Law of Large Numbers, the statistical principle that larger samples result in more precise estimation. However, existing interventions appear to have had limited success, perhaps because they merely demonstrate the Empirical Law of Large Numbers rather than support students’ conceptual understanding of why this phenomenon occurs. This dissertation developed a sequence of activities, Growing Certain, which provided support for two mechanistic explanations of the Empirical Law of Large Numbers for students in a simulation-based introductory statistics course: swamping, the decreasing influence of extreme values on the mean as sample size increases, and heaping, the increasing concentration of possible sample means around the population mean. Five students participated in over six hours of one-on-one clinical interviews of students, with analysis focused on one focal participant, “S”. S’s responses were analyzed using a detailed coding of S’s articulation of mechanism components. S already displayed strong inclination towards swamping in the pre-interview questions, and their articulation of swamping became more sophisticated as they progressed in Growing Certain. However, S’s understanding of the connections between population and sample were weak throughout, and S had a lot of difficulty reasoning about multiple sample means simultaneously in a sampling distribution. S’s lack of abstraction of the sample mean appeared to support them in attending to the dynamics of swamping, but hindered them in being able to reason about heaping. Future research could examine representations that bridge swamping and heaping, and to examine individual differences in attention to the mechanistic components of the Empirical Law of Large Numbers."
  },
  {
    "objectID": "posts/2019-05-06-cv-quotation/2019-05-06-cv-quotation.html",
    "href": "posts/2019-05-06-cv-quotation/2019-05-06-cv-quotation.html",
    "title": "CV: Sale Items vs. Research Plan",
    "section": "",
    "text": "./assets/body-header.qmdKaren Kelsky writes about the importance of identifying a research plan and building your CV around this plan in her book The Professor is In: The Essential Guide to Turning Your Ph.D. into a Job.\n\nA reader once wrote to explain: During my years as a tenure track assistant professor, I went about publishing and doing research the way I do the grocery shopping: concentrating on the sale items like conferences, book reviews, on-line collaborations, i.e., all things that seemed “affordable.” As a result stock up on unnecessary items and find myself too tired to focus on the important things, those items that do not go on sale, but that are the nbuilding block of a good kitchen: articles and books. Although I have managed to publish quite a bit, I have squandered a lot of time and energy, because I did not have a clearly elaborated research plan. (p. 95)\n\nI see this strategy of adding easy line items to the CV with many graduate students and early career professionals. This is not to say that giving talks and writing blog posts are worthless, they aren’t. But, as Roger Peng pointed out in The Effort Report, if you think about your time, which is finite, as a pie chart, anything you do adds a “wedge” into your pie chart and by definition takes away area (time) from something else. If you have time constraints, it is better to optimize the time you spend, and for most academics that means focusing on scholarship, especially articles and books which are often weighted more in tenure and promotion decisions than other elements of scholarship.\nThe advice Karen gives is solid; build a research agenda and then spend your time on things that matter."
  },
  {
    "objectID": "posts/2019-06-17-2019-statprep-workshops/2019-06-17-2019-statprep-workshops.html",
    "href": "posts/2019-06-17-2019-statprep-workshops/2019-06-17-2019-statprep-workshops.html",
    "title": "2019 StatPREP Workshops",
    "section": "",
    "text": "./assets/body-header.qmdI just finished helping out with two StatPREP workshops in Columbia, Maryland and Fort Worth, Texas, respectively. StatPREP is an initiative of the Mathematical Association of America (MAA), in conjunction with American Mathematical Association of Two-Year Colleges (AMATYC) and the American Statistical Association (ASA), to introduce data and computing into introductory statistics courses—specifically in community college classrooms.\n\n\n\nSummer 2019 StatPREP participants and workshop leaders at Howard Community College\n\n\nEach summer, workshops are held in four locations, and each location hosts a workshop for two consecutive years. This year’s hosts included:\n\nUniversity of Hartford (Hartford, CT)\nHoward Community College (Columbia, MD)\nTarrant County Community College (Fort Worth, TX)\nHighline College (Des Moines, WA)\n\nThis is my third summer working with the StatPREP team to help deliver these workshops and they get better each year. There are several things that happen at a workshop (see here for the 2019 schedules), but one of the neatest resources introduced to participants are the “Little Apps”.\nStatPREP Little Apps are interactive Shiny applications “designed to let students explore specific statistical techniques in the context of data. Each Little App has tabs that show the graphs, an explain tab that helps to explain the concepts, codebook that tells about the dataset and the variables, and R commands that show the R commands that were used in the Little App.” Below is a screen shot of a Little App for exploring ideas related to linear regression.\n\n\n\nScreenshot of the Regression Little App\n\n\nYou can see all of the Little Apps and play around with them here.\nAnother great idea I got from StatPREP this year was the workshop website. Danny Kaplan, resident StatPREP genius and all-around R renaissance guru, used blogdown to create the 2019 workshop website. This was slick, not only for creating a totally rad site, but also for easily being able to add tags and categories to inter-link common posts and resources. Definitely something I will use in the future.\n\n\n\nTags used in the StatPrep workshop website"
  },
  {
    "objectID": "posts/2020-12-16-packages-and-resources-for-data-viz/index.html",
    "href": "posts/2020-12-16-packages-and-resources-for-data-viz/index.html",
    "title": "Packages and Resources for Data Viz",
    "section": "",
    "text": "./assets/body-header.qmd\nI have written several notes to myself over the years as reminders. These include ideas for research, R packages I have seen and may use sometime), things to-do, etc. I am in the process of making some of these notes more public on my blog. This will act as a more searchable, curated “note” for myself, but also makes it available to anyone else who would benefit.\nThese are resources I was compiling for use in our course, EPsy 1261: Understanding Data Stories through Visualization & Computing."
  },
  {
    "objectID": "posts/2020-12-16-packages-and-resources-for-data-viz/index.html#online-books",
    "href": "posts/2020-12-16-packages-and-resources-for-data-viz/index.html#online-books",
    "title": "Packages and Resources for Data Viz",
    "section": "Online Books",
    "text": "Online Books\n\nHealy, K. (2018). Data Visualization: A Practical Introduction.\n\nsocviz: R package to accompany Kieran Healy’s book, Data Visualization: A Practical Introduction. It includes datasets, functions, and course materials.\n\nWilke, C. O. (2019). Fundamentals of Data Visualization.\n\npractical_ggplot2: R package to accompany Claus Wilke’s book, Fundamentals of Data Visualization. Great suggestions for making plots look great!"
  },
  {
    "objectID": "posts/2020-12-16-packages-and-resources-for-data-viz/index.html#general-resources-blog-posts-slides-etc.",
    "href": "posts/2020-12-16-packages-and-resources-for-data-viz/index.html#general-resources-blog-posts-slides-etc.",
    "title": "Packages and Resources for Data Viz",
    "section": "General Resources (Blog Posts, Slides, etc.)",
    "text": "General Resources (Blog Posts, Slides, etc.)\n\nBarrett, M. (2019). Designing ggplots: Making clear figures that communicate. Slides from a workshop presented at Southern California R Users All-Hands Meetup, Burbank, CA.\nChase, W. (2019, May 29). R you ready to make charts? Slides from a Philly Dataviz Meetup.\nCorrell, M. (2018, November 20). What does “visualization literacy” mean, anyway? Multiple Views: Visualization Research Explained. Blog post.\nHullman, J. (2020, April 06). Being Bayesian with visualization. Multiple Views: Visualization Research Explained. Blog post.\nKashnitsky, I. (2019). [Dotplot – the single most useful yet largely neglected dataviz type](https://ikashnitsky.github.io/2019/dotplot/ (Replace barplots). Blog post.\nKirk, A. (2019, August 07). Five ways to design for red-green colour blindness. Blog post.\nLeo, S. (2019, March 27). Mistakes, we’ve drawn a few: Learning from our errors in data visualisation. The Economist. Online.\nRanzolin, D. (2019, July 07). Thoughts on animation and movement in data visualization. Blog post.\nScherer, C. (2019). The evolution of a ggplot. Blog post.\nSchwabish, J. (2018, November 19). Teaching data visualization to kids. PolicyViz. Online.\nTol, P. (2019, May 25). Notes on color palettes for scientific visualizations. Blog post."
  },
  {
    "objectID": "posts/2020-12-16-packages-and-resources-for-data-viz/index.html#tools",
    "href": "posts/2020-12-16-packages-and-resources-for-data-viz/index.html#tools",
    "title": "Packages and Resources for Data Viz",
    "section": "Tools",
    "text": "Tools\n\nCoolors: Online palette generator and explorer. Can also display created alettes under many types of colorblindness.\nDigital Story Templates: The Financial Times has created a set of six digital story templates to help its reporters and editors commission digital and visual stories.\nNAPA Cards: Narrative Patterns for Data-Driven Storytelling.\nNeatline: Neatline allows scholars, students, and curators to tell stories with maps, images and timelines.\nRAW Graphs: RAW Graphs is an open source data visualization framework built with the goal of making the visual representation of complex data easy for everyone.\nScrollama: Scrollama is a modern and lightweight JavaScript library for scrollytelling.\nTinEye: TinEye is a tool that can extract color palettes from an image. It also has image search capabilities.\nThe Vistorian. The Vistorian is an online platform that provides interactive visualization for various kinds of networks. It is a collaborative open-source research-project currently in the prototyping phase. It also allows you to import CSV files to visualize.\nWho can use this?: A tool that brings attention and understanding to how color contrast can affect different people with visual impairments."
  },
  {
    "objectID": "posts/2020-12-16-packages-and-resources-for-data-viz/index.html#tutorials",
    "href": "posts/2020-12-16-packages-and-resources-for-data-viz/index.html#tutorials",
    "title": "Packages and Resources for Data Viz",
    "section": "Tutorials",
    "text": "Tutorials\n\nBurkhart, C. Streetmaps: Create a streetmap of your favorite city with ggplot2 and powerpoint. Blog post. Burkhart’s site also includes other tutorials to help you master ggplot2. Each tutorial provides a step-by-step guide that teaches you how to create visualizations that go beyond the basics of ggplot2."
  },
  {
    "objectID": "posts/2020-12-16-packages-and-resources-for-data-viz/index.html#r-packages",
    "href": "posts/2020-12-16-packages-and-resources-for-data-viz/index.html#r-packages",
    "title": "Packages and Resources for Data Viz",
    "section": "R Packages",
    "text": "R Packages\nExtensions\n\nbrolgar: Package to browse over longitudinal data graphically and analytically in R, by providing tools to efficiently explore raw longitudinal data, calculate features (summaries) for individuals, and evaluate diagnostics of statistical models.\nggrough: Package to converts ggplot2 plots to rough/sketchy charts, using the javascript roughjs library.\nvtree: Package for displaying nested subsets of a data frame.\nwaffle: Package to create waffleplots\n\nPalettes/Themes\n\npaletteer: Comprehensive collection of palettes in R. Includes palettes from many, many different R packages.\ndutchmasters: Collection of color palettes based on paintings by the Dutch masters Johannes Vermeer and Rembrandt van Rijn.\nharrypotter: Collection of palettes based on the Harry Potter film series.\nNineteenEightyR: Collection of color palettes based on the 80s.\ntvthemes: Collection of various ggplot2 themes and color/fill palettes based on everybody’s favorite TV shows.\nwesanderson: Collection of color palettes based on Was Anderson movies.\n\nColor Blindness\n\ncolorblindr: Simulate colorblindness in production-ready R figures."
  },
  {
    "objectID": "posts/2020-12-16-packages-and-resources-for-data-viz/index.html#data-for-viz",
    "href": "posts/2020-12-16-packages-and-resources-for-data-viz/index.html#data-for-viz",
    "title": "Packages and Resources for Data Viz",
    "section": "Data for Viz",
    "text": "Data for Viz\n\nR4DS online learning community: A GitHub organization for the R4DS online learning community.\nTidy Tuesday: Real-world data that can be used for wrangling and visualization.\nThe Stanford Open Policing Project: Standardized police stop data are available to download (by location). The data are provided in both CSV and RDS formats. In addition, shapefiles are available for select locations."
  },
  {
    "objectID": "posts/2020-12-16-packages-and-resources-for-data-viz/index.html#miscellaneous",
    "href": "posts/2020-12-16-packages-and-resources-for-data-viz/index.html#miscellaneous",
    "title": "Packages and Resources for Data Viz",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\nFlyer from Columbia Data Science Institute art contest"
  },
  {
    "objectID": "posts/2021-02-05-some-tv-updates/index.html",
    "href": "posts/2021-02-05-some-tv-updates/index.html",
    "title": "Some TV Updates",
    "section": "",
    "text": "./assets/body-header.qmdWe are almost a year into this dumpster fire called Coronavirus, so I thought I would do a little updating about the TV shows I am watching to pass the time through this nightmare. In no particular order, other than how they come to mind, the “winners” are:\nAll-American: I am watching this on Netflix and am most of the way through Season 2 (although Season 3 is airing now). How I did not know about this show is a mystery. It is a teen drama produced by Greg Berlanti, who also has written and produced some of my other favorite shows (Dawson’s Creek, Everwood, Brothers and Sisters, Dirty Sexy money, Arrow).\nCardinal: This Canadian crime drama is dark and I am into it. I am in Season 2. Hulu only has the first three seasons, although four have been made. Each season only has 6 episodes, which is the perfect length for a TV series. (Fewer episodes seems to be the norm in other countries; a model the U.S. could learn from since many shows suffer a blight of content when they try to include 20 episodes in a season. HBO and Showtime have done better with this than other networks.)\nHackerville: Hackerville was a show released on HBO Europe that followed two police officers (one from Germany and the other from Romania) as they tried to track down a hacker who stole 9.99 euros from a German Bank. The show was fine, not surprisingly only lasted a single season. Although it wasn’t “must-see TV”, it was good enough to watch and kill some time.\nLupin: This Netflix original is inspired by Mauric Leblanc’s master thief of literature, Arsenne Lupin. It was made in France, so unless you speak the language of love, you will need the English audio track or subtitles. This was another good, not great, Netflix offering. They keep putting out more and more mediocre content.\nTed Lasso: Perhaps the best show that has come out in the last year. (And this on Apple TV which has produced some bombs.) Jason Sudeikis plays an American football coach who is hired to coach an English soccer team. Funny, sweet, and just all-around an enjoyable show.\nBig Sky: I am currently in the middle of Season 1 of this David Kelley (Doogie Howser, Ally McBeal, Boston Legal, Big Little Lies) criime drama. His shows are hit-or-miss for me; loved his earlier stuff, but his recent opus is less than compelling. Big Sky is interesting because of the characters and the acting. Katheryn Winnick and Kylie Bunbury are great, and Ryan Phillipe was a nice surprise. Watching this “live” reminds me of how much I hate waiting for new episodes.\nYour Honor: This is a gem. Airing in real-time on Showtime, this is a nail-biter and I can’t wait for new episodes. Bryan Cranston is great (as always). Well worth a watch, although I have no idea how they will make a second season.\nI am also watching some garbage that I can’t seem to stop watching. The Resident, The Good Doctor, and The Rookie are in my Hulu recording queue. For Life was good at the beginning, but the last few episodes have been less than awesome.\nI also continue to watch the The Bold Type, mainly as a summer show, although it seems to have jumped the shark in the last season or so. If any of the show runners/producers of Younger read this….get on making new seasons of this show!!! I miss Liza Miller, Kelsey, Maggie, and Diana. (Also, more shows need to cast Sutton Foster; bring back Bunheads)."
  },
  {
    "objectID": "posts/2020-01-13-apa-tables-using-rmarkdown-part-4/2020-01-13-apa-tables-using-rmarkdown-part-4.html",
    "href": "posts/2020-01-13-apa-tables-using-rmarkdown-part-4/2020-01-13-apa-tables-using-rmarkdown-part-4.html",
    "title": "APA Tables using RMarkdown: Part 4",
    "section": "",
    "text": "./assets/body-header.qmd\nThis is the fourth part of a short blog series I am writing to re-create some of the sample tables found in the 7th edition APA Publication Manual. In this post I will attempt to mimic Table 7.17 (p. 220). To do so, I will incorporate many ideas that I covered in the first, second, and third of these posts.\n\n\n\nTable 7.17 from the 7th edition of the APA Publication Manual (p. 220).\n\n\nRather than re-create this table using the data from Table 7.17, I will illustrate mimicing this table with summary information gleaned from a different dataset.\n\nMy Process\nI will again render to PDF and set up the YAML to import the caption package (LaTeX) and set up the APA caption formatting.\n\n---\ntitle: \"Untitled\"\nauthor: \"Andrew Zieffler\"\ndate: \"1/12/2020\"\noutput: pdf_document\nheader-includes:\n   - \\usepackage{caption}\n   - \\captionsetup[table]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=newline}\n---\n\nIn the RMarkdown body, I will load a few packages and also import a data set that I will use to create the summary values akin to those in Table 7.10.\n\n# Load libraries\nlibrary(broom)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(tidyverse)\n\n# Import data\nmn = read_csv(\"https://raw.githubusercontent.com/zief0002/rustic-penguin/main/data/mn-schools.csv\") |&gt;\n  mutate(public = if_else(sector == \"Public\", 1, 0))\n\n# View data\nhead(mn)\n\n# A tibble: 6 × 6\n  name                               grad sector    sat tuition public\n  &lt;chr&gt;                             &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 Augsburg College                   65.2 Private  1030    39.3      0\n2 Bethany Lutheran College           52.6 Private  1065    30.5      0\n3 Bethel University, Saint Paul, MN  73.3 Private  1145    39.4      0\n4 Carleton College                   92.6 Private  1400    54.3      0\n5 College of Saint Benedict          81.1 Private  1185    43.2      0\n6 Concordia College at Moorhead      69.4 Private  1145    36.6      0\n\n\nThe data in were collected from http://www.collegeresults.org and contain 2011 institutional data for n=33 Minnesota colleges and universities. The codebook is available here.\nWe need to compute the regression results from fitting an OLS model. Here I will predict variation in graduation rates using the other variables in the dataset. I will also use the tidy() output from the broom package to obtain the coefficient-level output that is summarized in the table.\n\n# Fit model\nlm.1 = lm(grad ~ 1 + public + sat + tuition, data = mn)\n\n# Obtain coefficient-level summaries\ntab_01 = tidy(lm.1)\n\n# View table\ntab_01\n\n# A tibble: 4 × 5\n  term        estimate std.error statistic     p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 (Intercept)  -68.3     12.6       -5.44  0.00000755 \n2 public        -0.647    4.72      -0.137 0.892      \n3 sat            0.104    0.0159     6.54  0.000000364\n4 tuition        0.470    0.242      1.94  0.0617     \n\n\nWe also need to obtain the limits of the confidence intervals. We can simply take the estimate and add/subtract two standard errors to get these limits.\n\n# Compute confidence limits\ntab_01 = tab_01 %&gt;%\n  mutate(\n    LL = estimate - 2*std.error,\n    UL = estimate + 2*std.error\n  )\n\n# View table\ntab_01\n\n# A tibble: 4 × 7\n  term        estimate std.error statistic     p.value       LL      UL\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  -68.3     12.6       -5.44  0.00000755  -93.4    -43.2  \n2 public        -0.647    4.72      -0.137 0.892       -10.1      8.78 \n3 sat            0.104    0.0159     6.54  0.000000364   0.0721   0.136\n4 tuition        0.470    0.242      1.94  0.0617       -0.0135   0.953\n\n\nNow we just arrange the columns in the order we want to create the table and also re-name the elements in the term column to correspond to the text we want in the table.\n\n# Select columns\ntab_01 = tab_01 %&gt;%\n  select(\n    term, estimate, std.error, LL, UL, p.value\n  ) %&gt;%\n  mutate(\n    term = c(\"Constant\", \"Sector$^a$\", \"Median SAT score\", \"Tuition$^b$\")\n  )\n\n# View table\ntab_01\n\n# A tibble: 4 × 6\n  term             estimate std.error       LL      UL     p.value\n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n1 Constant          -68.3     12.6    -93.4    -43.2   0.00000755 \n2 Sector$^a$         -0.647    4.72   -10.1      8.78  0.892      \n3 Median SAT score    0.104    0.0159   0.0721   0.136 0.000000364\n4 Tuition$^b$         0.470    0.242   -0.0135   0.953 0.0617     \n\n\nI will then pipe this into the kable() function to set the column names, column alignment, and table caption. The digits= argument is included to round the values in each column. (Since the first column is text we set this to NA.)\nI also employ similar kableExtra function from those introduced in Part 1, Part 2, and Part 3 to make the table the full page width, include the footnote, center the header names, and increase the width of the first column, and add the top header row.\n\nkable(\n  tab_01,\n  format = \"latex\",\n  booktabs = TRUE,\n  escape = FALSE,\n  col.names = c(\"\", \"\", \"\", \"$LL$\", \"$UL$\", \"\"),\n  align = c(\"l\", \"c\", \"c\", \"c\", \"c\", \"c\"),\n  digits = c(NA, 3, 3, 3, 3, 3),\n  caption = \"Regression Coefficients of Institutional Predictors of Graduation Rates\"\n  ) %&gt;%\n  kable_styling(full_width = TRUE) %&gt;%\n  footnote(\n    general_title = \"Note.\",\n    general = \"$N=33$. $^a$0 = private schools and 1 = public schools. $^b$Tuition is measured in thousands of dollars.\",\n    footnote_as_chunk = TRUE,\n    escape = FALSE\n    ) %&gt;%\n  row_spec(row = 0, align = \"c\") %&gt;%\n  column_spec(column = 1, width = \"1.5in\") %&gt;%\n  add_header_above(\n    c(\"Variable\" = 1, \"$B$\" = 1, \"$SE$\" = 1, \"95\\\\\\\\% CI\" = 2, \"$p$\" = 1),\n    escape = FALSE\n  )\n\n\nWe need to remove the extraneous mid-rules by altering the LaTeX syntax as we did in Part 2.\n\n\\begin{table}\n\n\\caption{\\label{tab:}Regression Coefficients of Institutional Predictors of Graduation Rates}\n\\centering\n\\begin{tabu} to \\linewidth {&gt;{\\raggedright\\arraybackslash}p{1.5in}&gt;{\\centering}X&gt;{\\centering}X&gt;{\\centering}X&gt;{\\centering}X&gt;{\\centering}X}\n\\toprule\n\\multicolumn{1}{c}{Variable} & \\multicolumn{1}{c}{$B$} & \\multicolumn{1}{c}{$SE$} & \\multicolumn{2}{c}{95\\% CI} & \\multicolumn{1}{c}{$p$} \\\\\n\\cmidrule(l{3pt}r{3pt}){4-5}\n\\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{} & \\multicolumn{1}{c}{$LL$} & \\multicolumn{1}{c}{$UL$} & \\multicolumn{1}{c}{}\\\\\n\\midrule\nConstant & -68.297 & 12.564 & -93.424 & -43.170 & 0.000\\\\\nSector$^a$ & -0.647 & 4.716 & -10.078 & 8.784 & 0.892\\\\\nMedian SAT score & 0.104 & 0.016 & 0.072 & 0.136 & 0.000\\\\\nTuition$^b$ & 0.470 & 0.242 & -0.014 & 0.953 & 0.062\\\\\n\\bottomrule\n\\multicolumn{6}{l}{\\textit{Note.} $N=33$. $^a$0 = private schools and 1 = public schools. $^b$Tuition is measured in thousands of dollars.}\\\\\n\\end{tabu}\n\\end{table}\n\n\n\n\nAlternative Table\nAn alternative table puts the confidence intevral in a single column by putting the confidence limits in square brackets. This is akin to Table 7.16 (p. 219) in the 7th edition of the APA Publication Manual.\n\n\n\nTable 7.16 from the 7th edition of the APA Publication Manual (p. 219).\n\n\nTo create this I will again use the tidy() function and create the confidence limits. I will then create a new column in the tidy output that concatenates the confidence limits and places them inside square brackets. Lastly I will select the columns I want for the table and set the text for the “Variables” column.\n\n# Get coefficient-level output\n# Create confidence limits\ntab_01 = tidy(lm.1) %&gt;%\n  mutate(\n    LL = estimate - 2*std.error,\n    UL = estimate + 2*std.error\n  )\n\n# Put confidence limits inside square brackets\ntab_01$CI = paste0(\"[\", round(tab_01$LL, 2), \", \", round(tab_01$UL, 2), \"]\")\n\n# Get output for table\ntab_01 = tab_01 %&gt;%\n  select(\n    term, estimate, std.error, statistic, p.value, CI\n  ) %&gt;%\n  mutate(\n    term = c(\"Constant\", \"Sector$^a$\", \"Median SAT score\", \"Tuition$^b$\")\n  )\n\ntab_01\n\n# A tibble: 4 × 6\n  term             estimate std.error statistic     p.value CI              \n  &lt;chr&gt;               &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;           \n1 Constant          -68.3     12.6       -5.44  0.00000755  [-93.42, -43.17]\n2 Sector$^a$         -0.647    4.72      -0.137 0.892       [-10.08, 8.78]  \n3 Median SAT score    0.104    0.0159     6.54  0.000000364 [0.07, 0.14]    \n4 Tuition$^b$         0.470    0.242      1.94  0.0617      [-0.01, 0.95]   \n\n\nFinally, I will use kable() and the kableExtra functionality to create the table.\n\nkable(\n tab_01,\n format = \"latex\",\n booktabs = TRUE,\n escape = FALSE,\n col.names = c(\"Variables\", \"$B$\", \"$SE$\", \"$t$\", \"$p$\", \"95\\\\% CI\"),\n align = c(\"l\", \"c\", \"c\", \"c\", \"c\", \"c\"),\n digits = c(NA, 2, 2, 2, 3, NA),\n caption = \"Regression Coefficients of Institutional Predictors of Graduation Rates\"\n ) %&gt;%\n kable_styling(full_width = TRUE) %&gt;%\n footnote(\n   general_title = \"Note.\",\n   general = \"$N=33$. $^a$0 = private schools and 1 = public schools. $^b$Tuition is measured in thousands  dollars.\",\n   footnote_as_chunk = TRUE,\n   escape = FALSE\n   ) %&gt;%\n row_spec(row = 0, align = \"c\") %&gt;%\n column_spec(column = 1, width = \"1.5in\") %&gt;%\n column_spec(column = 6, width = \"1in\")"
  },
  {
    "objectID": "posts/2018-04-06-epsy-8271-announcement/2018-04-06-epsy-8271-announcement.html",
    "href": "posts/2018-04-06-epsy-8271-announcement/2018-04-06-epsy-8271-announcement.html",
    "title": "Statistics Education Research Seminar: Teaching Statistics from a Modeling Perspective",
    "section": "",
    "text": "./assets/body-header.qmd\nMy colleague Robert delMas is teaching our doctoral-level research seminar, EPsy 8271 next fall, and it looks to be an interesting topic. The details about the course follow:"
  },
  {
    "objectID": "posts/2018-04-06-epsy-8271-announcement/2018-04-06-epsy-8271-announcement.html#epsy-8271-statistics-education-research-seminar-teaching-statistics-from-a-modeling-perspective-3-credits",
    "href": "posts/2018-04-06-epsy-8271-announcement/2018-04-06-epsy-8271-announcement.html#epsy-8271-statistics-education-research-seminar-teaching-statistics-from-a-modeling-perspective-3-credits",
    "title": "Statistics Education Research Seminar: Teaching Statistics from a Modeling Perspective",
    "section": "EPSY 8271 | Statistics Education Research Seminar: Teaching Statistics from a Modeling Perspective (3 credits)",
    "text": "EPSY 8271 | Statistics Education Research Seminar: Teaching Statistics from a Modeling Perspective (3 credits)\nDay/Time: Fridays, 9:00 a.m.–11:30 a.m. (Fall 2018) Location: 220 Wulling Hall  Instructor: Robert delMas, Ph.D.\nThis seminar will focus on research related to teaching introductory statistics through a modeling approach. Students will read and discuss:\n\nScholarship related to the similarities and differences between mathematical and statistical models\nResearch conducted on the development of students’ (and teachers’) understanding of probability and statistics through modeling-based instructional approaches\n\nDuring this process, students will learn about and critique research methods, critically examine research findings from studies, and consider implications for teaching.\nAs a capstone to the course, each student will produce a scholarly paper that explores, in more depth, a research area covered during the course.\nFor more information, contact Robert delMas, delma001@umn.edu."
  },
  {
    "objectID": "posts/2019-12-12-thesis-season/2019-12-12-thesis-season.html",
    "href": "posts/2019-12-12-thesis-season/2019-12-12-thesis-season.html",
    "title": "Thesis Season",
    "section": "",
    "text": "./assets/body-header.qmdIt is the time of the semester when it seems every graduate student wants to make progress on their milestones, and several oral prelim exams and thesis defenses get crammed into a 1–2 week period. In the spirit of what I call “Thesis Season”, here are some good reads to keep this all in perspective.\nLuke Burns wrote an inspired piece for McSweeney’s entitled FAQ: The “Snake Fight” Portion of Your Thesis Defense. To give you a quick preview,\n\nQ: Do I have to kill the snake? A: University guidelines state that you have to “defeat” the snake. There are many ways to accomplish this. Lots of students choose to wrestle the snake. Some construct decoys and elaborate traps to confuse and then ensnare the snake. One student brought a flute and played a song to lull the snake to sleep. Then he threw the snake out a window.\n\nIt is worth the read. Thanks to Jeff Bye for bring this to my attention.\nThis type of thesis defense strategy was also proposed by xkcd savant Randall Monroe.\n\n\n\nxkcd thesis defense\n\n\nHappy Thesis Season!"
  },
  {
    "objectID": "posts/2018-03-15-boys-hockey-post-hoc/2018-03-15-boys-hockey-post-hoc.html",
    "href": "posts/2018-03-15-boys-hockey-post-hoc/2018-03-15-boys-hockey-post-hoc.html",
    "title": "Minnesota State High School Boys Hockey Predictions (Post Hoc Analysis)",
    "section": "",
    "text": "./assets/body-header.qmd\nIn two previous posts, post 1 and post 2, I used Monte Carlo simulation to predict the winner of the 2018 Minnesota State High School Boys Hockey tournament. Now that the tournament is over, I wanted to analyze how the model did and also think about ways to improve the predictions should I want to re-run such a simulation in the future."
  },
  {
    "objectID": "posts/2018-03-15-boys-hockey-post-hoc/2018-03-15-boys-hockey-post-hoc.html#accuracy-of-the-predictions",
    "href": "posts/2018-03-15-boys-hockey-post-hoc/2018-03-15-boys-hockey-post-hoc.html#accuracy-of-the-predictions",
    "title": "Minnesota State High School Boys Hockey Predictions (Post Hoc Analysis)",
    "section": "Accuracy of the Predictions",
    "text": "Accuracy of the Predictions\nSo, how well did the simulation do in predicting the state tournament champion? Orono was the winner of the Class A tournament. Before the tournament started I gave them a 16% chance of winning the state tournament. This was the third highest percentage, but was not that different than the other two higher percentages (Hermantown: 20% and Mahtomedi: 18%). In the Class AA tournament, the winner was Minnetonka. Again, there were two teams that had a higher percent chance of winning, but only by about 1–2%.\nThis seems pretty good. If I look at the teams that made the final four, three of the top four predictions in the Class A tournament made the semifinals, and the same in Class AA. Before I get too excited about this, it is interesting to note that if you had picked the top four seeds in each class to make the semifinals you would have done just as well."
  },
  {
    "objectID": "posts/2018-03-15-boys-hockey-post-hoc/2018-03-15-boys-hockey-post-hoc.html#picking-the-bracket-based-on-elo-ratings",
    "href": "posts/2018-03-15-boys-hockey-post-hoc/2018-03-15-boys-hockey-post-hoc.html#picking-the-bracket-based-on-elo-ratings",
    "title": "Minnesota State High School Boys Hockey Predictions (Post Hoc Analysis)",
    "section": "Picking the Bracket Based on Elo Ratings",
    "text": "Picking the Bracket Based on Elo Ratings\nHow well might we have done if we had just used the Elo ratings to fill out our bracket? Below is a bracket with the Elo predicted winners. Predictions that would have been wrong have a strikethrough.\n\nClass A Tournament\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass AA Tournament\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe wouldn’t have done very well. This is not, perhaps, surprising as the Elo ratings for many of the top teams are quite similar. Thus upsets will likely happen. In addition, these are high school players, and the emotion of playing in the state tournament is not factored into any rating system."
  },
  {
    "objectID": "posts/2018-03-15-boys-hockey-post-hoc/2018-03-15-boys-hockey-post-hoc.html#improvments-for-the-future",
    "href": "posts/2018-03-15-boys-hockey-post-hoc/2018-03-15-boys-hockey-post-hoc.html#improvments-for-the-future",
    "title": "Minnesota State High School Boys Hockey Predictions (Post Hoc Analysis)",
    "section": "Improvments for the Future",
    "text": "Improvments for the Future\nBefore I suggest ways to improve the predictions, I want to state that I am a little uneasy about predicting high school games. I took this on as an exercise in learning and a fun augmentation to my viewing of the state tournament. That being said, here are things I would like to do in the future.\n\nSimulation Speed: The first thing I would improve would be the simulation speed. I did a quick code using a for() loop. This is incredibly slow. I think this could be improved using functionality from the purrr package.\nDynamic Parameters in the Elo Ratings: I used a static \\(K\\)-factor of 15 in computing the Elo ratings. I could have upweighted certain games using diffeent values of \\(K\\). For example, conference games and section playoff games could have used higher weights.\nPredict Elo within Class: I computed the Elo ratings by considering all the teams in the state. I think it might be useful to explore predicting Elo within class. This could be either used directly, or weighted and combined with the overall Elo rating in some fashion.\nPredict Probabilites for Other Events: Aside from predicting the probability of winning the tournament, I might predict probabilities of making the semifinals or finishing in the top 3.\nConsider Other Rating Models: It might be interesting to use a rating system such as Glicko that includes an uncertainty parameter."
  },
  {
    "objectID": "posts/2019-10-22-television/2019-10-22-television.html",
    "href": "posts/2019-10-22-television/2019-10-22-television.html",
    "title": "Television",
    "section": "",
    "text": "./assets/body-header.qmd\nIn 2017, my wife and I were fed up with the cost of cable television and were considering “cutting the cord”. In addition to cable, we also had Hulu, Netflix, and Amazon Prime, so our hypothesis was that we probably didn’t need cable. Before making the decision to cut cable, I wanted some data to help ensure that I would not be eliminating many of the TV shows that I watched. So, I started recording every episode of every TV show that I watched in a Google Sheet.\n# Load libraries\nlibrary(dplyr)\nlibrary(readr)\nlibrary(janitor)\nlibrary(plotly)\nlibrary(tidyr)\n\n# Import data\ntv = read_csv(\"~/Documents/data/andy_tv.csv\") %&gt;%\n  drop_na()\n\n# View data\nhead(tv)\n\n# A tibble: 6 × 8\n   Year Media   Network      Show       Season   Episode  Time Episode_name  \n  &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;         \n1  2018 Netflix CBC (Canada) 21 Thunder Season 1       1    44 Pilot         \n2  2018 Netflix CBC (Canada) 21 Thunder Season 1       2    43 Road Game     \n3  2018 Netflix CBC (Canada) 21 Thunder Season 1       3    43 Freefalling   \n4  2018 Netflix CBC (Canada) 21 Thunder Season 1       4    43 Fixed         \n5  2018 Netflix CBC (Canada) 21 Thunder Season 1       5    43 Heaven Or Hell\n6  2018 Netflix CBC (Canada) 21 Thunder Season 1       6    43 War\nI then used Plotly to create a hierarchical donut plot of the shows and media I used to view those shows. (Note. At the time I only had my 2017 data, but now, since I have several years worth of data I need to filter to get the 2017 data.)\n# Filter 2017 data\ntv_2017 = tv %&gt;% filter(Year == 2017)\n\n# Compute time per show for better plotting\ntv17 = tv_2017 %&gt;% \n  group_by(Media, Show, Network) %&gt;% \n  summarize(Time = sum(Time)) %&gt;% \n  ungroup() %&gt;%\n  select(Show, Media, Time) \n\n`summarise()` has grouped output by 'Media', 'Show'. You can override using the\n`.groups` argument.\n\nhead(tv17)\n\n# A tibble: 6 × 3\n  Show                 Media         Time\n  &lt;chr&gt;                &lt;chr&gt;        &lt;dbl&gt;\n1 Bosch                Amazon Prime   805\n2 Goliath              Amazon Prime   448\n3 Mozart in the Jungle Amazon Prime   814\n4 Private Sales        Amazon Prime   140\n5 Red Oaks             Amazon Prime   834\n6 Vikings              Amazon Prime   836\nI then looked for duplicate TV programs across media so that I could combine them. For example, I was watching Vikings at the time on Cable, but it was also available on Hulu.\n# Find duplicates\n#tv17 %&gt;% get_dupes(Show)\n\n# Combine duplicates\ntv17[tv17$Show == \"Lucifer\" & tv17$Media == \"Cable\", ]$Show = \"Lucifer \"\ntv17[tv17$Show == \"Madam Secretary\" & tv17$Media == \"Cable\", ]$Show = \"Madam Secretary \"\ntv17[tv17$Show == \"Vikings\" & tv17$Media == \"Hulu\", ]$Show = \"Vikings \"\nLastly, I needed to obtain the the time for each media outlet and the total time across all outlets to correctly plot the heirarchy in the donut plot.\n# Obtain time by media outlet\nsecond = tv17 %&gt;% \n  group_by(Media) %&gt;% \n  summarize(Time = sum(Time)) %&gt;%\n  ungroup() %&gt;%\n  mutate(Show = Media, Media = \"2017\")  %&gt;%\n  select(Show, Media, Time) \n\n# Obtain total time\nfirst = data.frame(\n  Show = \"2017\", \n  Media = \"\",\n  Time = sum(second$Time)\n)\n\n# Combine the hierarchies\ntv_17 = rbind(first, second, tv17)\nFinally I could use the plot_ly() function from the plotly library to create the donut plot.\nplot_ly(tv_17) %&gt;%\n  add_trace(\n    labels = ~Show,\n    hoverinfo = 'text',\n    hovertext = ~paste(\"&lt;b&gt;\", Show, '&lt;/b&gt;&lt;br /&gt;&lt;br /&gt;Time Watching: ', round(Time/60, 1), ' Hours'),\n    parents = ~Media,\n    values = ~Time,\n    type = 'sunburst',\n    branchvalues = 'total'\n  ) %&gt;%\n  layout(\n    sunburstcolorway = c(\n      \"#cc2a36\",\"#edc951\",\"#4f372d\", \"#00a0b0\"\n    )\n  )\n\n\n\nTV shows Andy watched in 2017.\nIt turns out that most of what I was watching was through the streaming services and not cable. Those shows that I was watching on cable are also either on premium channels (e.g., HBO and Showtime) which are available as separate add-ons to most streaming services, or were shows that were also available on Hulu.\nI realized that after the fact, the one important thing I didn’t collect data on was live sports. This, it turns out, is quite important. Both my wife and I love college football and watching these games without having to leave the house every weekend means that we do need to have some sort of live TV option. Ultimately, we switched to Hulu Live which turned out to be a huge cost savings relative to cable. We chose this streaming service because of the sports channel options without needing an additional sports package; the big decider was that Hulu Live had BTN. (Reality check: Watching live sports on Hulu Live is not without its pain. It buffers more than I would like, especially during evernts that are more highly viewed. This seems to be especially true with any of the Fox stations (FSN, FS1, Fox) which televises many of the MLB, NFL, and NHL games I watch.)"
  },
  {
    "objectID": "posts/2019-10-22-television/2019-10-22-television.html#current-tv-habits-and-more-cutting",
    "href": "posts/2019-10-22-television/2019-10-22-television.html#current-tv-habits-and-more-cutting",
    "title": "Television",
    "section": "Current TV Habits and More Cutting",
    "text": "Current TV Habits and More Cutting\nI have continued to collect data on the TV shows I watch. I still haven’t collected data about the live sports I watch, but that is restricted to particular stations such as ESPN, ESPN2, FSN, FS1, FS2 (occasionally), and BTN. My sense is that the a la carte options we have long dreamed of, are in some ways here. Unfortunately, it is not quite the correct vision. Most a la carte options are still tied to particular channels or suites of channels rather than individual programs. And it is still cheaper to purchase those channels via a streaming service than to buy the episodes of the shows I like.\nMore options open up for streaming TV (Apple TV, Disney) everyday. In addition to Hulu Live, Youtube TV, Sling, and Playstation Vue (the options when we looked in 2017), there are many more: Apple TV, Disney, fubo, CBS, etc. The main differentiator seems to be channel selection, so it seems relevant to look at the network that produce and air the shows I like. Below I create a similar donut chart for my TV data, this time by network, since 2017.\n\n\n`summarise()` has grouped output by 'Network'. You can override using the\n`.groups` argument.\n`summarise()` has grouped output by 'Network'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\nTV shows Andy watched in 2018 and 2019 (as of October 30).\n\n\nIn looking at these plots, I must say that I watch more network TV than I thought I did (especially on CBS, which anecdotally I thought produced garbage). Although, many of the network shows were older shows that I watched on Netflix and Amazon Prime. The bevy of foreign networks each was primarily associated with a single show and more often than not were aired on Netflix. I am also starting to wonder about the cost-effectiveness for keeping HBO and Showtime, although my wife watches a lot of shows on those channels, so it is probably worth it.\nMany of the network programs are available with regular Hulu, and outside of sports we could probably dump Hulu Live. But, to paraphrase the great comedian Steve Martin, sports is like the sun and a day without sun is like…well…night."
  },
  {
    "objectID": "posts/2020-02-27-better-research-poster/2020-02-27-better-research-poster.html",
    "href": "posts/2020-02-27-better-research-poster/2020-02-27-better-research-poster.html",
    "title": "Better Research Poster",
    "section": "",
    "text": "./assets/body-header.qmd\nOur department has an annual Graduate Student Research Day (GSRD) in which students present their research for the rest of the department. Although there are five speakers, one from each program in the department, the majority of students participate in a poster session. This year my student Chelsey Legacy put together a poster to present some of the data we collected from a fall administration of the Statistics Teaching Inventory.\nOne of our research team members (Laura Le) suggested we use the “Better Research Poster” format. I was not familiar with this format, but a little Googling quickly brought up several articles (e.g., this Inside Higher Ed article), and tweets (#betterposter). Here is the example from the Inside Higher Ed article:\nThe idea behind this according to its originator, Mike Morrison, is that it quickly allows a poster session attendee to see the main finding, which is printed prominently in large type and in plain language. After reading about this, our team agreed to give it a try."
  },
  {
    "objectID": "posts/2020-02-27-better-research-poster/2020-02-27-better-research-poster.html#our-poster-and-experience",
    "href": "posts/2020-02-27-better-research-poster/2020-02-27-better-research-poster.html#our-poster-and-experience",
    "title": "Better Research Poster",
    "section": "Our Poster and Experience",
    "text": "Our Poster and Experience\nThe first thing that is plainly obvious is that the amount of text on one of these posters needs to be vastly reduced from a conventional research poster. For us this meant a lot of thinking about what was necessary to include in terms of introduction/rationale, methods, results, and discussion. We went through several iterations of rewrites, each time really trying to distill things down to what was essential. It is said brevity is the soul of wit; it is also the key to making one of these posters. Below is a screenshot of our final poster.\n\nAfter a couple drafts of the text, we made some design decisions for the poster. First, we opted to make the main finding area a bit smaller than the example poster. This gave us more room for the content. Secondly, we moved our results (which took up the most space) to the bottom of the poster. Initially we had the results on the right-side similar to the example poster, but the layout with the results moved to the bottom felt better both functionally (all the text is aligned at the top of the poster and can be read left-to-right) and aesthetically. Lastly, we removed all of the references from the poster itself (which removed a lot of text) and put them on a website that can be accessed from the QR code displayed on the poster.\nOne thing that came from trying different layouts and having to be cognizant of the amount of poster space was that we reduced the presented results to a series of six plots. We also included a short caption for each figure to help poster viewers navigate the findings. These captions were also re-edited many times before we settled on our final prose."
  },
  {
    "objectID": "posts/2020-02-27-better-research-poster/2020-02-27-better-research-poster.html#creating-the-poster",
    "href": "posts/2020-02-27-better-research-poster/2020-02-27-better-research-poster.html#creating-the-poster",
    "title": "Better Research Poster",
    "section": "Creating the Poster",
    "text": "Creating the Poster\nI ultimately used Apple’s Keynote to create our poster. In retrospect, I would have perhaps been better off using Pages or a more dedicated layout program. The big advantage in these programs is the use of guides and grids to align text boxes, margins, etc. I used a custom slide size and set the dimensions for the poster to 42” x 36” (or 3024 pixels x 2592 pixels).\nAs someone who thinks about layout and design (I love typography and book design!), I experimented with several different fonts. I wanted a font that was clean and readable. In our poster I settled on Lato which is a sans serif font with super clean lines. I also decided to use the same font for the headings, which we bolded and set in uppercase letters. (Google Fonts is a great site for seeing how different fonts pair together if you want to use differing fonts for body text and headings.) The size and spacing for the body text and headings were set by trial-and-error after we had finalized the text on the poster until we got something that looked good, filled the space appropriately.\nAfter deciding on the background color for the main finding, we used complimentary color palettes in the plots we created. This was tricky as the color palettes for the plots also need to reflect the measurement scale of the data For example, the colors used to display the likert scale data needed to reflect the ordinality of the responses; lighter hues used for less positive values and darker hues used for more positive values. We also needed to be sure that the palettes were color-blind friendly. We used this simulator to test the palettes on the final images.\nWe generated a QR code from this site that linked to the website where we posted the references and a PDF copy of the poster. One other important caveat for printing posters is that your images need to be pretty high resolution to avoid pixelation when they print. We converted all of our images to at least 300dpi using Apple’s Preview application. You could pretty much use any imaging application for this (e.g., ImageMagick, Inkscape, MS Paint)."
  },
  {
    "objectID": "posts/2020-02-27-better-research-poster/2020-02-27-better-research-poster.html#resources",
    "href": "posts/2020-02-27-better-research-poster/2020-02-27-better-research-poster.html#resources",
    "title": "Better Research Poster",
    "section": "Resources",
    "text": "Resources\nBelow is the Keynote file I used to create our poster. Feel free to use or adapt it. I also exported this to PowerPoint, but the Lato font seems to generate slightly differently on the PPTX slide.\n\nKeynote file for our poster\nPowerPoint file for our poster\n\nHere are some other resources that may be helpful:\n\nMike Morrison’s Better Scientific Poster repository on OSF\nButterick’s Practical Typography: A free online book that presents some of the basic ideas of fonts and typography.\nQR Code Generator: Create and format a QR code that links to whatever URL or file you want.\nColor Blindness Simulator: Simulate what the colors in an image look like for people with many different types of color-blindness.\nSpoonflower: This place will print your poster on fabric and ship it to you for a great price relative to many other places.\nList of Fee Fonts: The fonts we used may not be acceptable for commerical use, and you may want to find more open/free fonts. This is but one of many. Google Fonts also has many fonts available for commerical use. Thank you to Brianna Miller for bringing this to my attention."
  },
  {
    "objectID": "posts/2018-04-02-epsy-5271-part-03/2018-04-02-epsy-5271-part-03.html",
    "href": "posts/2018-04-02-epsy-5271-part-03/2018-04-02-epsy-5271-part-03.html",
    "title": "Q&A with Becoming a Teacher of Statistics Class: Part III",
    "section": "",
    "text": "./assets/body-header.qmd\nThis post is the third in a series of blogposts in which I respond to questions from the students in the Becoming a Teacher of Statistics course. In today’s posting I respond to questions related to teaching.\n\nWith a flipped classroom, the professor tapes their lecture and has students view the video online. I think the logical extreme of this is that at some point, certain lectures will become immensely popular or polished to the point of surpassing local professors’ lectures. Could you see the flipped classroom evolving to the point of removing the need for local professors to record themselves, given certain excellent online lectures?\nPotentially. Although I hope not. In my experience, not every class emphasizes the same thing, so while there are some concepts that are consistent across courses, the “local” professors bring something unique to the class they are teaching. Maybe it is this variation that is important for the professor’s to capture, rather than re-inventing the wheel of trying to give a lecture on a common topic like OLS estimation, say. If professors could focus on how to build-on these pre-canned lectures, rather than spending time and effort re-inventing them, it seems like a better use of resources.\nTo play devil’s advocate for a minute, how do we know when we have hit the apex in video lectures? Is popularity a signifier of consenus of greatness? (Counterpoint: As of this writing, @realDonaldTrump has 49.8 million followers on Twitter.) Or is it that we come to this conclusion once a really great statistician has produced a video on a particular topic (e.g., Hastie and Tibshirani’s lectures on Machine Learning)?\nThis was tried in the early 1960’s for introductiry statistics, when the Continental Classroom broadcast a course in probability and statistics on NBC taught by Frederick Mosteller. They also developed a textbook and other materials to accompany the course. Students received credit by enrolling in the course through a local college or university, and successfully completing the course examinations, which were mailed out to participating institutions. Aside from instant name recognition, this course was also immensely popular; more than 75,000 students took the probability and statistics course for credit at 320 colleges—which was broadcast between 6:00–7:00AM. (About 1.2 million people viewed the course but did not take it for credit.)\nSixty-plus years later, many people have probably never heard of the Continental Classroom, or haven’t thought about it in many decades. The content (which you can view at the American Statistical Association’s headquarters in Alexandria, VA) is, not surprisingly, out of date. In the Twitter age things go out of date quickly. Even when attempts to update content are made, it is still hard. There was a great series of approximately 30 videos recorded in 1989, Against All Odds: Inside Statistics, that introduced topics in statistics. (These loosely corresponded with topics in David Moore’s Introduction to the Practice of Statistics textbook.) By the early 2000’s these seemed quite dated. Although the videos were updated in 2013, it is unclear how much the content changed. (Why are we still worrying about creating stem-and-leaf plots in 2018!??)\n\n\nIn your CATALST paper written with Huberty you mention that after school computer labs can be a potential improvement for the CSI courses you installed in Minnesota schools. Since it’s been a couple years, have you seen any schools take you up on this idea? I would agree that more one-on-one time with a teacher may be more beneficial for shyer students. Especially for those in the beginning of the course that have the preconceived notion that they “suck at mathematics and therefore will not do as well as in your course”.\nI think many of the teachers make sure time is available after school for students to work on the computers with TinkerPlots™. What I don’t know is how many students take advantage of it. Many of them are involved in after-school activities, or have other obligations that keep them from making it into the lab after school.\nI think what tends to be more important than the one-on-one time is the pedagogy used in the classroom. The educational research overwhelmingly suggests that pedagogy matters for student learning, and that students in student-centered classrooms outperform their counterparts in lecture-based courses. (This is especilly true for minority and disadvantaged students; see here.)\n\n\nDo you change your material and/or approach when teaching intro stats to high schoolers (AP or College in the Schools, for example) versus teaching it to undergrads versus grad students?\nGreat question. Let me preface my answer with saying that although I taught high school for four years, this was around 1998 (the Backstreet Boys and N’Sync were big) and I haven’t really taught that age-level since.\nFrom working with high school teachers, and from some recollection, the big difference is that when teaching a high school class, you have far more time than we do at the university/college level. High school courses tend to meet every day, while we only meet twice a week. This means you can allow more time for the concepts and skills to develop when teaching a high school course.\nThe assessments are also generally far less high-stakes than they are at the college level. In college you may have one or two exams that make up almost all of your grade. In high school, it is not uncommon to have many assessment opportunities throughout a course.\nSo, material-wise, I would employ many of the same materials in the high school course that we do in the college course. But, I would supplement these with other activities and materials that build-on the ideas and give students opportunities to practice what they’ve learned.\nPedagogically, I would try to approach things the same way, including discussions and cooperative learning. I believe that learning takes hard work and it is the student’s responsibility. I can facilitate that learning, but it is ultimately up to the student to take on that hard work. The brain is a muscle that needs to be worked out. Like physical work outs, you need to build-up to larger “weights”. Focusing on the process, not the product, is also important as the goal (learning) is a long-term outcome. It may be harder to convinve a high school student that this process, which takes time, is worthwhile."
  },
  {
    "objectID": "posts/2019-06-05-higher-education-in-minnesota/2019-06-05-higher-education-in-minnesota.html",
    "href": "posts/2019-06-05-higher-education-in-minnesota/2019-06-05-higher-education-in-minnesota.html",
    "title": "Higher Education in Minnesota",
    "section": "",
    "text": "./assets/body-header.qmdI was recently perusing a book from 1960, Minnesota Heritage: A Panoramic Narrative of the Historical Development of the North Star State and came across the following map showing the locations of the colleges and universities in the state at the time.\n\n\n\n\n\nMinnesota Colleges and Universities in 1960\n\n\n\n\n\nThe text referring to the map made an inference about the accessibility to higher education,\n\nAt a glance the map shows, these facilities for higher education are quite uneveny distributed. Fourteen institutions, including four public junior colleges and the University, with its major campuses at Minneapolis, St. Paul, and Duluth, are concentrated in Hennepin, Ramsey, and St. Lousi counties, all in the eastern half of the state. The eighteen remaining colleges are scattered among twelve counties. The entire western half of the state has only four colleges, two in the same city.\n\nI wondered if that distribution had changed since 1960, so I collected the 2018 location data on colleges and universities in Minnesota. This is available as a CSV file here. I then used ggplot() to create a map of these locations.\n\nlibrary(dplyr)\nlibrary(geosphere)\nlibrary(ggplot2)\nlibrary(ggmap)\nlibrary(ggrepel)\nlibrary(ggthemes)\nlibrary(readr)\n\n# Read in college data\ncolleges = read_csv(\"https://raw.githubusercontent.com/zief0002/Public-Stuff/master/data/mn-colleges-geo.csv\")\nhead(colleges)\n\n# A tibble: 6 × 6\n  name                     type    level          city          lat  long\n  &lt;chr&gt;                    &lt;chr&gt;   &lt;chr&gt;          &lt;chr&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 Augsburg University      Private University     Minneapolis  45.0 -93.2\n2 Bemidji State University Public  University     Bemidji      47.5 -94.9\n3 Bethany Lutheran College Private Junior College Mankato      44.2 -94.0\n4 Bethel University        Private University     Saint Paul   45.1 -93.2\n5 Carleton College         Private College        Northfield   44.5 -93.2\n6 Central Lakes College    Public  Junior College Brainerd     46.3 -94.2\n\n# Reorder the institution level\ncolleges = colleges %&gt;%\n  mutate(\n    level = factor(level, levels = c(\"Junior College\", \"College\", \"University\"))\n  )\n\n\nTo mimic the 35-mile circles on the map I adapted code from https://egallic.fr/en/maps-with-r/.\n\n# --\n# Given the long/lat coordinates of an origin (x) and a radius (radius) in km,\n# returns the coordinates of 360 points on the circle of center x and radius radius km.\n# --\n# x (numeric vector) : coordinates of the origin of the circle\n# radius (numeric) : radius of the circle\n# http://egallic.fr/en/maps-with-r/\n# --\n\ndistantCircle = function(x, radius) {\n  # Creation de 360 points distincts sur le cercle de centre\n  # x et de rayon radius\n  resul = do.call(\"rbind\", lapply(0:360, function(bearing) {\n    res = destPoint(p = x, b = bearing, d = radius)\n    rownames(res) = NULL\n    return(data.frame(res))\n  }))\n  resul$dist = radius / 1000\n  return(resul)\n}\n\n# Store circle data along with college for easier filtering\nn = nrow(colleges)\ncircles = list(NA, n)\n\nfor(i in 1:n){\n  circles[[i]] = data.frame(\n    distantCircle(x = c(colleges$long[i], colleges$lat[i]), radius = 35*1000)\n    )\n  circles[[i]]$name = colleges$name[i]\n}\n\ncircles = do.call(rbind, circles)\n\ncircles = circles %&gt;%\n  left_join(colleges[c(1, 2, 3)], by = \"name\")\n\n\nFinally, I created the map.\n\n# Get MN map info\nmn = map_data(\"state\", region = \"Minnesota\")\n\n## Plot map\nggplot(data = mn, aes(x = long, y = lat)) +\n  geom_polygon(aes(group = group), fill = \"white\", color = \"black\") +\n  geom_polygon(data = circles, aes(x = lon, y = lat, group = name), linetype = 1, \n               fill = \"skyblue\", alpha = 0.3) +\n  geom_point(data = colleges, size = 3) +\n  theme_void() +\n  coord_map(projection = \"mercator\") +\n  scale_shape_manual(name = \"\", values = c(18, 16, 17, 15, 13))\n\n\n\n\nLocations of the 44 Minnesota colleges and universities c.2018.\n\n\n\n\n\nThe original map also indicated the type of college and whether it was publicly or privately funded.\n\nggplot(data = mn, aes(x = long, y = lat)) +\n  geom_polygon(aes(group = group), fill = \"white\", color = \"black\") +\n  geom_polygon(data = circles, aes(x = lon, y = lat, group = name, fill = type), \n               linetype = 1, alpha = 0.3) +\n  geom_point(data = colleges, aes(shape = level, fill = type), size = 3) +\n  theme_void() +\n  coord_map(projection = \"mercator\") +\n  scale_shape_manual(name = \"\", values = c(23, 21, 24)) +\n  ggsci::scale_fill_d3(name = \"\") +\n  facet_wrap(~type) +\n  guides(fill = FALSE)\n\nWarning: The `&lt;scale&gt;` argument of `guides()` cannot be `FALSE`. Use \"none\" instead as\nof ggplot2 3.3.4.\n\n\n\n\n\n\nHere is another variation of the map facetting on level.\n\nggplot(data = mn, aes(x = long, y = lat)) +\n  geom_polygon(aes(group = group), fill = \"white\", color = \"black\") +\n  geom_polygon(data = circles, aes(x = lon, y = lat, group = name, fill = type), \n               linetype = 1, alpha = 0.3) +\n  geom_point(data = colleges, aes(shape = level, fill = type), size = 3) +\n  theme_void() +\n  coord_map(projection = \"mercator\") +\n  scale_shape_manual(name = \"\", values = c(23, 21, 24)) +\n  ggsci::scale_fill_d3(name = \"\") +\n  facet_wrap(~level) +\n  guides(shape = FALSE, color = FALSE)\n\n\n\n\n\nWhile the locations are still typically clustered on the eastern half of the state, there are several more options on the western side of the state in 2018. This is likely due to the population density of the state (see below). Moreover there are public options throughout the state.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMinnesota population density as of the 2000 census. Accessed from https://www.gis.leg.mn/html/maps/population.html"
  },
  {
    "objectID": "posts/2018-03-08-boys-hockey/2018-03-08-boys-hockey.html",
    "href": "posts/2018-03-08-boys-hockey/2018-03-08-boys-hockey.html",
    "title": "Minnesota State High School Boys Hockey Predictions (Updated Quarterfinals)",
    "section": "",
    "text": "./assets/body-header.qmd\nIn a previous post, I used Monte Carlo simulation to predict the winner of the 2018 Minnesota State High School Boys Hockey tournament. Now that the quarterfinal games have been played, I thought I would update my predictions. The process for this is to:\nI simulated the Class A state tournament 10,000 times using the same process as described in my previous post."
  },
  {
    "objectID": "posts/2018-03-08-boys-hockey/2018-03-08-boys-hockey.html#class-a-tournament",
    "href": "posts/2018-03-08-boys-hockey/2018-03-08-boys-hockey.html#class-a-tournament",
    "title": "Minnesota State High School Boys Hockey Predictions (Updated Quarterfinals)",
    "section": "Class A Tournament",
    "text": "Class A Tournament\n\n\n\nOriginal (at beginning of tournament) and updated Elo ratings (after the quartefinal games) for the eight Class A teams that qualified for the state tournament.\n\n\nTeam\nOriginal Elo\nUpdated Elo\n\n\n\n\nHermantown\n1639.196\n1657.756\n\n\nOrono\n1624.538\n1624.505\n\n\nMahtomedi\n1609.513\n1619.517\n\n\nAlexandria\n1587.179\n1571.723\n\n\nMonticello\n1571.198\n1569.096\n\n\nThief River Falls\n1562.803\n1568.797\n\n\nMankato East\n1551.141\n1549.453\n\n\nLitchfield/Dassel-Cokato\n1530.773\n1515.494\n\n\n\n\n\n\n# Enter teams in rank order\nteam_1 = \"Hermantown\"\nteam_2 = \"Mahtomedi\"\nteam_3 = \"Orono\"\nteam_4 = \"Alexandria\"\n\n\n# Set up empty vector to store winner in\nchampion = rep(NA, 10000)\n\n\nfor(i in 1:10000){\n  \n  ### SIMULATE THE SEMIFINALS\n  \n  # Predict Game 1 winner: team_1 vs. team_4\n  p_game_1 = predict(elo_reg_season, data.frame(home = team_1, visitor = team_4))\n  w_game_1 = ifelse(runif(1, min = 0, max = 1) &lt;= p_game_1, team_1, team_4)\n  \n  # Predict Game 2 winner: team_2 vs. team_3\n  p_game_2 = predict(elo_reg_season, data.frame(home = team_2, visitor = team_3))\n  w_game_2 = ifelse(runif(1, min = 0, max = 1) &lt;= p_game_2, team_2, team_3)\n  \n  \n  ### SIMULATE THE FINALS\n  \n  # Predict Game 3 winner: winner Game 1 vs. winner Game 2\n  p_game_3 = predict(elo_reg_season, data.frame(home = w_game_1, visitor = w_game_2))\n  w_game_3 = ifelse(runif(1, min = 0, max = 1) &lt;= p_game_3, w_game_1, w_game_2)\n  \n  \n  champion[i] = w_game_3\n  \n}\n\n\n\n\nProbability that each of the eight Class A teams will win the state tournament.\n\n\nTeam\nOriginal Probability\nUpdated Probability\n\n\n\n\nHermantown\n0.1987\n0.3214\n\n\nOrono\n0.1802\n0.2671\n\n\nMahtomedi\n0.1127\n0.2415\n\n\nAlexandria\n0.1086\n0.1700\n\n\nMonticello\n0.1127\n0.0000\n\n\nThief River Falls\n0.0939\n0.0000\n\n\nMankato East\n0.0812\n0.0000\n\n\nLitchfield/Dassel-Cokato\n0.0635\n0.0000\n\n\n\n\n\nBased on these simulations, Hermantown is still the favorite, and Mahtomedi and Orono also have a chance of winning the Class A tournament."
  },
  {
    "objectID": "posts/2018-03-08-boys-hockey/2018-03-08-boys-hockey.html#class-aa-tournament",
    "href": "posts/2018-03-08-boys-hockey/2018-03-08-boys-hockey.html#class-aa-tournament",
    "title": "Minnesota State High School Boys Hockey Predictions (Updated Quarterfinals)",
    "section": "Class AA Tournament",
    "text": "Class AA Tournament\n\n\n\nElo ratings and rankings for the eight Class AA teams that qualified for the state tournament.\n\n\nTeam\nOriginal Elo\nUpdated Elo\n\n\n\n\nEdina\n1718.589\n1741.221\n\n\nMinnetonka\n1716.570\n1711.781\n\n\nSt. Thomas Academy\n1691.315\n1710.036\n\n\nDuluth East\n1693.757\n1692.917\n\n\nSTMA\n1655.561\n1643.501\n\n\nCentennial\n1625.291\n1618.199\n\n\nLakeville North\n1578.414\n1569.408\n\n\nHill-Murray\n1557.501\n1549.936\n\n\n\n\n\n\n# Enter teams in rank order\nteam_1 = \"Minnetonka\"\nteam_2 = \"Edina\"\nteam_3 = \"Duluth East\"\nteam_4 = \"Centennial\"\n\n\n# Set up empty vector to store winner in\nchampion = rep(NA, 10000)\n\n\nfor(i in 1:10000){\n  \n  ### SIMULATE THE SEMIFINALS\n  \n  # Predict Game 1 winner: team_1 vs. team_4\n  p_game_1 = predict(elo_reg_season, data.frame(home = team_1, visitor = team_4))\n  w_game_1 = ifelse(runif(1, min = 0, max = 1) &lt;= p_game_1, team_1, team_4)\n  \n  # Predict Game 2 winner: team_2 vs. team_3\n  p_game_2 = predict(elo_reg_season, data.frame(home = team_2, visitor = team_3))\n  w_game_2 = ifelse(runif(1, min = 0, max = 1) &lt;= p_game_2, team_2, team_3)\n  \n  \n  ### SIMULATE THE FINALS\n  \n  # Predict Game 3 winner: winner Game 1 vs. winner Game 2\n  p_game_3 = predict(elo_reg_season, data.frame(home = w_game_1, visitor = w_game_2))\n  w_game_3 = ifelse(runif(1, min = 0, max = 1) &lt;= p_game_3, w_game_1, w_game_2)\n  \n  \n  champion[i] = w_game_3\n  \n}\n\n\n\n\nProbability that each of the eight Class AA teams will win the state tournament.\n\n\nTeam\nOriginal Probability\nUpdated Probability\n\n\n\n\nEdina\n0.2083\n0.3113\n\n\nMinnetonka\n0.1852\n0.2901\n\n\nSt. Thomas Academy\n0.1495\n0.2487\n\n\nDuluth East\n0.0701\n0.1499\n\n\nSTMA\n0.2098\n0.0000\n\n\nCentennial\n0.1016\n0.0000\n\n\nLakeville North\n0.0416\n0.0000\n\n\nHill-Murray\n0.0339\n0.0000\n\n\n\n\n\nSt. Thomas Academy’s loss to Centennial really shook things up. Edina and Minnetonka are now the favorites in the Class AA tournament, with Duluth East a not so distant third."
  },
  {
    "objectID": "posts/2020-01-09-apa-tables-using-rmarkdown/2020-01-09-apa-tables-using-rmarkdown.html",
    "href": "posts/2020-01-09-apa-tables-using-rmarkdown/2020-01-09-apa-tables-using-rmarkdown.html",
    "title": "APA Tables Using RMarkdown",
    "section": "",
    "text": "./assets/body-header.qmd\nOften it is useful to format table output to make it look good or to adhere a particular style (e.g., APA). There are several packages that help in this endeavor when working in an Rmarkdown document. When I create tables using RMarkdown, the primary packages I use are:\n\nThe kable() function from the knitr package; and\nFunctions from the kableExtra package.\n\nThere are many packages for formatting tables, among others, include the flextable package, the gt package, the huxtable package, and the expss package. I have tried all of these and many more. I settled on knitr because of its robustness and ease of use—along with kableExtra, it can create almost any table I can imagine using a general set of syntax.\nI also use LaTeX and PDF output when I need to really format using the APA style. My experience with HTML and APA is that I can get really close, but there are one or two elements that I can’t seem to get formatted correctly, which varies depending on the package I try. The closest I get with HTML (without using bookdown) seems to be the flextable package. It may also be worth mentioning the papaja package package. This can set up an entire RMarkdown document to adhere to APA formatting, but is beyond the scope of what I typically need.\nTo illustrate my process for table creation using LaTeX, I will attempt to re-create the following table from the 7th edition of the APA Publication Manual.\n\n\n\nTable 7.6 from the 7th edition of the APA Publication Manual (p. 213).\n\n\n\nMy Process\nTo begin I will load a few packages.\n\n# Load libraries\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(tidyverse)\n\nThe workhorse function from the knitr package for table creation is the kable() function, and the primary input to the kable() function is a data frame. Below I will set up a data frame to mimic the content from our goal table.\n\ntab_01 = data.frame(\n  scale = c(\"BAS-T\", \"SR\", \"BDI\", \"ASRM\", \"M-SRM\"),\n  high = c(\"46.17 (2.87)\", \"17.94 (1.88)\", \"7.11 (6.50)\", \n           \"6.46 (4.01)\", \"11.05 (3.36)\"),\n  moderate = c(\"37.99 (1.32)\", \"11.52 (1.84)\", \"6.18 (6.09)\", \n               \"5.63 (3.69)\", \"11.76 (2.75)\"),\n  p = c(\"&lt;.001\", \"&lt;.001\", \".254\", \".109\", \".078\")\n)\n\nAlthough I simply entered the data values, this data frame could also be created as the ouput of one or more functions.\nWe can now use the kable() function to rename the columns, round numeric values (which we don’t have since I entered the numbers as quoted text), set alignment for each column, set a caption, etc.\n\nkable(\n  tab_01,\n  format = \"latex\",\n  booktabs = TRUE,\n  col.names = c(\"Scale\", \"High BAS group\", \"Moderate BAS group\", \"p\"),\n  align = c(\"l\", \"c\", \"c\", \"c\"),\n  caption = \"Means and Standard Deviations of Scores on Baseline Measures\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis gets us 90% of the way there. There are a few things to do to make this ready for primetime. First we will italicize the “p” in the header row. To do this we need to use some LaTeX commands. So that it compiles correctly we will also add the argument escape=FALSE into the kable() function.\n\nkable(\n  tab_01,\n  format = \"latex\",\n  booktabs = TRUE,\n  escape = FALSE,\n  col.names = c(\"Scale\", \"High BAS group\", \"Moderate BAS group\", \"\\\\textit{p}\"),\n  align = c(\"l\", \"c\", \"c\", \"c\"),\n  caption = \"Means and Standard Deviations of Scores on Baseline Measures\"\n  )\n\n\nTo further format the table we will use functionality from the kableExtra package. To do this we pipe the kable() output into the functions from the kableExtra package. Here I center all of the column headings using the rowspec() function and also style the table to be full page width.\n\nkable(\n  tab_01,\n  format = \"latex\",\n  booktabs = TRUE,\n  escape = FALSE,\n  col.names = c(\"Scale\", \"High BAS group\", \"Moderate BAS group\", \"\\\\textit{p}\"),\n  align = c(\"l\", \"c\", \"c\", \"c\"),\n  caption = \"Means and Standard Deviations of Scores on Baseline Measures\"\n  ) %&gt;%\n  row_spec(row = 0, align = \"c\") %&gt;%\n  kable_styling(full_width = TRUE)\n\n\nNext we will add the footnote using the footnote() function from kableExtra. The different arguments I use here are:\n\ngeneral_title= Allows you to change the default footnote title from Note: to Note.\ngeneral= The actual text of the footnote\nthreeparttable=TRUE will force the width of caption and footnotes be the width of the original table. It’s useful when you have long paragraph of footnotes.\nfootnote_as_chunk=TRUE sets the footnote to printed in a chunk (without line break after Note.).\n\nTo make the threeparttable= argument work we also need to add longtable=TRUE into the kable() function.\n\nkable(\n  tab_01,\n  format = \"latex\",\n  booktabs = TRUE,\n  escape = FALSE,\n  longtable = TRUE,\n  col.names = c(\"Scale\", \"High BAS group\", \"Moderate BAS group\", \"\\\\textit{p}\"),\n  align = c(\"l\", \"c\", \"c\", \"c\"),\n  caption = \"Means and Standard Deviations of Scores on Baseline Measures\"\n  ) %&gt;%\n  row_spec(row = 0, align = \"c\") %&gt;%\n  kable_styling(full_width = TRUE) %&gt;%\n  footnote(\n    general_title = \"Note.\",\n    general = \"Standard deviations are presented in parentheses. BAS = Behavioral Activation System; BAS-T = Behavioral Activation System-Total sores from the Behavioral Inhibition System/Behavioral Activation System Scales; SR = Sensitivity to Reward scores from the Sensitivity to Punishment and Sensitivity to Reward Questionnaire; BDI = Beck Depression Inventory scores; ASRM = Altman Self-Rating Mania Scale scores; M-SRM = Modified Social Rhythm Metric Regularity scores.\",\n    threeparttable = TRUE,\n    footnote_as_chunk = TRUE\n    )\n\n\nLastly we need to adjust the captioning. This requires us to use the caption package from LaTeX. (Note: This is not an R package!) We load the caption package in the YAML of our RMarkdown document. If you are using tinytex this package will install automatically if you do not have it. Otherwise, you may need to install this LaTeX package. We add the header-include: field into our YAML and then use the \\usepackage{} LaTeX function to load the caption package. Here is what this will look like:\n\n---\ntitle: \"Untitled\"\nauthor: \"Andrew Zieffler\"\ndate: \"1/10/2020\"\noutput: pdf_document\nheader-includes:\n   - \\usepackage{caption}\n---   \n\nThe caption package includes the captionsetup{} function. To format the caption using the APA formatting we need:\n\nTable caption in italics\nTable numbering in boldface\nLine separating the table number and caption\n\nThis can be included in the YAML as well:\n\n---\ntitle: \"Untitled\"\nauthor: \"Andrew Zieffler\"\ndate: \"1/10/2020\"\noutput: pdf_document\nheader-includes:\n   - \\usepackage{caption}\n   - \\captionsetup[table]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=newline}\n---\n\nFor more options see here. You can also set up your figure captioning in a similar way; use figure instead of `table’ between the square brackets.\n\nShazaam! We have a nice looking table that is formatted to APA style."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Andrew Zieffler",
    "section": "",
    "text": "Andrew Zieffler\nAcademic. Data lover. Statistics enthusiast.\n\n\n\n\n\n\nLocation, email, what-not\n178 EdSciB 56 East River Road Minneapolis, MN 55455"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Andrew Zieffler",
    "section": "",
    "text": "Andrew Zieffler\nAcademic. Data lover. Statistics enthusiast.\n\n\n\n\n\n\n\nTeaching\nI teach (have taught) the following courses at the University of Minnesota. Some of my old course webpages can be found here, and many of the materials are available in various repositories in my Github.\n\nEPsy 1261: Understanding Data Stories through Visualization and Computing\nEPsy 3264: Basic and Applied Statistics\nEPsy 5244: Survey Design, Sampling, and Implementation\nEPsy 5261: Introductory Statistical Methods\nEPsy 8220: Methods for Categorical Response Data in Educational Research\nEPsy 8251: Methods in Data Analysis for Educational Research I\nEPsy 8252: Methods in Data Analysis for Educational Research II\nEPsy 8261: Statistical Methods I: Probability and Inference\nEPsy 8262: Statistical Methods II: Regression and The General Linear Model\nEPsy 8264: Advanced Multiple Regression\nEPsy 8282: Statistical Analysis of Longitudinal Data I"
  },
  {
    "objectID": "posts/2019-12-06-reproducibilitea-reading-group/2019-12-06-reproducibilitea-reading-group.html",
    "href": "posts/2019-12-06-reproducibilitea-reading-group/2019-12-06-reproducibilitea-reading-group.html",
    "title": "ReproducibiliTEA Journal Club",
    "section": "",
    "text": "./assets/body-header.qmdThis semester I took part in the University of Minnesota’s ReproducibiliTEA Journal Club. ReprodicibiliTEA is “a grassroots journal club initiative that helps young researchers create local Open Science journal clubs at their universities to discuss diverse issues, papers and ideas about improving science, reproducibility and the Open Science movement.” (Read more at https://reproducibilitea.org/).\n\n\n\nUMN ReproducibiliTEA Chapter (Fall 2019)\n\n\nThe University of Minnesota chapter was sponsored by the Minnesota Center for Philosophy of Science (which meant we had food, coffee, and of course, tea each meeting) and followed the model from clubs in UK and Europe described on OSF. We met once a month and read a wonderful set of articles this semester.\n\nSeptember 19: Fidler, F., & Wilcox, J. (2018). Reproducibility of scientific results. In E. N. Zalta (ed.), The Stanford Encyclopedia of Philosophy (Winter edition).\nSeptember 19: Ioannidis, J. P. (2005). Why most published research findings are false. PLoS medicine, 2(8), e124. doi: 10.1371/journal.pmed.0020124\nOctober 17: Smaldino, P. E., & McElreath, R. (2016). The natural selection of bad science. Royal Society Open Science, 3(9), 160384. doi: 10.1098/rsos.160384\nOctober 31: Nosek, B. A., & Bar-Anan, Y. (2012). Scientific utopia: I. Opening scientific communication. Psychological Inquiry, 23(3), 217-243. doi: 10.1080/1047840X.2012.692215\nNovember 14: John, L. K., Loewenstein, G., & Prelec, D. (2012). Measuring the prevalence of questionable research practices with incentives for truth telling. Psychological science, 23(5), 524–532. doi: 10.1177/0956797611430953\nDecember 5: Munafò, M. R., et al. (2017). A manifesto for reproducible science. Nature Human Behaviour, 1(1), 0021. doi: 10.1038/s41562-016-0021\n\nThe participants were all fantastic and I learned a great deal. We are hoping to meet spring semester as well. Feel free to join us. For more information, or to join the email list, contact Amy Riegelman (aspringe@umn.edu) or Alan Love (aclove@umn.edu)."
  },
  {
    "objectID": "posts/2018-06-03-statistical-society-of-canada/2018-06-03-statistical-society-of-canada.html",
    "href": "posts/2018-06-03-statistical-society-of-canada/2018-06-03-statistical-society-of-canada.html",
    "title": "Computing Talk at SSC 2018",
    "section": "",
    "text": "./assets/body-header.qmdI am giving a talk at the 46th Annual Meeting of the Statistical Society of Canada in Montreal on June 05, 2018. The talk is part of an invited session on Teaching Statistics to Graduate Students in the Health and Social Sciences. Information, including the slides, is available below.\nTitle: Statistical Computing: Non-Ignorable Missingness in the Graduate-Level Social Science Curriculum\nAbstract: In 2010, Nolan and Temple Lang pointed out that \"computational literacy and programming are as fundamental to statistical practice and research as mathematics\". Since that time computation has become an even more important skillset for researchers and scientists who use statistics. Many graduate-level statistics programs in the social sciences have yet to adopt statistical computing into the curriculum. Students either learn computing on their own or its teaching is relegated to specialty, often advanced, coursework. In either case, it is often only a small minority of quantitatively-focused students that are exposed to computing. The majority of graduate-level social science students, however, are not quantitatively focused. To what extent should they learn statistical computing? Which aspects of statistical computing should they be exposed to? In this talk, I explore these questions and offer some advice for teaching statistical computing to graduate-level social science students.\nSlides: Available at https://github.com/zief0002/website/raw/master/public/files/computing-talk-2018-05-07.pdf"
  },
  {
    "objectID": "posts/2019-05-02-student-awards/2019-05-02-student-awards.html",
    "href": "posts/2019-05-02-student-awards/2019-05-02-student-awards.html",
    "title": "Student Awards! So Proud!",
    "section": "",
    "text": "./assets/body-header.qmdOn April 26, 2019, two of my Statistics Education graduate students, Chelsey Legacy and Vimal Rao, were given awards during the annual Psychological Foundations and QME Awards and Recognition Ceremony.\n\nChelsey Legacy (first row; right) was awarded the Graduate Student Teaching Award for her incredible work in the EPsy 3264 classroom. Vimal Rao (second row; second from right) was awarded the Graduate Student Leadership Award for his work in building community. I am very proud of these students and these awards are an affirmation of their dedication and achievements in the program this year."
  },
  {
    "objectID": "posts/2020-01-12-apa-tables-using-rmarkdown-part-3/2020-01-12-apa-tables-using-rmarkdown-part-3.html",
    "href": "posts/2020-01-12-apa-tables-using-rmarkdown-part-3/2020-01-12-apa-tables-using-rmarkdown-part-3.html",
    "title": "APA Tables using RMarkdown: Part 3",
    "section": "",
    "text": "./assets/body-header.qmd\nThis is the third part of a short blog series I am writing to re-create some of the sample tables found in the 7th edition APA Publication Manual. In this post I will attempt to mimic Table 7.10 (p. 215). To do so, I will incorporate many ideas that I covered in the first and second of these posts.\n\n\n\nTable 7.10 from the 7th edition of the APA Publication Manual (p. 215).\n\n\nRather than re-create this table using the data from Table 7.10, I will illustrate mimicing this table with summary information gleaned from a different dataset.\n\nMy Process\nI will again render to PDF and set up the YAML to import the caption package (LaTeX) and set up the APA caption formatting.\n\n---\ntitle: \"Untitled\"\nauthor: \"Andrew Zieffler\"\ndate: \"1/12/2020\"\noutput: pdf_document\nheader-includes:\n   - \\usepackage{caption}\n   - \\captionsetup[table]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=newline}\n---\n\nIn the RMarkdown body, I will load a few packages and also import a data set that I will use to create the summary values akin to those in Table 7.10.\n\n# Load libraries\nlibrary(corrr)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(tidyverse)\n\n# Import data\nmn = read_csv(\"https://raw.githubusercontent.com/zief0002/rustic-penguin/main/data/mn-schools.csv\") |&gt;\n  mutate(public = if_else(sector == \"Public\", 1, 0))\n\n# View data\nhead(mn)\n\n# A tibble: 6 × 6\n  name                               grad sector    sat tuition public\n  &lt;chr&gt;                             &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 Augsburg College                   65.2 Private  1030    39.3      0\n2 Bethany Lutheran College           52.6 Private  1065    30.5      0\n3 Bethel University, Saint Paul, MN  73.3 Private  1145    39.4      0\n4 Carleton College                   92.6 Private  1400    54.3      0\n5 College of Saint Benedict          81.1 Private  1185    43.2      0\n6 Concordia College at Moorhead      69.4 Private  1145    36.6      0\n\n\nThe data in were collected from http://www.collegeresults.org and contain 2011 institutional data for n=33 Minnesota colleges and universities. The codebook is available here.\nWe need to compute the sample size, mean, and standard deviation for each of the four variables. I will compute these and store them in a data frame. We will also need the correlations between each set of variables. I will compute these and store them in a second data frame. Then I will combine these two into a single data frame to use with kable().\n\n# Compute n, M, and SD\ntab_01 = data.frame(\n  n = c(\n    sum(!is.na(mn$grad)), \n    sum(!is.na(mn$public)), \n    sum(!is.na(mn$sat)),\n    sum(!is.na(mn$tuition))\n    ),\n  M = c(\n    mean(mn$grad, na.rm = TRUE), \n    mean(mn$public, na.rm = TRUE), \n    mean(mn$sat, na.rm = TRUE),\n    mean(mn$tuition, na.rm = TRUE)\n    ),\n  SD = c(\n    sd(mn$grad, na.rm = TRUE), \n    sd(mn$public, na.rm = TRUE), \n    sd(mn$sat, na.rm = TRUE),\n    sd(mn$tuition, na.rm = TRUE)\n    )\n)\n\n# Compute correlations\ntab_02 = mn %&gt;%\n  select(grad, public, sat, tuition) %&gt;%\n  correlate() %&gt;%\n  shave(upper = TRUE) %&gt;%\n  fashion(decimals = 2, na_print = \"—\") \n\n# Combine information into single table\ntab_03 = cbind(tab_02[1], tab_01, tab_02[-1])\n\n# View table\ntab_03\n\n     term  n            M          SD grad public  sat tuition\n1    grad 33   60.9515152  16.7375723    —      —    —       —\n2  public 33    0.3030303   0.4666937 -.40      —    —       —\n3     sat 33 1101.2121212 111.4256360  .89   -.19    —       —\n4 tuition 33   32.2531818  11.3083707  .75   -.77  .61       —\n\n\nThe next thing I will do is to change the text in the rowname column to correspond to similar rownames in Table 7.10. Note the use of $^a$ and $^b$ to produce superscripts of “a” and “b” respectively. I will inlcude these in the footnote to define these variables.\n\ntab_03$rowname = c(\n  \"1. Graduation rate\", \n  \"2. Sector$^a$\", \n  \"3. Median SAT score\", \n  \"4. Tuition$^b$\"\n  )\n\nI will then pipe this into the kable() function to set the column names, column alignment, and table caption. The digits= argument is included to round the values in each column. (Since the first column is text we set this to NA.)\nI also employ similar kableExtra function from those introduced in Part 1 and and Part 2 to make the table the full page width, include the footnote, center the header names, and increase the width of the first column.\n\nkable(\n  tab_03,\n  format = \"latex\",\n  booktabs = TRUE,\n  escape = FALSE,\n  longtable = TRUE,\n  col.names = c(\"Variable\", \"$n$\", \"$M$\", \"$SD$\", \"1\", \"2\", \"3\", \"4\"),\n  align = c(\"l\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\"),\n  digits = c(NA, 0, 2, 2, 2, 2, 2, 2),\n  caption = \"Descriptive Statistics and Correlations for Study Variables\"\n  ) %&gt;%\n  kable_styling(full_width = TRUE) %&gt;%\n  footnote(\n    general_title = \"Note.\",\n    general = \"$^a$0 = private schools and 1 = public schools. $^b$Tuition is measured in thousands of dollars.\",\n    threeparttable = TRUE,\n    footnote_as_chunk = TRUE,\n    escape = FALSE\n    ) %&gt;%\n  row_spec(row = 0, align = \"c\") %&gt;%\n  column_spec(column = 1, width = \"1.5in\")\n\n\nTo alleviate the off-diagonal hyphens in the correlation part of the table we need to set the “-” value to blank text. We can do this with indexing.\n\ntab_03[1, 6:8] = \"\"\ntab_03[2, 7:8] = \"\"\ntab_03[3, 8] = \"\"\n\nThen we can re-run the kable() syntax.\n\n\n\np-Values and Stars\nI would not personally add the significance stars to the correlations in the table. This is inconsistent with what the American Statistical Association has outlined for good statistical practice.\nBut, if you insist on poor statistical decisions (he says snarkily) you can also use indexing to add the stars. The problem is that this turns numbers to text strings, so every other value in that column will also be a text string, and the digits= argument will no longer be able to round things.\nHere I use indexing to turn every value in the first column of correlations into text string. (The dash in the diagonal ) Then you could add the NA into the digits= argument of kable(). You would also need to add the appropriate text into the table note.\n\n# Change correlation values in first column\ntab_03[2, 5] = \"-.40$^{*}$\"\ntab_03[3, 5] = \".89$^{***}$\"\ntab_03[4, 5] = \".75$^{***}$\"\n\n#Create table\nkable(\n  tab_03,\n  format = \"latex\",\n  booktabs = TRUE,\n  escape = FALSE,\n  longtable = TRUE,\n  col.names = c(\"Variable\", \"$n$\", \"$M$\", \"$SD$\", \"1\", \"2\", \"3\", \"4\"),\n  align = c(\"l\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\"),\n  digits = c(NA, 0, 2, 2, 2, 2, 2, 2),\n  caption = \"Descriptive Statistics and Correlations for Study Variables\"\n  ) %&gt;%\n  kable_styling(full_width = TRUE) %&gt;%\n  footnote(\n    general_title = \"Note.\",\n    general = \"$^a$0 = private schools and 1 = public schools. $^b$Tuition is measured in thousands of dollars.\",\n    symbol = c(\"$p&lt;.05.$\", \"$p&lt;.01.$\", \"$p&lt;.001.$\"),\n    symbol_manual = c(\"$^{*}$\", \"$^{**}$\", \"$^{***}$\"),\n    threeparttable = TRUE,\n    footnote_as_chunk = TRUE,\n    escape = FALSE\n    ) %&gt;%\n  row_spec(row = 0, align = \"c\") %&gt;%\n  column_spec(column = 1, width = \"1.5in\")"
  },
  {
    "objectID": "posts/2020-03-26-welcome-to-my-office/2020-03-26-welcome-to-my-office.html",
    "href": "posts/2020-03-26-welcome-to-my-office/2020-03-26-welcome-to-my-office.html",
    "title": "Welcome to My Office",
    "section": "",
    "text": "./assets/body-header.qmdIn the great pandemic of 2020 many of us are working from home. Since 40+ students now see me in my home, I thought I would write a post about the room that they often see me in. So in that spirit, welcome to my home office.\n\n\n \n\nLEFT: Where the “magic” happens. Seated on the couch are two of my three newly minted teaching assistants. RIGHT: Another view of the office.\n\n\n\nWhen we bought our house and moved in about 13 years ago, this used to be a coat closet (see pic below). This is the ground floor of a four-level split (although I don’t think it is considered “above ground” by Zillow) which basically consisted of a narrow hallway that appended the coat closet and a bathroom in the other direction. The floor, which I later learned was concrete, was covered in an industrial indoor/outdoor carpet—not a bad choice considering the door to the garage is on this floor.\n\n\n\nPicture of the hall and closet where my current office is when we bought the house in 2007. The closet on the left is where my desk is now!\n\n\nAfter living in the house for several years and working on various rooms, Lauren (my wife) and I decided it was time to tackle this floor. Lauren had been inside one of our neighbor’s houses and seen that they had turned the space where the closet was into a room; thus was born the inspiration for my office.\nMy friend Tim and I did almost all of the work transforming this into what it is now. We had to move the HVAC from the middle of the room to the outside wall, build the half-walls (behind the computer and behind the couch) to hide some original construction, put up the stone, and lay the floor. I did all the trim work with Lauren’s help.\nAlmost all of the design choices are mine (with input from Lauren). The stone wall was the first stone that I ever installed. It was easy to adhere to the drywall behnd it. The flooring is a bamboo wood that I got a great deal on during a closeout sale at Lumber Liquidators. The floor looks great, but it definitely has scrathches from the dogs. I’m not sure I would go with Bamboo again in the future.\nThe big shaggy looking rug comes from Ikea (rumor has that the GOT folks created John Snow’s fur cloak by dying a larger version of this rug). The two rugs under the computer are sheepskin which I found super cheap at one of the vintage places in Edina. The computer table was a splurge and came from Room & Board. (The two delivery guys who brought it in had on white gloves to not leave fingerprints on it!)\n\n\n\nThe outside wall directly behind where I sit. The fruit crates contain part of my comic collection. The picture on the wall is an artist’s rendering of Burton Hall where the EPsy offices used to be prior to moving to EdSciB.\n\n\nThe couch was a recent purchase from Wayfair (Enfield Mid Century Modern Loveseat). The bookcase was a throwback to my college days when scrap lumber and cinder blocks were the cheapest bookcase around. The fruit crates I got from an old bookstore in Dinkytown that went out of business and used to use them as book storage. I use them as comic book storage; good size plus I think they look neat!\nIt is not the most exciting office in the world, but it is mine. Thanks for visiting!"
  },
  {
    "objectID": "posts/2019-06-26-what-to-do-about-p-values/2019-06-26-what-to-do-about-p-values.html",
    "href": "posts/2019-06-26-what-to-do-about-p-values/2019-06-26-what-to-do-about-p-values.html",
    "title": "What to do about p-values?",
    "section": "",
    "text": "./assets/body-header.qmd\nIn March, the ASA published a special issue of The American Statistician (TAS) related to statistical inference in the 21st century. In the initial article, Moving to a World Beyond “p &lt; 0.05”, Wassersein, Schirm, and Lazar (2019) write for the ASA saying,\n\n“The ASA Statement on P-Values and Statistical Significance stopped just short of recommending that declarations of “statistical significance” be abandoned. We take that step here. We conclude, based on our review of the articles in this special issue and the broader literature, that it is time to stop using the term “statistically significant” entirely. Nor should variants such as “significantly different,” “p &lt; 0.05,” and “nonsignificant” survive, whether expressed in words, by asterisks in a table, or in some other way. (p. 2)”\n\nSince reading this, I have been thinking about how to address these suggestions (mandates?) in our statistics curriculum. The suggestions offered in the article are nice, but a bit big-picture for applied scientists, things like:\n\nAccept uncertainty\nBe thoughtful\nBe open\nBe modest\n\nRon Wasserstein in his talk at USCOTS wasn’t much more helpful in what to do, but gave a gem of a quote when he said,\n\n“Small p-values are like a right-swipe in Tinder. It means you have an interest. It doesn’t mean you’re ready to book the wedding venue.”\n\nBut how does all of this translate in a statistical methods course (built around the general linear model) for graduate students in the social sciences? That is what I have been thinking about since March, and so far I have a few, albeit very few, ideas abut how to do this. Here are some things I plan on changing/incorporating into the course.\n\nInterpreting Results from Hypothesis Tests\nFirst off, I will be actively discouraging the use of anything related to “\\(p&lt;.05\\)” or the use of the word “significance”. I have done this in the past, but this year I am really goiing to go after it. The question is what to do instead? I am going to try using language such as “consistent with” or “compatible with” in these interpretations. For example,\n\nThe p-value of .003 suggests that the empirical data are inconsistent with the hypothesis that the regression coefficient differs from zero only because of sampling error. This, along with the positive sample slope provide evidence that time spent on homework likely has a positive association with GPA.\n\nThis type of interpretation is not that different than I have had students use in the past. I think the bigger difference is for results with larger p-values. Here the emphasis is\n\nThe p-value of .13 does not provide evidence against the hypothesis that the regression coefficient differs from zero only because of sampling error. While the empirical data are consistent with this hypothesis, it may also be the case that there is a positive relationship between time spent on homework and GPA (as evidenced by the positve sample slope) but the data do not contain enough statistical information to ascertain this relationship.\n\nThe big diffeence is I want to eliminate the “reject” or “fail to reject” language I have used in the past. In the second case, I also want students to illuminate the idea that data are consistent with multiple hypothesized values.\n\n\nConfidence Intervals as Compatibility Intervals\nWhen teaching about confidence intervals, I am thinking about focusing on them as “compatibility intervals” as suggested by Amrhein, Greenland, and McShane (2019). Here are four points I plan on stressing:\n\nJust because the interval gives the values most compatible with the data, given the assumptions, it doesn’t mean values outside it are incompatible; they are just less compatible.\nNot all values inside are equally compatible with the data, given the assumptions. The point estimate is the most compatible, and values near it are more compatible than those near the limits.\nLike the 0.05 threshold from which it came, the default 95% used to compute intervals is itself an arbitrary convention.\nLast, and most important of all, be humble: compatibility assessments hinge on the correctness of the statistical assumptions used to compute the interval. In practice, these assumptions are at best subject to considerable uncertainty.\n\nTo help with these ideas I am going to have students use visual representations of these that make the uncertainty more apparant. In the past I have had students create coefficient plots, but these tended to use lines to show the CIs for each of the regression coefficients. This year I am going to use Claus Wilke’s ungeviz package to create these plots. This package employs color density to indicate uncertainty; darker more dense color is associated with more certainty and lighter less dense color is associated with less certainty.\n\n\nWarning: `stat(ndensity)` was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(ndensity)` instead.\n\n\nWarning: Removed 4 rows containing missing values (`geom_tile()`).\n\n\n\n\n\nCoefficient plots for a multiple regression model using time spent on homework and level of parent education to predict GPA. The plot on the left shows the type of coefficient plot I used to have students create, and the plot on the right uses color density to show the uncertainty.\n\n\n\n\nThe syntax is straightforward. After fitting a linear model, obtain the coefficient-level output using tidy() and then use Wilke’s stat_confidence_density() function. I found his package documentation quite good and easy to implement. Here is the syntax I used. (The keith-gpa.csv data used is available here.)\n\n# Fit model\nlm.1 = lm(gpa ~ 1 + homework + parent_ed, data = keith)\n\n# Coefficient plot\nbroom::tidy(lm.1) %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  ggplot(aes(x = estimate, y = term)) +\n    stat_confidence_density(aes(moe = std.error, fill = stat(ndensity)), height = 0.15, confidence = 0.68) +\n    geom_point(aes(x = estimate), size = 2) +\n    xlim(-1, 3) +\n    theme_bw() +\n    scale_fill_gradient(low = \"#eff3ff\", high = \"#6baed6\") +\n    theme(\n      panel.grid.major = element_blank(),\n      panel.grid.minor = element_blank()\n      ) +\n    scale_y_discrete(name = \"Coefficients\", labels = c(\"Intercept\", \"Time spent\\non homework\")) +\n    xlab(\"Estimate\")\n\n\n\nRegression Smoothers\nI also want students to consider uncertainty in plots of their regression lines. In the past I have used geom_smooth() with the argument se=TRUE (default). However, this just draws the confidence enevelope as lines, which has the same issues as the earlier coefficient plot. To voercome this, I used ideas from Felix Schonbrodt to bootstrap potential regression lines and overlay them on a plot so that I could implement color density to illustrate uncertainty. I wrote these into a function stat_watercolor_smooth() in my educate package available via github. Note that I am NOT an R programmer; at best a script kiddie, so I do not promise these are curated nor great code.\nFor example, here is a line fitted using a simple linear regression.\n\nlibrary(educate)\n\nggplot(data = keith, aes(x = homework, y = gpa)) +\n  stat_watercolor_smooth(k = 1000, method = \"lm\") +\n  geom_smooth(method = \"lm\", se = FALSE, size = 0.5, color = \"black\") +\n  theme_bw() +\n  xlab(\"Time spent on homework (in hours)\") +\n  ylab(\"GPA (on a 100-pt scale)\")\n\n\nBy omitting the method=\"lm\" arguement a loess smoother will be fitted, similar to geom_smooth(). This can be useful for evaluating the linearity assumption in regression.\n\nbroom::augment(lm.1) %&gt;%\n  ggplot(aes(x =  .fitted, y = .resid)) +\n    geom_point() +\n    stat_watercolor_smooth(k = 1000) +\n    geom_hline(yintercept = 0, size = 0.5, color = \"black\") +\n    theme_bw() +\n    xlab(\"Model fitted values\") +\n    ylab(\"Model residuals\")\n\n\nThe uncertainty displayed in the plot suggests that empirical values are not inconsistent with the assumption of linearity (i.e., the uncertainty in the conditional mean values encompasses 0).\n\n\nDensity Plots and Uncertainty\nI use density plots in these methods courses and have used the sm.density() package for many years due to its ability to produce confidence envelopes for particular models. One drawback is that this package uses base R graphics. So this summer I wrote some syntax to implement some of these features using ggplot. These functions are available in my educate package via github.\nI used the same idea of In this package I bootstrap to create a spaghetti plot of hypothetical densities (which produce a “confidence enevelope”) and then use color density to show uncertainty.\n\nlibrary(educate)\n\nggplot(data = keith, aes(x = gpa)) +\n  stat_density_watercolor(k = 1000) +\n  stat_density(size = 0.5, geom = \"line\") +\n  theme_bw() +\n  xlab(\"GPA (on a 100-pt scale)\") +\n  ylab(\"Probability density\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nProbability density plot of the GPA variable. The plot also displays the uncertainty of the density via 1000 bootstrapped densities.\n\n\n\n\nWe can also use this to evaluate normality assumptions. This is quite useful for examining model assumptions. For example, here I plot the density of the marginal residuals from the previously fitted linear model.\n\nbroom::augment(lm.1) %&gt;%\nggplot(aes(x = .resid)) +\n  stat_density_watercolor(k = 1000, model = \"normal\") +\n  stat_density(size = 0.5, geom = \"line\") +\n  theme_bw() +\n  xlab(\"Model residuals\") +\n  ylab(\"Probability density\")\n\n\n\n\nProbability density plot of the model residuals. The plot also displays 1000 bootstrapped densities drawn from a normal distribution.\n\n\n\n\nHere the empirial density associated with the model residuals is not inconsistent with the assumption of normality (at least marginally).\n\n\nConclusion\nThese are my inital ideas. I am really curious to talk with other statistics educators to hear how they are addressing the p-value post TAS publication.\n\n\n\nReference\n\nAmrhein, V., Greenland, S., & McShane, B. (2019). Comment: Retire statistical significance. Nature, 567, 305–307.\n\n\nWasserstein, R. L., Schirm, A. L., & Lazar, N. A. (2019). Moving to a world Beyond “p &lt; 0.05.” The American Statistician, 73(sup1), 1–19. http://doi.org/10.1080/00031305.2019.1583913\n\nI have been in a"
  },
  {
    "objectID": "posts/2022-04-26-mission-statements/index.html",
    "href": "posts/2022-04-26-mission-statements/index.html",
    "title": "Mission Statements",
    "section": "",
    "text": "./assets/body-header.qmdSometime over the pandemic I read a book called Essentialism: The Disciplined Pursuit of Less by Greg McKeown. It was a classic business-ish time management book in which the premise is be more selective in how you spend your time and effort. Like most of these books, the author came across a bit cultish in his zeal for this, and the thesis was hammered via way too much repetitiveness.\nThat being said, I did resonate with the message, and also enjoyed some of the examples that he wrote about. One of those examples had to do with mission statements. The author points out that a well-crafted mission statement lays outr a strategy that is both concrete and inspirational, and has intent that is meaningful and memorable. This, he posits, reduces decision making later on since all future decisions will need to cohere with the mission.\nMost mission statements that I have seen (especially in academia) are inspirational, but too general ans vague to make decisions from. For example, consider the mission statement of our college (CEHD):\n\nThe mission of the University of Minnesota College of Education and Human Development is to contribute to a just and sustainable future through engagement with the local and global communities to enhance human learning and development at all stages of the life span.\n\nInspirational? Yes. But, this is about as vague as it gets. McKeown argues that one should be able to make hiring decisions based on the mission. How would you differentiate between multiple qualified candidates based on this mission? This is mission statement by committee if I have ever seen it (which is academia in a nutshell).\nMcKeown writes that in putting together a mission statement, the essentialist would ask: “If we could be truly excellent at only one thing, what would it be?” He also suggests that a second question that needs to be answered is: “How will we know if we have succeeded?” The CEHD mission statement fails to answer both of these questions.\nThe mission statement for my program, Quantitative Methods in Education, is not much better.\n\nQME strives to be a premier program recognized for leadership, innovation, and excellence, and to enable human potential through the advancement of education. QME prepares students to become cutting-edge professionals in educational measurement, evaluation, statistics, and statistics education, through excellence in teaching, research, and service; and through investigating and developing research methodology in education.\n\nReading this, it is unclear what our mission is at all, let alone what one thing we are aiming to be truly excellent at. When we wrote it, it ended up being so general in order to appease all of the stakeholders in the program. Which academic program doesn’t want to be recognized for leadership, innovation, and excellence? Are there programs that want to produce graduates who aren’t excellent at teaching, research, and service?\nI also have no idea how we will know when we have succeeded. Is it when we are recognized for leadership, innovation, AND excellence? By whom? Is it when we produce “cutting-edge” professionals? When our graduates are recognized for their excellence teaching, research, AND service? Or is it when our students investigate and develop research methodology that enables human potential through the advancement of education? (What does that even mean???)\nWorse than all of these is my department’s mission statement. The Educational Psychology mission statement is:\n\nEducational psychology involves the study of cognitive, emotional, and social learning processes that underlie education and human development across the lifespan. Research in educational psychology advances scientific knowledge of those processes and their application in diverse educational and community settings. The department provides training in the psychological foundations of education, research methods, and the practice and science of counseling psychology, school psychology, and special education. Faculty and students provide leadership and consultation to the state, the nation, and the international community in each area of educational psychology. The department’s scholarship and teaching enhance professional practice in schools and universities, community mental health agencies, business and industrial organizations, early childhood programs, and government agencies.\n\nThis is more of a description of what the faculty and students in our department do; it does not lay out a mission. Even worse, it isn’t even very inspirational."
  },
  {
    "objectID": "posts/2020-01-11-apa-tables-using-r-markdown-part-2/2020-01-11-apa-tables-using-r-markdown-part-2.html",
    "href": "posts/2020-01-11-apa-tables-using-r-markdown-part-2/2020-01-11-apa-tables-using-r-markdown-part-2.html",
    "title": "APA Tables using RMarkdown: Part 2",
    "section": "",
    "text": "./assets/body-header.qmd\nThis is the second part of a short blog series I am writing to re-create some of the sample tables found in the 7th edition APA Publication Manual. In this post I will attempt to re-create Table 7.8 (p. 214). To do so, I will incorporate many ideas that I covered in the first of these posts.\n\n\n\nTable 7.8 from the 7th edition of the APA Publication Manual (p. 214).\n\n\n\nMy Process\nI will again render to PDF and set up the YAML to import the caption package (LaTeX) and set up the APA caption formatting.\n\n---\ntitle: \"Untitled\"\nauthor: \"Andrew Zieffler\"\ndate: \"1/11/2020\"\noutput: pdf_document\nheader-includes:\n   - \\usepackage{caption}\n   - \\captionsetup[table]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=newline}\n---\n\nIn the RMarkdown body, I will load a few packages and also create the data frame that includes the table data.\n\n# Load libraries\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(tidyverse)\n\n# Create table\ntab_02 = data.frame(\n  parameter = c(\n    \"Maximum asymptote, proportion\", \n    \"Crossover, in ms\", \n    \"Slope, as change in proportion per ms\"\n    ),\n  m1 = c(.843, 759, .001),\n  s1 = c(.135, 87, .0002),\n  m2 = c(.877, 694, .002),\n  s2 = c(.082, 42, .0002),\n  t = c(0.951, 2.877, 2.635),\n  p = c(.347, .006, .012),\n  d = c(0.302, 0.840, 2.078)\n)\n\nI will then pipe this into the kable() function to set the column names, column alignment, and table caption. The column headings we will want to use are associated with those in the lowest header row of Table 7.8. We will add the higher header row later.\nI also employ similar kableExtra function from those introduced in Part 1 to make the table the full page width and include the footnote. The only addition is the use of escape=FALSE as an additional argument in the footnote() function to use LaTeX syntax (namely the $) to typeset the mathematics in the footnote.\n\nkable(\n  tab_02,\n  format = \"latex\",\n  booktabs = TRUE,\n  escape = FALSE,\n  longtable = TRUE,\n  col.names = c(\"\", \"M\", \"SD\", \"M\", \"SD\", \"\", \"\", \"\"),\n  align = c(\"l\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\"),\n  caption = \"Results of Curve-Fitting Analysis Examining the Time Course of Fixations to the Target\"\n  ) %&gt;%\n  kable_styling(full_width = TRUE) %&gt;%\n  footnote(\n    general_title = \"Note.\",\n    general = \"For each participant, the logistic function was fit to target fixations separately. The maximum asymptote is the asymptotic degree of looking at the end of the time course fixations. The crossover is the point in time when the function crosses the midway point between peak and baseline. The slope represents the rate of change in the function measured at the crossover. Mean parameter values for each of the analyses are shown for the 9-year-olds ($n=24$) and the 16-year-olds ($n=18$), as well as the results of $t$ tests (assuming unequal variance) comparing the parameter estimates between the two ages.\",\n    threeparttable = TRUE,\n    footnote_as_chunk = TRUE,\n    escape = FALSE\n    )\n\n\nAgain, this gets us about 90% of the way there. The first adjustment we will want to make is to make the first column wider. To do this we will use the column_spec() function from kableExtra to change the column width of the first column. For most of my work, I play around with this width until it looks good.\n\nkable(\n  tab_02,\n  format = \"latex\",\n  booktabs = TRUE,\n  escape = FALSE,\n  longtable = TRUE,\n  col.names = c(\"\", \"M\", \"SD\", \"M\", \"SD\", \"\", \"\", \"\"),\n  align = c(\"l\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\"),\n  caption = \"Results of Curve-Fitting Analysis Examining the Time Course of Fixations to the Target\"\n  ) %&gt;%\n  kable_styling(full_width = TRUE) %&gt;%\n  footnote(\n    general_title = \"Note.\",\n    general = \"For each participant, the logistic function was fit to target fixations separately. The maximum asymptote is the asymptotic degree of looking at the end of the time course fixations. The crossover is the point in time when the function crosses the midway point between peak and baseline. The slope represents the rate of change in the function measured at the crossover. Mean parameter values for each of the analyses are shown for the 9-year-olds ($n=24$) and the 16-year-olds ($n=18$), as well as the results of $t$ tests (assuming unequal variance) comparing the parameter estimates between the two ages.\",\n    threeparttable = TRUE,\n    footnote_as_chunk = TRUE,\n    escape = FALSE\n    ) %&gt;%\n  column_spec(column = 1, width = \"2in\")\n\n\nThe second thing we will adjust is that in the original table there is a slight indent in the text “Slope, as change in proportion per ms” to indicate that the text has overflowed onto the next line. To do this we have to include some LaTeX syntax into the actual data.\nWe want insert a linebreak into the text after the word “proportion” and then indent the text slightly. The LaTeX syntax for a linebreak is two forward slashes \\\\, and that for indenting is \\hspace{1em} where \\hspace{} indicates a horizontal space and 1em is the width of this space; in our case I use a width of one em.\nIf we were using a LaTeX program this syntax would work fine, but in RMarkdown, the forward slash \\ is a special escape character. Thus, if we really want a forward slash we need to use two slashes, \\\\, the first escapes and the second is our actual slash. Thus for our syntax we need four forward slashes (to really get two) for the linebreak and then we need to again use two to make the horizontal space. This looks like this:\n\n\\\\\\\\\\\\hspace{1em}\n\nWe will add this directly into the data frame exactly where we want the linebreak and indentation.\n\n# Create table with linebreak and indentation\ntab_02 = data.frame(\n  parameter = c(\n    \"Maximum asymptote, proportion\", \n    \"Crossover, in ms\", \n    \"Slope, as change in proportion \\\\\\\\\\\\hspace{1em}per ms\"\n    ),\n  m1 = c(.843, 759, .001),\n  s1 = c(.135, 87, .0002),\n  m2 = c(.877, 694, .002),\n  s2 = c(.082, 42, .0002),\n  t = c(0.951, 2.877, 2.635),\n  p = c(.347, .006, .012),\n  d = c(0.302, 0.840, 2.078)\n)\n\nThen we can re-run the syntax to create our table.\n\nThe last thing we need to do is add in the top header row. To do this we will employ the add_header_above() function from the kableExtra package. This function takes an argument header= that according to the documentation, gives a “(named) character vector with the column span as values. For example, c(\" \" = 1, \"title\" = 2) can be used to create a new header row for a 3-column table with”title” spanning across column 2 and 3.”\nWe will also include escape=FALSE in this function to again use LaTeX syntax to format the text in this heading. (Note: Placing text inside a set of $ will italicize it as it typesets it mathematically.) The new syntax for us is:\n\nkable(\n  tab_02,\n  format = \"latex\",\n  booktabs = TRUE,\n  escape = FALSE,\n  longtable = TRUE,\n  col.names = c(\"\", \"M\", \"SD\", \"M\", \"SD\", \"\", \"\", \"\"),\n  align = c(\"l\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\", \"c\"),\n  caption = \"Results of Curve-Fitting Analysis Examining the Time Course of Fixations to the Target\"\n  ) %&gt;%\n  kable_styling(full_width = TRUE) %&gt;%\n  footnote(\n    general_title = \"Note.\",\n    general = \"For each participant, the logistic function was fit to target fixations separately. The maximum asymptote is the asymptotic degree of looking at the end of the time course fixations. The crossover is the point in time when the function crosses the midway point between peak and baseline. The slope represents the rate of change in the function measured at the crossover. Mean parameter values for each of the analyses are shown for the 9-year-olds ($n=24$) and the 16-year-olds ($n=18$), as well as the results of $t$ tests (assuming unequal variance) comparing the parameter estimates between the two ages.\",\n    threeparttable = TRUE,\n    footnote_as_chunk = TRUE,\n    escape = FALSE\n    ) %&gt;%\n  column_spec(column = 1, width = \"2in\") %&gt;%\n  add_header_above(\n    c(\"Logistic parameter\" = 1, \"9-year-olds\" = 2, \"16-year-olds\" = 2,\n      \"t(40)\" = 1, \"$p$\" = 1, \"Cohens $d$\" = 1),\n    escape = FALSE\n    )\n\n\nBy default the add_header_above() function adds a bottom border for each column that includes text. To mimic Table 7.8 we need to remove that border from a few of those columns. I know of no way to do this in the R syntax. So, what we need to do is examine the actual LaTeX syntax that is being produced when we create this table. If you run this syntax in the Console, it will produce the following LaTeX syntax (which is rendered into the formatted table in your RMarkdown document).\n\n\\begin{ThreePartTable}\n\\begin{TableNotes}[para]\n\\item \\textit{Note.} \n\\item For each participant, the logistic function was fit to target fixations separately. The maximum asymptote is the asymptotic degree of looking at the end of the time course fixations. The crossover is the point in time when the function crosses the midway point between peak and baseline. The slope represents the rate of change in the function measured at the crossover. Mean parameter values for each of the analyses are shown for the 9-year-olds ($n=24$) and the 16-year-olds ($n=18$), as well as the results of $t$ tests (assuming unequal variance) comparing the parameter estimates between the two ages.\n\\end{TableNotes}\n\\begin{longtabu} to \\linewidth {&gt;{\\raggedright\\arraybackslash}p{2in}&gt;{\\centering}X&gt;{\\centering}X&gt;{\\centering}X&gt;{\\centering}X&gt;{\\centering}X&gt;{\\centering}X&gt;{\\centering}X}\n\\caption{\\label{tab:}Results of Curve-Fitting Analysis Examining the Time Course of Fixations to the Target}\\\\\n\\toprule\n\\multicolumn{1}{c}{Logistic parameter} & \\multicolumn{2}{c}{9-year-olds} & \\multicolumn{2}{c}{16-year-olds} & \\multicolumn{1}{c}{t(40)} & \\multicolumn{1}{c}{$p$} & \\multicolumn{1}{c}{Cohens $d$} \\\\\n\\cmidrule(l{3pt}r{3pt}){1-1} \\cmidrule(l{3pt}r{3pt}){2-3} \\cmidrule(l{3pt}r{3pt}){4-5} \\cmidrule(l{3pt}r{3pt}){6-6} \\cmidrule(l{3pt}r{3pt}){7-7} \\cmidrule(l{3pt}r{3pt}){8-8}\n & M & SD & M & SD &  &  & \\\\\n\\midrule\nMaximum asymptote, proportion & 0.843 & 0.1350 & 0.877 & 0.0820 & 0.951 & 0.347 & 0.302\\\\\nCrossover, in ms & 759.000 & 87.0000 & 694.000 & 42.0000 & 2.877 & 0.006 & 0.840\\\\\nSlope, as change in proportion \\hspace{1em}per ms & 0.001 & 0.0002 & 0.002 & 0.0002 & 2.635 & 0.012 & 2.078\\\\\n\\bottomrule\n\\insertTableNotes\n\\end{longtabu}\n\\end{ThreePartTable}\n\nWe can copy-and-paste this syntax outside of a code chunk and it will also be rendered into a table. The RMarkdown document will read and render any LaTeX syntax so long as you are outputting to a PDF.\nThe \\cmidrule{} syntax is what is used to include the bottom borders (called “mid-rules” in the typesetter’s world). We can delete all of them except for \\cmidrule(l{3pt}r{3pt}){2-3} and \\cmidrule(l{3pt}r{3pt}){4-5}. These are drawing the mid-rules under the second/third and fourth/fifth columns, respectively.\n\n\\begin{ThreePartTable}\n\\begin{TableNotes}[para]\n\\item \\textit{Note.} \n\\item For each participant, the logistic function was fit to target fixations separately. The maximum asymptote is the asymptotic degree of looking at the end of the time course fixations. The crossover is the point in time when the function crosses the midway point between peak and baseline. The slope represents the rate of change in the function measured at the crossover. Mean parameter values for each of the analyses are shown for the 9-year-olds ($n=24$) and the 16-year-olds ($n=18$), as well as the results of $t$ tests (assuming unequal variance) comparing the parameter estimates between the two ages.\n\\end{TableNotes}\n\\begin{longtabu} to \\linewidth {&gt;{\\raggedright\\arraybackslash}p{2in}&gt;{\\centering}X&gt;{\\centering}X&gt;{\\centering}X&gt;{\\centering}X&gt;{\\centering}X&gt;{\\centering}X&gt;{\\centering}X}\n\\caption{\\label{tab:}Results of Curve-Fitting Analysis Examining the Time Course of Fixations to the Target}\\\\\n\\toprule\n\\multicolumn{1}{c}{Logistic parameter} & \\multicolumn{2}{c}{9-year-olds} & \\multicolumn{2}{c}{16-year-olds} & \\multicolumn{1}{c}{t(40)} & \\multicolumn{1}{c}{$p$} & \\multicolumn{1}{c}{Cohens $d$} \\\\\n\\cmidrule(l{3pt}r{3pt}){2-3} \\cmidrule(l{3pt}r{3pt}){4-5}\n & M & SD & M & SD &  &  & \\\\\n\\midrule\nMaximum asymptote, proportion & 0.843 & 0.1350 & 0.877 & 0.0820 & 0.951 & 0.347 & 0.302\\\\\nCrossover, in ms & 759.000 & 87.0000 & 694.000 & 42.0000 & 2.877 & 0.006 & 0.840\\\\\nSlope, as change in proportion \\hspace{1em}per ms & 0.001 & 0.0002 & 0.002 & 0.0002 & 2.635 & 0.012 & 2.078\\\\\n\\bottomrule\n\\insertTableNotes\n\\end{longtabu}\n\\end{ThreePartTable}\n\nThis gives us our final table."
  },
  {
    "objectID": "posts/2020-07-22-a-history-of-r/2020-07-22-a-history-of-r.html",
    "href": "posts/2020-07-22-a-history-of-r/2020-07-22-a-history-of-r.html",
    "title": "A History of R",
    "section": "",
    "text": "./assets/body-header.qmd\nTomorrow (July 23, 2020 ) I am speaking on the history of the R statistical software environment as part of a virtual panel for a Lunch & Learn. I like how they used “featuring” in the announcement. It is like I am a hip-hop star! The title of my talk is, A History of R (in 15 minutes…and mostly in pictures)."
  },
  {
    "objectID": "posts/2020-07-22-a-history-of-r/2020-07-22-a-history-of-r.html#references-and-resources",
    "href": "posts/2020-07-22-a-history-of-r/2020-07-22-a-history-of-r.html#references-and-resources",
    "title": "A History of R",
    "section": "References and Resources",
    "text": "References and Resources\nFirst, my slides:\n\nPDF of the Slides\n\nThe slides include a long list of references, many of which are videos of talks by the people who appear in my slides. And believe me, they can tell their story better than I can. Unfortunately when I flattened the PDF the links no longer work, so I re-print them here with links. The new PDF should have working links!\nHistory of S and R [video]\n\nBecker, R. (2016). Forty years of S. Presentation at the UseR! conference. Stanford, CA.\nChamber, J. M (2014). Interface Efficiency and Big Data. Keynote address at the UseR! conference. Los Angeles, CA.\nDalgaard, P. (2018). What’s in a name? 20 years of R release management. UseR! Conference, Brisbane, Australia.\nDalgaard, P. (2020). A brief history of R and some thoughts about its future. CelebRation 2020 conference, Copenhagen, Denmark.\nPeng, R. (2015). Overview and history of R.\nRevolutions. (2014). John Chambers recounts the history of S and R. An interview by Trevor Hastie.\n\nHistory of S and R [papers/slides/resources]\n\nBecker, R. A., & Chambers, J. M. (1978). Design and implementation of the “S” system for interactive data analysis. The IEEE Computer Society’s Second International Computer Software and Applications Conference, 1978. COMPSAC ’78., 626–629.\nChambers, J. M. (2006). History of S and R (with some thoughts for the future). Presentation at the UseR! Conference, Vienna, Austria.\nD’Agostino McGowan, L. (2017). R release names. Livefreeordichotomize blog.\nGentleman, R. (2009). R and modern statistical computing. Talk.\nGentleman, R., & Ihaka, R. (2000). Lexical scope and statistical computing. Journal of Computational and Graphical Statistics, 9(3), 491–508.\nIhaka, R. (1998). R: Past and future history. Interface ’98.\nIhaka, R. (2009). The R Project: A brief history and thoughts about the future. Massey University Statistics Day. Massey University, Palmerston North, New Zealand.\nIhaka, R., & Gentleman, R. (1996). R: A language for data analysis and graphics. Journal of Computational and Graphical Statistics, 5(3), 299–314.\nThe R Foundation. (n.d.). The R Project for Statistical Computing. [website]"
  },
  {
    "objectID": "posts/2019-11-24-two-online-articles/2019-11-24-two-online-articles.html",
    "href": "posts/2019-11-24-two-online-articles/2019-11-24-two-online-articles.html",
    "title": "Two Online Articles",
    "section": "",
    "text": "./assets/body-header.qmd\nThe internet is filled with interesting reads. Some thought-provoking, some inspiring, some enlightening, some just plain fun. Over the past few months I came across two that I would like to share with you.\n\nArticle 1: How Margaret Dayhoff Brought Modern Computing to Biology\nThe first, How Margaret Dayhoff Brought Modern Computing to Biology was published by The Smithsonian and provided a biological sketch of Margaret Dayoff’s early adoption of computation to catalog and analyze biological data, in her case proteins. What are commonplace methods now (e.g., creating a database) were really inspired ways of dealing with the (at the time) mass amounts of information and data and this article was a nice showcase of this incredible researcher.\n\n\n\nMargaret Dayhoff was a pioneer of using computers to tackle some of the biggest scientific questions of the day. (Photo illustration by Smithsonian.com; Photo courtesy NIH National Library of Medicine / Ruth Dayhoff).\n\n\n\n\nArticle 2: Should a Ph.D. Be Hard?\nThe second article that I wanted to share was entitled, Should a PhD be ‘hard’?. This piece, written by Katherine Firth and published on Research Degree Insiders, contemplates what students mean when they are finding a Ph.D. hard. Firth writes that she often finds that students in this position often mean one of two things:\n\n“The first group find the PhD ‘hard’ and worry that this means they aren’t smart enough, that they won’t finish, that they don’t belong in the PhD program.”\n“The second type of student is really struggling. They are finding the PhD tough for the wrong reasons.”\n\nFirth posits that students in the first group often worry about whether they are smart enough or whether they belong. (Note. In my experience this is completely natural and the answers are: they are, and they do.) She points out that many times these students were great students and perhaps haven’t really experienced something that was “hard” previously. Thus they need feedback and tools to help them both persevere and learn from setbacks and failures. While not diminishing the struggles the first group of students face, it is students in the latter group that Firth worries about. These are the students who are finding the fdegree difficult because of a toxic research culture. Her advice for this type of student centers around finding a support structure. She notes, “if you are being bullied or are getting sick because of the PhD, that’s not okay. Find a way to make the PhD hard like climbing a mountain, not hard like being hit with a stick.”\nShe ends the piece by reminding us that,\n\nA PhD is about sustained intellectual engagement, creating new knowledge, and contributing to the community of scholars in your field, through an extended work of academic writing, and (perhaps) presenting your work verbally to others."
  },
  {
    "objectID": "posts/2018-02-19-college-in-the-schools/index.html",
    "href": "posts/2018-02-19-college-in-the-schools/index.html",
    "title": "College in the Schools",
    "section": "",
    "text": "./assets/body-header.qmd\nCollege in the Schools is a concurrent enrollment program in which existing University of Minnesota courses are taught in high schools by high school teachers. All schools wishing to offer a CIS course go through an application process, including an interview and approval by the post-secondary academic department sponsoring the course of any high school teachers who will be teaching the course.\nSince CIS is a concurrent enrollment program, students are enrolled simultaneously in the University of Minnesota course (EPsy 3264: Basic and Applied Statistics) and a course at their high school. (As such, they receive two separate grades.) This setup allows for variation between the university and high school schedules. It also allows high schools to cover additional material or devote additional time to topics as needed (e.g., to accommodate requirements of state testing)."
  },
  {
    "objectID": "posts/2018-02-19-college-in-the-schools/index.html#cis-statistics",
    "href": "posts/2018-02-19-college-in-the-schools/index.html#cis-statistics",
    "title": "College in the Schools",
    "section": "CIS Statistics",
    "text": "CIS Statistics\nWe implemented the CIS statistics course in the 2015–2016 academic year. Since that time, we have had 17 high schools teach a CIS statistics course. The map below shows these schools.\n\n\n\n\n\n\nCurriculum, Pedagogy, and Assessment\nTo meet the University of Minnesota requirements, the high school students experience the same curriculum and assessments that is used at the university. (They are also graded using the same standards as students at the university.) The course uses the CATALST curriculum which immerses students in the nuts-and-bolts of statistical inference from the first day of the course via a focus on modeling and simulation. One of the primary goals of the CATALST curriculum is the promotion of statistical thinking. This is achieved by emphasizing the “core logic of statistical inference” throughout the curriculum, rather than procedural knowledge.\nThe underlying philosophy of CATALST is that students are primarily responsible for their own learning. Students meet that responsibility by reading the materials, completing the homework, and carrying out any other preparation prior to each class period.\nThis is also true in the classroom, where students work with 2–3 other students through a series of structured activities and discussions. This group-work is where a majority of the student learning takes place. In the classroom, the role of the instructor is more “guide-on-the-side”: leading larger classroom discussions, posing questions, and helping students draw connections and make sense of the material.\nStudents use the TinkerPlots™ software during class and as a part of their at-home preparation and homework. TinkerPlots™ employs allows students to create models and simulate date using an interactive set of tools. The visual display of the simulation process helps students gain a deeper conceptual understanding of the underlying data generation process used in statistical inference.\nBecause of the emphasis on cooperative group learning in the course, students are evaluated using both group and individual assessments. Individual-level assessments offer students an opportunity to show they have learned the material, while group assessments push students’ understanding of the course material through questions that extend the content in ways they may not have previously encountered (while at the same time offering these extensions via the relative safety of the group).\n\n\nProfessional Development\nHigh school teachers who are accepted to teach the CIS course engage in ongoing professional development (PD). Each year they are required to complete 30 hours of PD: a 3-day workshop each summer and and a 1-day workshop in both the fall and spring semester.\nIn addition to the PD, CIS teachers are also observed by University of Minnesota faculty. These observations occur at least once during the initial year that a school adopts a CIS course and at least once every three years thereafter."
  },
  {
    "objectID": "posts/2018-02-19-college-in-the-schools/index.html#resources",
    "href": "posts/2018-02-19-college-in-the-schools/index.html#resources",
    "title": "College in the Schools",
    "section": "Resources",
    "text": "Resources\nMore information about the CIS statistics course and the CATALST curriculum can be found in the following resources:\n\nGarfield, J., delMas, R., & Zieffler, A. (2012). Developing statistical modelers and thinkers in an introductory, tertiary-level statistics course. ZDM—The International Journal on Mathematics Education, 44(4), 883–898.\nZieffler, A., & Huberty, M. (2015). A catalyst for change in the high school math curriculum. CHANCE, 28(3), 44–49. doi: 10.1080/09332480.2015.1099365"
  },
  {
    "objectID": "posts/2019-07-24-dissertation-writing/2019-07-24-dissertation-writing.html",
    "href": "posts/2019-07-24-dissertation-writing/2019-07-24-dissertation-writing.html",
    "title": "Dissertation Writing",
    "section": "",
    "text": "./assets/body-header.qmd\nMany times I see students get paralyzed by the idea of writing dissertation (or paper). Anne Lamott gives this advice to struggling writers:\n\nThirty years ago, my older brother, who was ten years old at the time, was trying to get a report on birds written that he’d had three months to write. [It] was due the next day. We were out at our family cabin in Bolinas, and he was at the kitchen table close to tears, surrounded by binder paper and pencils and unopened books on birds immobilized by the hugeness of the task ahead. Then my father sat down beside him, put his arm around my brother’s shoulder, and said, “Bird by bird, buddy. Just take it bird by bird.”\n\nAnne Lamott, Bird by Bird: Some Instructions on Writing and Life"
  },
  {
    "objectID": "posts/2020-09-14-journal-of-statistics-education-jse-references/2020-09-14-journal-of-statistics-education-jse-references.html",
    "href": "posts/2020-09-14-journal-of-statistics-education-jse-references/2020-09-14-journal-of-statistics-education-jse-references.html",
    "title": "Journal of Statistics and Data Science Education (JSE) Reference Examples",
    "section": "",
    "text": "./assets/body-header.qmd\nThis post is to act as more of a reference for myself regarding the referencing for JSE articles. JSE references use the American Statistical Association’s style guide which is here.\nZotero’s Style Repository includes a style file here."
  },
  {
    "objectID": "posts/2020-09-14-journal-of-statistics-education-jse-references/2020-09-14-journal-of-statistics-education-jse-references.html#article-examples",
    "href": "posts/2020-09-14-journal-of-statistics-education-jse-references/2020-09-14-journal-of-statistics-education-jse-references.html#article-examples",
    "title": "Journal of Statistics and Data Science Education (JSE) Reference Examples",
    "section": "Article Examples",
    "text": "Article Examples\nGould, R. (2004), “Variability: One Statistician’s View,” Statistics Education Research Journal, 3, 7–16. Available at https://iase-web.org/documents/SERJ/SERJ3(2)_Gould.pdf?1402525005.\n– – (2017), “Data Literacy is Statistical Literacy,” Statistics Education Research Journal, 16, 22–25. Available at https://iase-web.org/documents/SERJ/SERJ16(1)_Gould.pdf?1498680916.\nJustice, N., Le, L., Sabbag, A., Fry, E., Ziegler, L., and Garfield, J. (2020), “The CATALST Curriculum: A Story of Change,” Journal of Statistics Education, 28, 175–186, DOI: 10.1080/10691898.2020.1787115. Available at https://www.tandfonline.com/doi/full/10.1080/10691898.2020.1787115.\n\nIf two or more works by the same author or team of authors have the same publication date, list them by order of appearance in the text and distinguish them by lowercase “a,” “b,” and so on, after the date: “(1970a).”"
  },
  {
    "objectID": "posts/2020-09-14-journal-of-statistics-education-jse-references/2020-09-14-journal-of-statistics-education-jse-references.html#book-examples",
    "href": "posts/2020-09-14-journal-of-statistics-education-jse-references/2020-09-14-journal-of-statistics-education-jse-references.html#book-examples",
    "title": "Journal of Statistics and Data Science Education (JSE) Reference Examples",
    "section": "Book Examples",
    "text": "Book Examples\nAmerican Statistical Association. (2005), Guidelines for Assessment and Instruction in Statistics Education: College Report, Washington, DC: Author. Available at https://www.amstat.org/ASA/Education/Guidelines-for-Assessment-and-Instruction-in-Statistics-Education-Reports.aspx.\nBreiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. I. (1984), Classification and Regression Trees, Belmont, California: Wadsworth.\nFranklin, C., Bargagliotti, A., Case, C., Kader, G., Schaeffer, R., and Spangler, D. (2015), The Statistical Education of Teachers, Arlington, VA: American Statistical Association. Available at https://www.amstat.org/asa/files/pdfs/EDU-SET.pdf."
  },
  {
    "objectID": "posts/2020-09-14-journal-of-statistics-education-jse-references/2020-09-14-journal-of-statistics-education-jse-references.html#book-chapterproceedings-examples",
    "href": "posts/2020-09-14-journal-of-statistics-education-jse-references/2020-09-14-journal-of-statistics-education-jse-references.html#book-chapterproceedings-examples",
    "title": "Journal of Statistics and Data Science Education (JSE) Reference Examples",
    "section": "Book Chapter/Proceedings Examples",
    "text": "Book Chapter/Proceedings Examples\nHolcomb, J., Chance, B., Rossman, A., and Cobb, G. (2010), “Assessing Student Learning about Statistical Inference,” in Data and Context in Statistics Education: Towards an Evidence-Based Society. Proceedings of the Eighth International Conference on Teaching Statistics, ed., C. Reading., Ljubljana, Slovenia. Voorburg, The Netherlands: International Statistical Institute. Available at https://iase-web.org/documents/papers/icots8/ICOTS8_5F1_CHANCE.pdf.\nLesh, R., Hoover, M., Hole, B., Kelly, A., and Post, T. (2000), “Principles for Developing Thought Revealing Activities for Students and Teachers,” in Handbook of Research Design in Mathematics and Science Education, eds., R. Lesh and A. Kelly, Mahwah, NJ: Lawrence Erlbaum Associates, pp. 591–645."
  },
  {
    "objectID": "posts/2020-09-14-journal-of-statistics-education-jse-references/2020-09-14-journal-of-statistics-education-jse-references.html#software-examples",
    "href": "posts/2020-09-14-journal-of-statistics-education-jse-references/2020-09-14-journal-of-statistics-education-jse-references.html#software-examples",
    "title": "Journal of Statistics and Data Science Education (JSE) Reference Examples",
    "section": "Software Examples",
    "text": "Software Examples\nBecker, R. A., Wilks, A. R., Brownrigg, R., Minka, T. P., and Deckmyn, A. (2018), “maps: Draw Geographical Maps,” R Package Version 3.3.0. Available at https://CRAN.R-project.org/package=maps.\nR Core Team (2020), R: A Language and Environment for Statistical Computing, Vienna, Austria: R Foundation for Statistical Computing. Available at https://www.R-project.org."
  },
  {
    "objectID": "posts/2018-04-09-epsy-5271-part-04/2018-04-09-epsy-5271-part-04.html",
    "href": "posts/2018-04-09-epsy-5271-part-04/2018-04-09-epsy-5271-part-04.html",
    "title": "Q&A with Becoming a Teacher of Statistics Class: Part IV",
    "section": "",
    "text": "./assets/body-header.qmd\nThis post is the fourth (and last) in a series of blogposts in which I respond to questions from the students in the Becoming a Teacher of Statistics course. In today’s posting I respond to questions that asked me for predictions about the future of statistics teaching and statistics education research.\nBefore I get into the Q&A, let me just state: Prediction is hard. Leland Wilkinson in The Future of Statistical Computing reminded us of this when he cited a prediction about computers that Andrew Hamilton made in a 1949 issue of Popular Mechanics\n\nWhere a calculator like the ENIAC today is equipped with 18,000 vacuum tubes and weighs 30 tons, computers in the future may have only 1,000 vacuum tubes and perhaps weigh only \\(1 \\dfrac{1}{2}\\) tons.\n\nSo, read my responses with caution.\n\nGiven your experience of evolution of Statistics Education research over the past around 30 years or so, where do you envision Stat Ed research to be, say in the ensuing 30 years? What major stumble-blocks, if any, might the stat educators face along the path? What should be some best strategies to overcome those?\nMy hope is that statistics education research will be breaking out of its infancy and statistics educators will be paying more attention to what the research is showing with regards to pedagogy and stduent learning. I also hope that the community of statistics education researchers has, by that time, begun to coalesce around a common research agenda.\nMy cynical answer is that we will be in the same place as we are right now, with too few statistics educators aware of the research that exists.\nThe reality will probably be somewhere between the two.\nAs I consider what challenges the research community faces, I can’t help but pause and think about how statistics education has changed over time, even in the last 10 years. Computing has played a major role in this change. This has led to curriculum changes and questions about how to incorporate computing into courses. How do you teach both the statistical and computing content together? How much computing do we teach? Which computing tools do we teach? I forsee these challenges only growing in the future.\nWhen I answered this in the Becoming a Teacher of Statistics class, Dennis Pearl asked me whether one issue for the research community is the pace at which technology is driving the pace of change. New computing tools (R packages, dashboards, etc.) are appearing every day. These tools are adopted (and just as quickly abandoned as the new tool comes along) before any research can possibly happen.\nResearch moves slowly. It is not uncommon for the timeline of research to be two to three years to get a study together, obtain IRB approval, conduct the study, write up the results, and disseminate them to the community. This timeline is not compatible with the pace of technological change.\nI don’t know what the answer to this is, but my first thought is that companies like Google are doing research in this fast-paced world, so maybe we can learn from what they are doing. My second thought is that companies like Google often have very clear measurements (did a person click on an ad), and statistics education does not (what does it mean to learn computing).\n\n\nJust like 20 years ago Bayesian Inference was not considered something to be taught in intro stats course (MCMC and computation were still a problem), do you think there is something that is overlooked nowadays that could have a potential to be taught in the future?\nAnother prediction question. r emo::ji(\"weary\") This one is easier. Yes.\nNow the follow-up is: Like what? I’m not sure, but inevitably something will rear up. In the 1990’s, statistics education began to push toward teaching more conceptual understanding and away from the formulaic and dogged calculation of probability problems. Asa result, probability took on a much smaller role in the curricula developed during this time. Twenty years later, the rise of simulation-based approaches to statistical inference brings probability modeling right back to the forefront, albeit in a different manner than before.\nI still am a bit skeptical about teaching Bayesian inference in an introductory course. The ideas yes; the methods no. While incredibly useful to an applied statistician and scientists, Bayesian calculations are not stright forward nor easy. Computing, of course, can make these quicker, but will students understand what they are doing? Or will it just be following recipes? Now teaching the big ideas behind Bayesian inference make perfect sense in an introductory course; the idea that combining prior beliefs with evidence (data) leads to updated beliefs. This makes a lot of sense to me. Having students reason about how thes posterior beliefs are impacted by the data when those prior beliefs are strong versus when they are weak can lead to great discussion of how we weigh evidence and what that means for drawing conclusion (and also why it is so hard to sway people’s opinions).\n\n\nThere are many foreign students coming to America for education purpose in different levels: high school, college and graduate level. Do you think this will influence the curriculum of the courses offered in America?\nProbably not. The American education system has an inertia that is very difficult to change. Even oodles of research evidence seems to have very little impact on this system. I have a theory that this may be because adults (i.e., parents) have gone through and experienced the system, and there is a hindsight bias about that experience. “In my day we worked on algebra problems nonstop, so you should too.”\nYou might think that at least colleges and universities would be quicker to change. It turns out that here too the system is slow to change. Individuals have more autonomy to make changes in their clasrooms at this level, but the sytem as a whole seems to be stuck on a fulcrum of “what we used to do”.\nIn statistics, the reformers have been asking for change in the introductory course for years, citing the changes in the high school curriculum and standards such as Common Core (or previous to that the NCTM mathemateics standards—which have been around since the 1980’s), with little result. So, I am not optimistic that an influx of foreign students will have an influence either."
  },
  {
    "objectID": "posts/2019-10-10-illustrating-the-gender-gap-in-fiction/2019-10-10-illustrating-the-gender-gap-in-fiction.html",
    "href": "posts/2019-10-10-illustrating-the-gender-gap-in-fiction/2019-10-10-illustrating-the-gender-gap-in-fiction.html",
    "title": "Illustrating the Gender Gap in Fiction",
    "section": "",
    "text": "./assets/body-header.qmdLoganberry Books, an independent bookstore in Cleveland, intitiated a social experiment for in March 2019 called Illustrating the Gender Gap in Fiction. This experiment, in honor of Women’s History Month, was decribed as, “a live performance art project where we will shelve the works by men in our LitArts room backwards”. What a cool idea, and very eye-opening.\n\n\n\nShelves of backward books emphasize the gender imbalance in literature.\n\n\nThis would likely be worse if the experiment was carried out with mathematics, statistics, and computer science books. May have to replicate this in my office at some point."
  },
  {
    "objectID": "posts/2020-06-30-to-richard-wright/2020-06-30-to-richard-wright.html",
    "href": "posts/2020-06-30-to-richard-wright/2020-06-30-to-richard-wright.html",
    "title": "Four Sheets to the Wind And a One Way Ticket to France",
    "section": "",
    "text": "./assets/body-header.qmdConrad Kent Rivers (1933–1968) was a renowned black poet who won the Savannah State Poetry Prize for his poem Poor Peon while he was still in high school. His poem Four Sheets to the Wind And a One-Way Ticket to France was published in his posthumous collection of poetry written about or dedicated to Richard Wright, The Wright Poems (1972).\n\n\n\nConrad Kent River’s The Wright Poems. Issued as Volume Eighteen in Paul Breman’s “Heritage Series” of African-American poetry chapbooks.\n\n\n\nFour Sheets to the Wind  And a One Way Ticket to France\n\nAs a child  I bought a red scarf and women told me how beautiful it looked,  wandering through the sous-sols as France wandered through me.\nIn the evenings  I would watch the funny people make love the way Maupassant said,  my youth allowed me the opportunity to hear all those strange  verbs conjugated in erotic affirmations. I knew love at twelve.\nWhen Salassie went before his peers and Dillinger goofed  I read in two languages, not really caring which one belonged to me.\nMy mother lit a candle for King George, my father went broke, we died.  When I felt blue the Champs understood, and when it was crowded  the alley behind Harry’s New York Bar soothed my restless spirit.\nI liked to watch the nonconformists gaze at the paintings  along Gauguin’s bewildered paradise.\nBraque once passed me in front of Café Misique.  I used to watch those sneaky professors examine the populace.  Americans never quite fitted in but they tried, so we smiled.\nI guess the money was too much for my folks.  Hitler was such a prig and a scare. We caught the long boat.  I stayed.\nMain Street was never the same. I read Gide and tried to  translate Proust. Now nothing is real except French wine.  For absurdity is reality, my loneliness unreal, my mind tired.\nAnd I shall die an old Parisian."
  },
  {
    "objectID": "posts/2019-12-11-helping-the-earth/2019-12-11-helping-the-earth.html",
    "href": "posts/2019-12-11-helping-the-earth/2019-12-11-helping-the-earth.html",
    "title": "Helping the Earth",
    "section": "",
    "text": "./assets/body-header.qmd\nOver the last few years I have tried to be a better steward of Planet Earth. There are a few specific actions that I have committed to that have actually had an impact on my life on a day-to-day basis. I share them with you in case you want to try them as well."
  },
  {
    "objectID": "posts/2019-12-11-helping-the-earth/2019-12-11-helping-the-earth.html#catalog-choice",
    "href": "posts/2019-12-11-helping-the-earth/2019-12-11-helping-the-earth.html#catalog-choice",
    "title": "Helping the Earth",
    "section": "Catalog Choice",
    "text": "Catalog Choice\nCatalog Choice is a non-profit organizatin whose mission is “to stop junk mail for good”. In their own words,\n\nCatalog Choice sends merchants your catalog opt-out request on your behalf, but please note that we have no control over how and when merchants will process your opt-out request. We’re constantly trying to work with merchants to encourage them to comply with your opt-out requests.\n\nSince signing up and entering in the unwanted catalogs, we no longer receive those annoying catalogs that we never opted in for. This is fantastic, as we used to just immediately toss them into the recyclying. But not getting them in the first place is better. I highly recommend this."
  },
  {
    "objectID": "posts/2019-12-11-helping-the-earth/2019-12-11-helping-the-earth.html#adopt-a-drain",
    "href": "posts/2019-12-11-helping-the-earth/2019-12-11-helping-the-earth.html#adopt-a-drain",
    "title": "Helping the Earth",
    "section": "Adopt-a-Drain",
    "text": "Adopt-a-Drain\nI adopted a drain at https://adopt-a-drain.org/ about a year ago. The idea is:\n\nVolunteer fifteen minutes, twice a month, for cleaner waterways and healthier communities. Storm drains flow directly to local lakes, rivers, and wetlands, acting as a conduit for trash and organic pollutants. Adopt a Drain asks residents to adopt a storm drain in their neighborhood and keep it clear of leaves, trash, and other debris to reduce water pollution.\n\nIt is easy, and kind of fun. After adopting a drain, you can set up a profile for it, and even give it a name (like a Cabbage Patch Kid). My drain is named Mandelbrot.\n\n\n\nMy drain’s profile on adopt-drain.org"
  },
  {
    "objectID": "posts/2019-12-11-helping-the-earth/2019-12-11-helping-the-earth.html#planted-my-lawn-with-native-prairie",
    "href": "posts/2019-12-11-helping-the-earth/2019-12-11-helping-the-earth.html#planted-my-lawn-with-native-prairie",
    "title": "Helping the Earth",
    "section": "Planted My Lawn with Native Prairie",
    "text": "Planted My Lawn with Native Prairie\nThis has been the most enjoyable and therapeutic of all my stewardship endeavors. I started adding plants to our yard as soon as we purchased our house (in 2007). This was in part to beautify the rather drab suburban lawn we inherited, but also in part to cut down on the amount of lawn to mow.\nA couple of years ago, I committed to tilling up a good chunk of our yard and planting native prairie seed. We bought our seed from a local company, Prairie Restoration, and the transformation was incredible. Within the first year we were seeing an increased butterfly, insect, and bird population. It is completely worth it.\n\n \n\nPics of our yard."
  },
  {
    "objectID": "posts/2020-03-01-book-publishing-in-academia/2020-03-01-book-publishing-in-academia.html",
    "href": "posts/2020-03-01-book-publishing-in-academia/2020-03-01-book-publishing-in-academia.html",
    "title": "Book Publishing in Academia",
    "section": "",
    "text": "./assets/body-header.qmd\nIn 2019, I read a book called Patience & Fortitude: A Roving Chronicle of Book People, Book Places, and Book Culture. Aside from being incredibly entertaining and interesting, there was a passage that struck a chord with me that related to academic publishing. In this passage, James Billington, Librarian of Congress from 1987–2015, was lamenting the degradation of the book through the “hyperspecialization and bureaucratization” found in the academic monograph.\nHe predicts that in the future these “books” that are primarily filled with data and are geared toward a very specialist audience will (hopefully) be relegated to the electronic format and no longer published in book form. In his words:\n\n\nBook publishing now plays a role in the certification of academic guilds by struggling into publication things with very limited interest that are written in jargon for a very narrow audience. They are not written in a language that enriches and improves and extends the human condition, but are merely written in a kind of bad functional form of communication among experts. That stuff should go online because that’s where it belongs. And I believe that is going to help the scholarly world, because right now there is a real divide in academia between those who get their books published, and those who don’t.\n\nJames Billington, Patience & Fortitude, p. 518\nAlthough Billington made this comment in the early 2000s, this resonated with me and the experiences I have had working with publishing companies. I am one of the lucky who have had their work published in book form. However, I have been outspoken about the outrageous prices (due to low publication runs and just over-pricing of this work in general) that are attached to this work. Since my experience working with Wiley in 2008, I have subsequently made all of my statistics writing (notes, textbooks, etc.) available for free online.\nThe ease of self-publishing a book online is trivial today with free tools like bookdown and github. Critics might argue that this ease leads to more ‘junk’ being published and we should weight the work that is published by a publisher more. I disagree. Publishers are interested in making money. Period. Work they deem will make money is published. Work that won’t is not. This is not how we should judge an academic portfolio.\nIf work is good, no matter what the avenue of publication, we should give academics credit. Our problem is that the old system of evaluation has not caught up to the way that many current academics (especially those positing and embracing ideas of Open Science) tend to make their work available. In my opinion, this is not only unfortunate, but a major problem. Science needs to be available to other academics, and to the general public. (This might be the one thing that I do agree with the Trump administration on!) It is time for the whole academy to start thinking about what scholarship is in today’s world and how the framework for evaluation of academics can change to consider more open accessible avenues of publication.\n\n\nReference\n\nBasebanes, N. A. (2001). Patience & Fortitude: A Roving Chronicle of Book People, Book Places, and Book Culture.* HarperCollins."
  },
  {
    "objectID": "posts/2021-02-15-trivia-weekend/index.html",
    "href": "posts/2021-02-15-trivia-weekend/index.html",
    "title": "Trivia Weekend",
    "section": "",
    "text": "./assets/body-header.qmdThis last weekend I competed in KVSC’s annual Trivia Weekend. Usually this is a 50 consecutive hour contest, but for 2021, it was only 33 hours. The last few years I competed with the team Take Me Back to Hackensack and we finished 31st out of 58 teams. Here is a photo of our team for this year.\n\nThis year we had a small team; between 2 and 6 players, depending on the hour, with four of us that were the base. Team size is unlimited, and the teams that do well in the contest are big. When we finished 8th (in 2006) our team had 20-30 members every hour.\nThe questions are hard and are more Google detective than trivia, per se. For example, one of the questions this year was:\n\nChills, and thrills. This 1968 comic featured a poster advertising a film playing at the Bijou theater. Name the comic strip on which poster appears, the name of film playing at the theater in the comic strip, an the month/date the comic strip first appeared.\n\nThe answer: Bugs Bunny / The Fiend / Sept 24\nWe did not get this one correct. Another question:\n\nKellogg’s ran a limited edition of fruit loops / tropical. On the back of box are 4 games. What was the name of the game that wanted you to locate 4 fruit of wrong color?\n\nAnswer: Flipped Fruit\nWe did get that one. The questions vary in point value (10 to 250 points), and also in how long you have to answer. The questions are read on the air, and you call your answer in to a phone line. It requires team communication, people searching, people calling in answers, and a high degree of sleep deprivation (at least it did when I was younger than I am now).\nI started playing trivia in 1989, when I cameoed on the team Hindu, Quaker, Sophomore Guys. I have played most years since. We changed our team name almost every year in the beginning (we were young and bucked tradition; I was trying to remember some of these names but the trivia history page only goes back to 1994). We then, consistently became Intimate Tupperware Party (or some variation thereof) from 1994-2012."
  },
  {
    "objectID": "posts/2018-04-11-r-markdown-theme-gallery/2018-04-11-r-markdown-theme-gallery.html",
    "href": "posts/2018-04-11-r-markdown-theme-gallery/2018-04-11-r-markdown-theme-gallery.html",
    "title": "R Markdown Theme Gallery",
    "section": "",
    "text": "./assets/body-header.qmd\nR Markdown is a great way to integrate R code into a document. An example of the default theme used in R Markdown HTML documents is shown below."
  },
  {
    "objectID": "posts/2018-04-11-r-markdown-theme-gallery/2018-04-11-r-markdown-theme-gallery.html#pre-packaged-themes",
    "href": "posts/2018-04-11-r-markdown-theme-gallery/2018-04-11-r-markdown-theme-gallery.html#pre-packaged-themes",
    "title": "R Markdown Theme Gallery",
    "section": "Pre-Packaged Themes",
    "text": "Pre-Packaged Themes\nThere are several other canned themes you can use rather than the default theme. There are 12 additional themes that you can use without installing any other packages: “cerulean”, “cosmo”, “flatly”, “journal”, “lumen”, “paper”, “readable”, “sandstone”, “simplex”, “spacelab”, “united”, and “yeti”. You can read the R Markdown documentation for detail about how to change your theme from the default. Below are screenshots for each of them.\n\ncerulean Theme\n\n\n\ncosmo Theme\n\n\n\nflatly Theme\n\n\n\njournal Theme\n\n\n\nlumen Theme\n\n\n\npaper Theme\n\n\n\nreadable Theme\n\n\n\nsandstone Theme\n\n\n\nsimplex Theme\n\n\n\nspacelab Theme\n\n\n\nunited Theme\n\n\n\nyeti Theme"
  },
  {
    "objectID": "posts/2018-04-11-r-markdown-theme-gallery/2018-04-11-r-markdown-theme-gallery.html#even-more-themes",
    "href": "posts/2018-04-11-r-markdown-theme-gallery/2018-04-11-r-markdown-theme-gallery.html#even-more-themes",
    "title": "R Markdown Theme Gallery",
    "section": "Even More Themes",
    "text": "Even More Themes\nThere are several add-on R packages that you can install to implement even more R Markdown HTML themes. For example prettydoc, rmdformats, hrbrthemes, tufte, and tint. Below I show screenshots of the themes from these packages as well.\n\nipsum Theme (from hrbrthemes Package)\n\n\n\narchitect Theme (from prettydoc Package)\n\n\n\ncayman Theme (from prettydoc Package)\n\n\n\nhpstr Theme (from prettydoc Package)\n\n\n\nleonids Theme (from prettydoc Package)\n\n\n\ntactile Theme (from prettydoc Package)\n\n\n\nhtml_clean Theme (from rmdformats Package)\n\n\n\nhtml_docco Theme (from rmdformats Package)\n\n\n\nmaterial Theme (from rmdformats Package)\n\n\n\nreadthedown Theme (from rmdformats Package)\n\n\n\ntintHtml Theme (from tint Package)\n\n\n\ntufte_html Theme (from tufte Package)"
  },
  {
    "objectID": "posts/2019-06-03-richard-hamming-on-calculus/2019-06-03-richard-hamming-on-calculus.html",
    "href": "posts/2019-06-03-richard-hamming-on-calculus/2019-06-03-richard-hamming-on-calculus.html",
    "title": "Richard Hamming on the Teaching of Mathematics",
    "section": "",
    "text": "./assets/body-header.qmd\n\n\nThe way mathematics is currently taught it is exceedingly dull. In the calculus book we are currently using on my campus, I found no single problem whose answer I felt the student would care about! The problems in the text have the dignity of solving a crossword puzzle — hard to be sure, but the result is of no significance in life.\n\nRichard Hamming, Calculus and Discrete Mathematics\n\n\nReference\n\nHamming, R. (1984). Calculus and discrete mathematics. The College Mathematics Journal, 15(5), 388–389."
  },
  {
    "objectID": "posts/2019-05-01-cv-tips/2019-05-01-cv-tips.html",
    "href": "posts/2019-05-01-cv-tips/2019-05-01-cv-tips.html",
    "title": "CV Tips",
    "section": "",
    "text": "./assets/body-header.qmd\nMy colleague Joseph Rios and I helped organize a monthly reading and seminar series, QME and Friends Read, for interested graduate students in our program. In March, we hosted a seminar on the Curriculum Vitae, or CV. In preparing for that seminar, I looked through many faculty and student CVs. I also read many blog posts, and scholarly work about putting together a CV. Based on all of this and my own experiences and advice given to me, I decided to put together a post (maybe multiple posts) that includes some suggestions for compiling a CV."
  },
  {
    "objectID": "posts/2019-05-01-cv-tips/2019-05-01-cv-tips.html#start-early-update-often",
    "href": "posts/2019-05-01-cv-tips/2019-05-01-cv-tips.html#start-early-update-often",
    "title": "CV Tips",
    "section": "Start Early; Update Often",
    "text": "Start Early; Update Often\nStart putting together your CV as soon as you begin your graduate studies. Or, at the latest after your first term in graduate school. Update your CV after each term. In talking with faculty, many of my colleagues update their CV on a regular basis, for some this is monthly, for others weekly. The frequency will likely change as you progress through your academic career, and finding the right frequency is different for everyone. As such, the frequency is less important here than the regularity. Updating your CV is a scholarly habit that you will need to build. As you do more things, it is important to record them lest you forget and have to try and remember what you did three months ago."
  },
  {
    "objectID": "posts/2019-05-01-cv-tips/2019-05-01-cv-tips.html#content-and-sequencing",
    "href": "posts/2019-05-01-cv-tips/2019-05-01-cv-tips.html#content-and-sequencing",
    "title": "CV Tips",
    "section": "Content and Sequencing",
    "text": "Content and Sequencing\nWhat should be included in your CV? A CV is different from a Resume. Curriculum Vitae literally means “course of life” and is meant to lay out the entirety of your academic life. This means that you will want to include information, among other things, on your:\n\nEducation\nAcademic Appointments\nTeaching Experience\nAcademic Honors and Awards\nPublications\n\nRefereed Journal Articles\nBooks\nBook Chapters\nConference Proceedings\nOther Publications (e.g., Book reviews, blogs, non-refereed pieces)\n\nGrants and Fellowships\nPresentations\n\nInvited Presentations\nWorkshops\n\nProfessional Service\n\nService to the Department, College, and University\nService to the Profession (e.g., journals you review for)\nService to the Community\n\n\nWhen you initially put your CV together, some of these categories may be combined. For example, you may not have any invited presentations/talks, so you might include a single heading of “Presentations”. Later in your career, as you accumulate invited talks, you may choose to categorize those entries under a separate sub-heading of “Invited Presentations”.\nThe sequencing of these headings varies, although typically “Education” and “Academic Appointments” are the first two things listed in the CV. After that, you might use the sequence of headings as an indication of importance. For example, since I am in a teaching-oriented position I include “Teaching Experience”” and “Teaching Honors and Awards” before “Publications” and “Presentations”. If I were in a traditional tenure track position, I might reverse that. (Note that if you take a tenure-track position, many universities have a template for your CV that they want you to adhere to, including the sequence of categories.)"
  },
  {
    "objectID": "posts/2019-05-01-cv-tips/2019-05-01-cv-tips.html#general-formattingtypography",
    "href": "posts/2019-05-01-cv-tips/2019-05-01-cv-tips.html#general-formattingtypography",
    "title": "CV Tips",
    "section": "General Formatting/Typography",
    "text": "General Formatting/Typography\nMake your CV look good. It is the first impression other academics have of you as a professional if they have not met you in person. Consider the same set of publications in the figures below. The first figure below shows those publications written using the default font in Word (Calibri; ugh) and very little formatting.\n\n\nNow, the same publications have been written using Bembo Standard, a readable serifed font. They have also been formatted and the links have been colored.\n\nFont matters. Formatting matters."
  },
  {
    "objectID": "posts/2019-05-01-cv-tips/2019-05-01-cv-tips.html#some-tips-for-different-sections",
    "href": "posts/2019-05-01-cv-tips/2019-05-01-cv-tips.html#some-tips-for-different-sections",
    "title": "CV Tips",
    "section": "Some Tips for Different Sections",
    "text": "Some Tips for Different Sections\nHere I lay out some advice for different sections of the CV. These are not hard-and-fast rules, but are based on advice given to me and things I have picked up from reviewing many CVs (for annual student reviews, job candidates, etc.) over the years.\n\nEducation\nThis is essentially a list of your degrees. List them from most recent to least recent. Give the year of completion, the institution and department for each degree. For example mine looks like this:\n\nEDUCATION\n\n2006    Ph.D., University of Minnesota. Quantitative Methods in Education, Department of Educational Psychology\n\n\n1998    B.S., Saint Cloud State University. Mathematics Education\n\n\n\nIf you are a Ph.D. student, you may also provide your thesis advisor (and also the thesis title if you are ABD). For example\n\nEDUCATION\n\n2020 (Anticipated)    Ph.D., University of Minnesota. Quantitative Methods in Education, Department of Educational Psychology\n\n\n   Thesis: A Longitudinal Investigation of the Development of College Students’ Reasoning About Bivariate Data\n\n\n   Thesis Advisor: Joan Garfield\n\n\n1998    B.S., Saint Cloud State University. Mathematics Education\n\n\n\n\nPublications and Presentations\nFor any of the categories where you are including bibliographic entries, use the style that your profession uses to format these. For example, in Educational Psychology we use APA. So all of my publications and presentations are formatted using the APA style. Within each category, it is also helpful if you:\n\nOrder the entries from most recent to least recent.\nBold your name.\nIndex each entry with a count. Make the oldest entry 1 and count forward in time. This has the advantage that you can easily find out how many entries you have by looking at the most recent, and also that you don’t have to re-number every time you add a new entry. (You will be amazed at how often you need to provide the counts of these things.)\nIf you have authored a publication with your students, indicate that. I include a super-scripted dagger next to a students name.\n\nHere for example is how I have listed my “Other Publications”.\n\nOTHER PUBLICATIONS\n\n[5]  Zieffler, A., & Justice, N.† (2015). Teardowns, historical renovation, and paint-and-patch: Curricular changes and faculty development. Invited comment on G. Cobb, ‘Mere renovation is too little too late: We need to Rethink our Undergraduate Curriculum from the Ground Up’. The American Statistician, 69(4). http://dx.doi.org/10.1080/00031305.2015.1093029\n\n\n[4]  Isaak, R., Zieffler, A., & Garfield, J. (2013). Response. Technology Innovations in Statistics Education, 7(3). http://escholarship.org/uc/item/8gq9t3c3\n\n\n[3]  Gill, B., Zieffler, A., Boynton, N., Alberts, K. S., Humphrey, P., McKenzie, J., & Posner, M. A. (2012). Response to ‘Statistics à la Mode’. AmStat News, February, x–xx. http://magazine.amstat.org/blog/2012/ 02/01/maa-response/\n\n\n[2]  Garfield, J., & Zieffler, A. (2011). Response to: “Towards more accessible conceptions of statistical inference” by C. Wild, M. Pfannkuch, M. Regan and N. J. Horton. Journal of the Royal Statistical Society. Series A, 174(Part 2), 280.\n\n\n[1]  Zieffler, A. (1999). A statistical journey with ninth grade students. Math Times, Winter, 14."
  },
  {
    "objectID": "posts/2018-03-03-boys-hockey/2018-03-03-boys-hockey.html",
    "href": "posts/2018-03-03-boys-hockey/2018-03-03-boys-hockey.html",
    "title": "Minnesota State High School Boys Hockey Predictions",
    "section": "",
    "text": "./assets/body-header.qmd\nThe state high school boys hockey tournament, scheduled for March 7–10, is one of the premiere sporting events in the state of Minnesota. According to Wikipedia, this event has drawn over 100,000 spectators 22 times in its history, eclipsing 135,000 spectoators in 2015. Many national caliber players played high school hockey in Minnesota, several taking part in the state tournament. Names like Neal Broten, Herb Brooks, and T. J. Oshie are alumni of state tournaments past.\nMinnesota high school hockey teams are split into two classes based on school enrollment sizes; the largest 64 schools are classified as Class AA and the remainder are classified as Class A. Each of these classes are subdivided into eight sections. (Sections are based on a combination of school location and competitivness.)\nThis year’s state tournament features 16 teams (8 from Class A and 8 from Class AA) that qualified for the tournament by winning their section tournament. Within each class, the top five teams are seeded (ranked) by the coaches of the teams that qualified for the state tournament. The #4 and #5 seeds play each other and the remaining three teams in each class are assigned by lottery to play the #1, #2, and #3 seeded teams. The seeds in the 2018 tournament, announced on March 3, are:\nTop eight seeds based on the 2018 state tournament bracket. The 6–8 seeds are based on the opponents the teams were assigned to in the lottery.\n\n\nSeed\nA\nAA\n\n\n\n\n1\nHermantown\nMinnetonka\n\n\n2\nMahtomedi\nEdina\n\n\n3\nOrono\nDuluth East\n\n\n4\nAlexandria\nSt. Thomas Academy\n\n\n5\nThief River Falls\nCentennial\n\n\n6\nLitchfield/Dassel-Cokato\nSt. Michael-Albertville (STMA)\n\n\n7\nMankato East/Loyola\nLakeville North\n\n\n8\nMonticello\nHill-Murray\nI wanted to make compute probabilities, à la fivethirtyeight, for which each team’s chances of winning the state tournament. The methodology I used to do this was:"
  },
  {
    "objectID": "posts/2018-03-03-boys-hockey/2018-03-03-boys-hockey.html#scraping-the-game-level-data",
    "href": "posts/2018-03-03-boys-hockey/2018-03-03-boys-hockey.html#scraping-the-game-level-data",
    "title": "Minnesota State High School Boys Hockey Predictions",
    "section": "Scraping the Game-Level Data",
    "text": "Scraping the Game-Level Data\nTo begin, I had to obtain the game data for both the regular season and the section tournaments. MN Hockey Hub is a website that has the results for all high school hockey games played in Minnesota. I used the rvest package to scrape these data. Unfortunately, MN Hockey Hub separates their regular season and section playoffs, so I had to run the scrape on each separately and then combine the data.\n\n# Beginning and ending dates for regular season games\nstart = as.Date(\"2017/11/22\", format = \"%Y/%m/%d\")\nend   = as.Date(\"2018/02/17\",format=\"%Y/%m/%d\")\n\n# Create an empty list with 101 elements\nresults = rep(list(NA), 88) \n\n# Initialize values\ntheDate = start\ni = 1\n\n# Loop over dates\nwhile (theDate &lt;= end){\n  url = paste0(\"http://www.mnhockeyhub.com/schedule/day/league_instance/60876/\",\n               format(theDate,\"%Y/%m/%d\"),\n              \"?subseason=434877&referrer=3596811\"\n               )\n  \n  results[[i]] = as.data.frame(\n    url %&gt;% \n      read_html() %&gt;% \n      html_table()\n    ) %&gt;% \n    mutate(date = theDate)\n  theDate = theDate + 1 \n  i = i + 1\n}\n\n## Scrape section games\nstart = as.Date(\"2018/02/17\", format = \"%Y/%m/%d\")\nend   = as.Date(\"2018/03/02\",format=\"%Y/%m/%d\")\nresults2 = rep(list(NA), 14) \ntheDate = start\ni = 1\n\nwhile (theDate &lt;= end){\n  url = paste0(\"http://www.mnhockeyhub.com/schedule/day/league_instance/60876/\",\n               format(theDate,\"%Y/%m/%d\"),\n               \"?subseason=486996\"\n  )\n  \n  results2[[i]] = as.data.frame(\n    url %&gt;% \n      read_html() %&gt;% \n      html_table()\n  ) %&gt;% \n    mutate(date = theDate)\n  theDate = theDate + 1 \n  i = i + 1\n}\n\nI then took these scraped data, and put them into a dataframe using do.call() and formatted the data frame using dplyr functions.\n\n# Transform lists to data frames\nhockey_reg_season = do.call(rbind, results)\nhockey_sections = do.call(rbind, results2)\n\n# Format the data\nlibrary(dplyr)\n\nhockey = rbind(hockey_reg_season, hockey_sections) %&gt;%\n  select(date,\n         home = Home,\n         home_score = H,\n         visitor = Visitor,\n         visitor_score = V,\n         location = Location\n  ) %&gt;%\n  mutate(\n    home_score = as.integer(home_score),\n    visitor_score = as.integer(visitor_score)\n  ) %&gt;%\n  tidyr::drop_na()\n\nAt this point I did a data integrity check. The first thing I noticed was that there were games in the data on every day from the start of the regular season through the section playoffs. I knew somethibng was wrong as high school teams do not play games on Sundays or on holidays. In looking at those days, it turned out the web scraper just duplicated games from future days (e.g., there was no game on 12-25-2017, so it just skipped ahead to the next day a game was played, 12-26-2017, and put those games in the December 25th date. It then also put them in the 12-26-2017 date as well.) Rather than try to program a solution in R, I outputted the data to a CSV file and manually removed the few dates that there were no games played.\n\n\n# A tibble: 2,047 × 7\n   date     home               home_score visitor  visitor_score location season\n   &lt;chr&gt;    &lt;chr&gt;                   &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt; \n 1 11/22/17 Anoka                       4 Rogers               1 Anoka A… Regul…\n 2 11/22/17 Providence Academy          2 Spring …             3 Plymout… Regul…\n 3 11/22/17 St. Paul Academy            3 Bloomin…             2 Drake A… Regul…\n 4 11/24/17 Armstrong/Cooper           10 Proctor              0 Chisago… Regul…\n 5 11/24/17 Baldwin-Woodville           2 St. Pau…             4 Amery I… Regul…\n 6 11/24/17 Eastview                    5 Park of…             3 Apple V… Regul…\n 7 11/24/17 Orono                       5 East Gr…             3 Orono I… Regul…\n 8 11/24/17 Greenway                    5 Mound W…             3 Hodgins… Regul…\n 9 11/24/17 Minnesota River             2 North B…             8 Turkey … Regul…\n10 11/24/17 Edina                       7 Holy Fa…             0 Plymout… Regul…\n# ℹ 2,037 more rows"
  },
  {
    "objectID": "posts/2018-03-03-boys-hockey/2018-03-03-boys-hockey.html#compute-elo-ratings",
    "href": "posts/2018-03-03-boys-hockey/2018-03-03-boys-hockey.html#compute-elo-ratings",
    "title": "Minnesota State High School Boys Hockey Predictions",
    "section": "Compute Elo Ratings",
    "text": "Compute Elo Ratings\nAfter scraping and formatting the data, I used the elo package to compute Elo ratings. The package vignette is helpful for understanding the syntax used to fit different Elo models. The elo.run() function initializes each team’s Elo rating to 1500, and then updates each team’s rating after every game played based on whether the team won or lost. In the model I fitted, I also accounted for the score differential.\nThe rate at which each team’s Elo rating changes based on a win or loss is referred to as the \\(K\\)-factor. Larger values of \\(K\\) have a bigger change in the Elo rating. I didn’t think that result of any one hockey game should impact the rating too greatly, so I chose a \\(K\\)-factor of 15. Rather than set this to a constant value for each game, the model I used adjusted this factor based on the score differential, mathematically,\n\\[\n15 \\times \\ln(| \\mathrm{Score}_{\\mathrm{Home}} - \\mathrm{Score}_{\\mathrm{Visitor}} |) + 1\n\\]\nTaking the natural logarithm helps to lessen the impact of games where one team runs up the score on another team.\nI also considered whether Elo ratings needed to take into account a home ice advantage. The empirical data indicated that the home team in the 2017–2018 season did not have an advantage, winning about 48% of the games played. Thus, ultimately, I decided not to include any home ice advantage (or disadvantage).\nSince during the regular season teams play games with other teams that are in other classes, I opted to fit one model taht included teams from both classes rather than fit the model within class.\n\nlibrary(elo)\nelo_reg_season = elo.run(score(home_score, visitor_score) ~ home + visitor +\n                    k(15*log(abs(home_score - visitor_score) + 1)), data = hockey)\n\nThe final Elo ratings and rankings for teams that qualified for the state tournament are shown below.\n\n\n\nElo ratings and rankings for the eight Class A teams that qualified for the state tournament.\n\n\nTeam\nElo\nRank\n\n\n\n\nHermantown\n1639.196\n11\n\n\nMahtomedi\n1624.538\n17\n\n\nOrono\n1609.513\n24\n\n\nAlexandria\n1587.179\n31\n\n\nThief River Falls\n1571.198\n39\n\n\nLitchfield/Dassel-Cokato\n1562.803\n42\n\n\nMankato East\n1551.141\n48\n\n\nMonticello\n1530.773\n60\n\n\n\n\n\n\n\n\nElo ratings and rankings for the eight Class AA teams that qualified for the state tournament.\n\n\nTeam\nElo\nRank\n\n\n\n\nMinnetonka\n1718.589\n1\n\n\nEdina\n1716.570\n2\n\n\nDuluth East\n1693.757\n3\n\n\nSt. Thomas Academy\n1691.315\n4\n\n\nCentennial\n1655.561\n7\n\n\nSTMA\n1625.291\n16\n\n\nLakeville North\n1578.414\n38\n\n\nHill-Murray\n1557.501\n46\n\n\n\n\n\nComparing these to the actual tournament seeds, we see several differences in the top four seeds. In Class A, the coaches included Alexandria in the top five, while our Elo model put Monticello in the top four. In Class AA, the Elo model and the coaches selected the same top five teams, but had a different rank ordering for those teams. There are also some ranking differences between the coaches picks and our Elo model for the other teams."
  },
  {
    "objectID": "posts/2018-03-03-boys-hockey/2018-03-03-boys-hockey.html#simulate-the-state-tournament",
    "href": "posts/2018-03-03-boys-hockey/2018-03-03-boys-hockey.html#simulate-the-state-tournament",
    "title": "Minnesota State High School Boys Hockey Predictions",
    "section": "Simulate the State Tournament",
    "text": "Simulate the State Tournament\nWe can now use these Elo ratings to determine the probability that one team would beat another. For example, in the Class A quarterfinal game between Hermantown (#1) and Monticello (#8), Hermantown’s probability of beating Monticello is 0.574 .\n\npredict(elo_reg_season, data.frame(home = \"Hermantown\", visitor = \"Monticello\"))\n\nI simulated the state tournament by using a random-number generator to determine the winner of each game. For instance, to simulate the Hermantown/Monticello game, I used the runif() function to generate a random number drawn from the uniform distribution between 0 and 1. If the result is less than or equal to 0.574, Hermantown is the winner; if not, Monticello wins. The syntax for simulating the Class A state tournament 10,000 times is below.\n\n# Enter teams in rank order\nteam_1 = \"Hermantown\"\nteam_2 = \"Mahtomedi\"\nteam_3 = \"Orono\"\nteam_4 = \"Alexandria\"\nteam_5 = \"Thief River Falls\"\nteam_6 = \"Litchfield/Dassel-Cokato\"\nteam_7 = \"Mankato East\"\nteam_8 = \"Monticello\"\n\n# Set up empty vector to store winner in\nchampion = rep(NA, 10000)\n\n\nfor(i in 1:10000){\n  \n  ### SIMULATE THE QUARTEFINALS\n  \n  # Predict Game 1 winner: team_1 vs. team_8\n  p_game_1 = predict(elo_reg_season, data.frame(home = team_1, visitor = team_8))\n  w_game_1 = ifelse(runif(1, min = 0, max = 1) &lt;= p_game_1, team_1, team_8)\n  \n  # Predict Game 2 winner: team_4 vs. team_4\n  p_game_2 = predict(elo_reg_season, data.frame(home = team_4, visitor = team_5))\n  w_game_2 = ifelse(runif(1, min = 0, max = 1) &lt;= p_game_2, team_4, team_5)\n  \n  # Predict Game 3 winner: team_3 vs. team_6\n  p_game_3 = predict(elo_reg_season, data.frame(home = team_3, visitor = team_6))\n  w_game_3 = ifelse(runif(1, min = 0, max = 1) &lt;= p_game_3, team_3, team_6)\n  \n  # Predict Game 4 winner: team_2 vs. team_7\n  p_game_4 = predict(elo_reg_season, data.frame(home = team_2, visitor = team_7))\n  w_game_4 = ifelse(runif(1, min = 0, max = 1) &lt;= p_game_4, team_2, team_7)\n  \n  \n  ### SIMULATE THE SEMIFINALS\n  \n  # Predict Game 5 winner: winner Game 1 vs. winner Game 2\n  p_game_5 = predict(elo_reg_season, data.frame(home = w_game_1, visitor = w_game_2))\n  w_game_5 = ifelse(runif(1, min = 0, max = 1) &lt;= p_game_5, w_game_1, w_game_2)\n  \n  # Predict Game 6 winner: winner Game 3 vs. winner Game 4\n  p_game_6 = predict(elo_reg_season, data.frame(home = w_game_4, visitor = w_game_3))\n  w_game_6 = ifelse(runif(1, min = 0, max = 1) &lt;= p_game_6, w_game_4, w_game_3)\n  \n  \n  ### SIMULATE THE FINALS\n  \n  # Predict Game 5 winner: winner Game 1 vs. winner Game 2\n  p_game_7 = predict(elo_reg_season, data.frame(home = w_game_5, visitor = w_game_6))\n  w_game_7 = ifelse(runif(1, min = 0, max = 1) &lt;= p_game_7, w_game_5, w_game_6)\n  \n  champion[i] = w_game_7\n  \n}\n\nNow I can compute the proportion of times each team “won” the state tournament.\n\ndata.frame(champion) %&gt;% \n  group_by(champion) %&gt;%\n  summarize(Probability = length(champion)/10000) %&gt;%\n  arrange(desc(Probability))\n\n\n\n\nProbability that each of the eight Class A teams will win the state tournament.\n\n\nTeam\nProbability\n\n\n\n\nHermantown\n0.1987\n\n\nMahtomedi\n0.1802\n\n\nOrono\n0.1612\n\n\nAlexandria\n0.1127\n\n\nThief River Falls\n0.1086\n\n\nLitchfield/Dassel-Cokato\n0.0939\n\n\nMankato East\n0.0812\n\n\nMonticello\n0.0635\n\n\n\n\n\nBased on these simulations, Mahtomedi, Hermantown and Orono all have about an equal chance of winning the Class A tournament.\nI also carried out a similar simulation for the Class AA tournament.\n\n# Enter teams in rank order\nteam_1 = \"Minnetonka\"\nteam_2 = \"Edina\"\nteam_3 = \"Duluth East\"\nteam_4 = \"St. Thomas Academy\"\nteam_5 = \"Centennial\"\nteam_6 = \"STMA\"\nteam_7 = \"Lakeville North\"\nteam_8 = \"Hill-Murray\"\n\n# Set up empty vector to store winner in\nchampion2 = rep(NA, 10000)\n\n\nfor(i in 1:10000){\n  \n  ### SIMULATE THE QUARTEFINALS\n  \n  # Predict Game 1 winner: team_1 vs. team_8\n  p_game_1 = predict(elo_reg_season, data.frame(home = team_1, visitor = team_8))\n  w_game_1 = ifelse(runif(1, min = 0, max = 1) &lt;= p_game_1, team_1, team_8)\n  \n  # Predict Game 2 winner: team_4 vs. team_4\n  p_game_2 = predict(elo_reg_season, data.frame(home = team_4, visitor = team_5))\n  w_game_2 = ifelse(runif(1, min = 0, max = 1) &lt;= p_game_2, team_4, team_5)\n  \n  # Predict Game 3 winner: team_3 vs. team_6\n  p_game_3 = predict(elo_reg_season, data.frame(home = team_3, visitor = team_6))\n  w_game_3 = ifelse(runif(1, min = 0, max = 1) &lt;= p_game_3, team_3, team_6)\n  \n  # Predict Game 4 winner: team_2 vs. team_7\n  p_game_4 = predict(elo_reg_season, data.frame(home = team_2, visitor = team_7))\n  w_game_4 = ifelse(runif(1, min = 0, max = 1) &lt;= p_game_4, team_2, team_7)\n  \n  \n  ### SIMULATE THE SEMIFINALS\n  \n  # Predict Game 5 winner: winner Game 1 vs. winner Game 2\n  p_game_5 = predict(elo_reg_season, data.frame(home = w_game_1, visitor = w_game_2))\n  w_game_5 = ifelse(runif(1, min = 0, max = 1) &lt;= p_game_5, w_game_1, w_game_2)\n  \n  # Predict Game 6 winner: winner Game 3 vs. winner Game 4\n  p_game_6 = predict(elo_reg_season, data.frame(home = w_game_4, visitor = w_game_3))\n  w_game_6 = ifelse(runif(1, min = 0, max = 1) &lt;= p_game_6, w_game_4, w_game_3)\n  \n  \n  ### SIMULATE THE FINALS\n  \n  # Predict Game 5 winner: winner Game 1 vs. winner Game 2\n  p_game_7 = predict(elo_reg_season, data.frame(home = w_game_5, visitor = w_game_6))\n  w_game_7 = ifelse(runif(1, min = 0, max = 1) &lt;= p_game_7, w_game_5, w_game_6)\n  \n  champion2[i] = w_game_7\n  \n}\n\n\ndata.frame(champion2) %&gt;% \n  group_by(champion2) %&gt;%\n  summarize(Probability = length(champion2)/10000) %&gt;%\n  arrange(desc(Probability))\n\n\n\n\nProbability that each of the eight Class AA teams will win the state tournament.\n\n\nTeam\nProbability\n\n\n\n\nMinnetonka\n0.2098\n\n\nEdina\n0.2083\n\n\nDuluth East\n0.1852\n\n\nSt. Thomas Academy\n0.1495\n\n\nCentennial\n0.1016\n\n\nSTMA\n0.0701\n\n\nLakeville North\n0.0416\n\n\nHill-Murray\n0.0339\n\n\n\n\n\nThe simulation results suggest that both Edina and St. Thomas Academy have a pretty good shot of winning the tournament, and Minnetonka and Duluth East are also in the mix."
  },
  {
    "objectID": "posts/2020-02-09-apa-style-table-in-rmarkdown-html-version/2020-02-09-apa-style-table-in-rmarkdown-html-version.html",
    "href": "posts/2020-02-09-apa-style-table-in-rmarkdown-html-version/2020-02-09-apa-style-table-in-rmarkdown-html-version.html",
    "title": "APA Style Table in RMarkdown (HTML Version)",
    "section": "",
    "text": "./assets/body-header.qmd\nI will try to replicate the the following table from the 7th edition of the APA Publication Manual."
  },
  {
    "objectID": "posts/2020-02-09-apa-style-table-in-rmarkdown-html-version/2020-02-09-apa-style-table-in-rmarkdown-html-version.html#styling-the-table-using-css",
    "href": "posts/2020-02-09-apa-style-table-in-rmarkdown-html-version/2020-02-09-apa-style-table-in-rmarkdown-html-version.html#styling-the-table-using-css",
    "title": "APA Style Table in RMarkdown (HTML Version)",
    "section": "Styling the Table Using CSS",
    "text": "Styling the Table Using CSS\nThis is pretty good, except for the borders (we have several horizontal lines that need to be deleted, and we need to draw borders above and below the header row and at the bottom of the table body) and the header row (which is bold). In an HTML document, we can change styles by including a Cascading Style Sheet file (CSS file).\nThe first thing we need to do is create a CSS file and save it in the same folder/directory as your RMD document. To do this,\n\nUnder the RStudio File menu, select New File &gt; Text File\n\nSave this file as table_style.css in the exact same folder as your RMD file. The file extension css indicate that this is a CSS file.\nNext, we need to tell your RMarkdown file to look at this file for its styling rules. We do this in the YAML of your RMD document. Change your YAML as follows:\n\n---\ntitle: \"Untitled\"\nauthor: \"Andrew Zieffler\"\ndate: \"1/12/2020\"\noutput: \n  html_document:\n    css: table-style.css\n---\n\nNow we can our styling rules to the CSS file. You can add the following syntax to table-style.css.\n\n/** Change border color to black **/\n.table&gt;thead&gt;tr&gt;th {\n  border-color: black;\n}\n\n\n/** Remove borders within the table body **/\n.table&gt;tbody&gt;tr&gt;td {\n  border: none;\n}\n\n\n/** Add a top border to the table header row **/\n.table thead tr:first-child { \n  border-top: 2px solid black;\n}\n\n\n/** Add a bottom border to the table body **/\n.table tbody tr:last-child {\n  border-bottom: 2px solid black;\n}\n\n\n/** Make the table header row a normal weight; not bold **/\n.table th{\n  font-weight: normal;\n}\n\n\n/** Make the caption italic and black **/\n.table caption{\n  font-style: italic;\n  color: black;\n}\n\n\n\nSave this and then re-knit your RMD file. Your table should now look like the following:\n\nFinally, to add the table numbering, we manually add Markdown formatted text above the code chunk that corresponds to our table name. This should be outside the code chunk!\n**Table 1**\nWhich gives us the following:"
  },
  {
    "objectID": "posts/2020-12-15-some-r-packages-to-keep-in-mind/index.html",
    "href": "posts/2020-12-15-some-r-packages-to-keep-in-mind/index.html",
    "title": "Some R Packages to Keep In Mind",
    "section": "",
    "text": "./assets/body-header.qmd\nI have written several notes to myself over the years as reminders. These include ideas for research, R packages I have seen and may use sometime), things to-do, etc. I am in the process of making some of these notes more public on my blog. This will act as a more searchable, curated “note” for myself, but also makes it available to anyone else who would benefit."
  },
  {
    "objectID": "posts/2020-12-15-some-r-packages-to-keep-in-mind/index.html#r-packages-for-productivity",
    "href": "posts/2020-12-15-some-r-packages-to-keep-in-mind/index.html#r-packages-for-productivity",
    "title": "Some R Packages to Keep In Mind",
    "section": "R Packages for Productivity",
    "text": "R Packages for Productivity\n\nblastula is a package for creating beautiful custom emails in R. It can be used to compose custom email bodies based on code, code output, and markdown; and send emails using SMTP servers - even GMail - or integrate with production services like RStudio Connect. Also available from CRAN."
  },
  {
    "objectID": "posts/2020-12-15-some-r-packages-to-keep-in-mind/index.html#packages-to-enhance-markdown-documents",
    "href": "posts/2020-12-15-some-r-packages-to-keep-in-mind/index.html#packages-to-enhance-markdown-documents",
    "title": "Some R Packages to Keep In Mind",
    "section": "Packages to Enhance Markdown Documents",
    "text": "Packages to Enhance Markdown Documents\n\n\nbookdown facilitates writing books and long-form articles/reports with R Markdown. There are several examples of books writte in bookdown and documentation available at https://bookdown.org/.\ncitr creates an RStudio addin to insert citations in an R Markdown document.\nemo can be used to easily add emoji into an R Markdown document.\nequatiomatic extracts output from the lm() function to write the equation using LaTeX.\nmarkdowntemplates includes a set of R markdown templates and knitr knit engine replacements.\npapaja can be used to easily prepare APA journal articles with R Markdown.\nvitae makes creating and maintaining a Resume or CV with R Markdown simple. It provides a collection of LaTeX templates, with helpful functions to add content to the documents.\nxaringan can be used to produce some slick slideshows with remark.js using R Markdown.\n\nxaringanExtra is a “playground of enhancements and extensions for xaringan slides”. This package also includes functions for sharing your slides on a website or Twitter.\nxaringanthemer adds some style to you xaringan produced slides."
  },
  {
    "objectID": "posts/2020-12-15-some-r-packages-to-keep-in-mind/index.html#packages-to-enhance-plots",
    "href": "posts/2020-12-15-some-r-packages-to-keep-in-mind/index.html#packages-to-enhance-plots",
    "title": "Some R Packages to Keep In Mind",
    "section": "Packages to Enhance Plots",
    "text": "Packages to Enhance Plots\n\n\ncolorblindr can simulate colorblindness in production-ready R figures.\nemoGG can be used to add emoji into your ggplots.\nextrafont makes it easier to include system fonts in your plots. Also available from CRAN.\nggrough converts your ggplot2 plots to rough/sketchy charts, using the excellent javascript roughjs library."
  },
  {
    "objectID": "posts/2020-12-15-some-r-packages-to-keep-in-mind/index.html#packages-for-creating-tables",
    "href": "posts/2020-12-15-some-r-packages-to-keep-in-mind/index.html#packages-for-creating-tables",
    "title": "Some R Packages to Keep In Mind",
    "section": "Packages for Creating Tables",
    "text": "Packages for Creating Tables\n\n\ngt creates “wonderful-looking tables using the R programming language. The gt philosophy: we can construct a wide variety of useful tables with a cohesive set of table parts. These include the table header, the stub, the column labels and spanner column labels, the table body, and the table footer.”\nkableExtra includes functionality to enhance kable() tables. There is extensive documentation for producing tables in HTML and LaTeX.\nmodelsummary creates “tables and plots to summarize statistical models and data”. These tables are also customizable.\nstargazer can be used to create tables of regression model output.\nstargazer-booktabs is a slightly modified version of the stargazer package which outputs tables using the booktabs (LaTeX) commands (\\toprule, \\midrule, and \\bottomrule) to include horizontal rules."
  },
  {
    "objectID": "posts/2020-12-15-some-r-packages-to-keep-in-mind/index.html#fun-r-packages",
    "href": "posts/2020-12-15-some-r-packages-to-keep-in-mind/index.html#fun-r-packages",
    "title": "Some R Packages to Keep In Mind",
    "section": "Fun R Packages",
    "text": "Fun R Packages\n\n\n\nFun package art\n\n\n\ncowsay creates a message accompanied by ASCII animal art. Also available from CRAN.\ndadjoke produces dad jokes with the groan() function.\nemokid outputs some emo messages when “you’re having trouble expressing how you feel about your broken code.”\nfortunes includes a collection of fortunes and wisdom from the R community. Available from CRAN.\nfun allows you to play minesweeper in R, among other things. Also available from CRAN.\nPlay Zork in R This blog post gives some code to load infocom games via Frotz and play them in the R console."
  },
  {
    "objectID": "posts/2018-04-11-spring-weather-in-minneapolis/2018-04-11-spring-weather-in-minneapolis.html",
    "href": "posts/2018-04-11-spring-weather-in-minneapolis/2018-04-11-spring-weather-in-minneapolis.html",
    "title": "Spring Weather in Minneapolis",
    "section": "",
    "text": "./assets/body-header.qmdIt feels like this spring has been especially terrible weather-wise. We have gotten a lot of snow and it has been cold. To evaluate whether this is the case or whether I have hindsight bias, I pulled some historical weather data for the month of April from Weather Underground.\n\nlibrary(dplyr)\nlibrary(forcats)\nlibrary(ggplot2)\nlibrary(ggridges)\nlibrary(readr)\nlibrary(viridis)\n\n# Read in data\napril = read_csv(\"https://raw.githubusercontent.com/zief0002/Public-Stuff/master/data/april-weather.csv\")\n\n# Filter dates\napril = april %&gt;%\n  filter(date &lt;= 11)\n\nI grabbed data back to 2008 (available at https://raw.githubusercontent.com/zief0002/Public-Stuff/master/data/april-weather.csv). To be fair about the comparisons I was making, I filtered the data so that I only had the first eleven days of April for each year represented in the data. (April is a month in Minnesota that has a lot of variation from the first to last day of the month.) I then counted the number of those days each year that it snowed, as well as, the number of days over 40℉, 50℉, and 60℉.\n\napril %&gt;%\n  group_by(year) %&gt;%\n  summarize(\n    days_with_snow = length(grep(pattern = \"Snow\", x = events)),\n    days_over_40 = sum(temp_high &gt; 40),\n    days_over_50 = sum(temp_high &gt; 50),\n    days_over_60 = sum(temp_high &gt; 60)\n  )\n\n# A tibble: 11 × 5\n    year days_with_snow days_over_40 days_over_50 days_over_60\n   &lt;dbl&gt;          &lt;int&gt;        &lt;int&gt;        &lt;int&gt;        &lt;int&gt;\n 1  2008              4            7            3            2\n 2  2009              3           10            4            0\n 3  2010              0           11           11            7\n 4  2011              2           11            9            4\n 5  2012              0           11            9            6\n 6  2013              4            7            2            0\n 7  2014              3            8            6            4\n 8  2015              2           11            6            3\n 9  2016              3            8            2            1\n10  2017              2           11           11            4\n11  2018              5            1            0            0\n\n\nThis suggested that my snow hypothesis (we have had more snow this year) may be erroneous. But, we have had a couple large snows; maybe we had more snow on the days we actually had snow. To evaluate this, I computed the total amount of precipitation for the years it actually snowed during the first eleven days of April.\n\n# Get the case numbers for the days that \"Snow\" is in the event column\nsnow_days = grep(pattern = \"Snow\", x = april$events)\n\n# Compute the total amount of precipitation on the days with snow\napril %&gt;%\n  filter(row_number() %in% snow_days) %&gt;%\n  group_by(year) %&gt;%\n  summarize(\n    total_snow = sum(precip, na.rm = TRUE)\n    )\n\n# A tibble: 9 × 2\n   year total_snow\n  &lt;dbl&gt;      &lt;dbl&gt;\n1  2008       0.96\n2  2009       0.47\n3  2011       0.08\n4  2013       1.39\n5  2014       0.81\n6  2015       0.71\n7  2016       0.06\n8  2017       0.18\n9  2018       0.82\n\n\nWhile more snow than the last couple years, 2018 doesn’t stand out as a banner April for the amount of snow. We will attribute this erroneous hypothesis to snow-blindness.\nHowever, my “April = 🌡” hypothesis seems spot on. In 2018, so far, we have only had one day where the daily high temperatur was over 40℉. In the 10 years previous, almost all of the first eleven days of April hit over 40℉. This is compounded by the fact that the one day it went over 40℉, the temperature rose to a balmy 41℉!\nLastly, I looked at the distribution of the average daily temperatures since 2008. This was inspired by a post I saw here and here.\n\n# Ridge plot of the distribution of average daily temperatures by year\nggplot(april, aes(x = temp_avg, y = fct_rev(factor(year)), fill = ..x..)) +\n  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01, gradient_lwd = 1.) +\n  scale_x_continuous(expand = c(0.01, 0)) +\n  scale_y_discrete(expand = c(0.01, 0)) +\n  scale_fill_viridis(\n    name = expression(paste('Temp. (',~degree,'F)',sep='')), \n    option = \"C\"\n  ) +\n  labs(\n    title = 'April Temperatures in Minneapolis',\n    subtitle = 'Mean temperatures for the first 11 days in April',\n    x = expression(paste('Average Daily Temperature (',~degree,'F)',sep=''))\n  ) +\n  theme_ridges(\n    font_size = 13, \n    grid = TRUE\n  ) + \n  theme(\n    axis.title.y = element_blank()\n  )\n\nWarning: The dot-dot notation (`..x..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(x)` instead.\n\n\n\n\n\nThis also suggests that we are experiencing a less than tropical April in Minneapolis."
  },
  {
    "objectID": "posts/2019-04-30-computer-brain/2019-04-30-computer-brain.html",
    "href": "posts/2019-04-30-computer-brain/2019-04-30-computer-brain.html",
    "title": "Computing in the ’60s",
    "section": "",
    "text": "./assets/body-header.qmdMy dad was a mathematics teacher at Sartell High School and was also really into computing and programming. I remember learning BASIC with him on our Apple IIe and also programming our TI-99/4A. (To save a program on the TI, we had to hook up an external tape recorder and write to a cassette tape. Then to run the program, you played the tape!)\n\n\n\nOld issues of Byte, Compute, Family Computing (check out the 1982 TV ad below), and other magazines and educational trades were stacked a mile-high in our living room; many providing the outlines of a program to create some graphical spectacle (e.g., a flashing Christmas tree or Jack-O-Lantern). Writing out the syntax to create these “masterpieces”, I began to learn how pixels on the computer screen could be manipulated to form images. I also experienced the anticipation of runnning the syntax only to have it fail because somewhere in the several 100 lines of code I just typed I missed a comma or semicolon, or mis-typed a digit…oh the drag of de-bugging.\n\n\n\n\nMy dad and Dan Brockton (the Sartell librarian) were early advocates for teaching programming in the high school setting. They were also instrumental in bringing early computers into the school and getting students interested in using them. Below is an article that appeared in the high school paper about one of those computers, the Programma 101.\n\n\n\nProgramma 101\n\n\nThere are several things that stand out from this article (not the least is the gendered use of language, although it was great to see at least one female in the picture).\n\nThe cost of that thing was $2070.\nIt prints at a rate of 30 characters per second.\nData was stored on a magnetic card.\nApparently at that time, that machine looked complicated."
  },
  {
    "objectID": "posts/2018-03-28-epsy-5271/2018-03-28-epsy-5271.html",
    "href": "posts/2018-03-28-epsy-5271/2018-03-28-epsy-5271.html",
    "title": "Q&A with Becoming a Teacher of Statistics Class",
    "section": "",
    "text": "./assets/body-header.qmd\nThis last Tuesday (03/27/2018) I was invited to go do a Q&A with the students in the EPsy 5271 Becoming a Teacher of Statistics course. This class also met via video-link with a similar course at Penn State. The students asked thoughful (sometimes difficult) questions and I tried to answer them. I asked them if I could blog out their questions and my responses and they kindy said “yes”. Rather than respond to all of them at once, I thought I would use this opportunity to create several blogposts. So without further ado, here we go.\n\nIs there a distinction between a statistics curriculum that is rich with computational experience, and a data science curriculum that includes statistical theory? Where do you think education in the fields of data science and statistics is headed, both separately and in relation to each other?\nThis is a tough question. My guess is yes. And the distinction is in the emphasis and the theoretical understandings. My other hypothesis, is that this distance between the two diminishes with experience beyond those received in your formal education. Let me give an example: Consider two recent graduates—one from a statistics program (but with many computational experiences) and the other from a CS program (but with many data/stat related experiences)— who both work in a DS company.\nBoth graduates are equipped to handle a DS job, but in different ways. For the stat major, computation is the tool and the problem is approached through their stat training (design, analysis). For the CS major it is likely the reverse. The statistical methods are the tool that inform their approach to tool building, or coding.\nIn 10 years time, while their approaches to the work may still be slightly different, the discrepancies between the “students” has likely all but vanished as they grow and experience things (skills that are required strengthen and those that aren’t weaken).\nFrom an educational perspective, it is unclear that one approach is better than the other. Although many people have opinions about this, there is not empirical evidence to suggest one approach is better (at least so far as I am aware). In looking at different undergraduate data science programs around the country, different institutions take different approaches to this. (Often it is a function of which department the faculty who started the program are based in rather than a well-planned out best-approach for students.)\n\n\nWhat strategies would you recommend for implementing student centered/cooperative learning in the setting of very large lecture sizes?\nThere are probably several strategies to make this work in a large class. For example, having students do a think-pair-share in which they (1) work for a minute individually, (2) pair with a fellow student to work, and then (3) share their work with another pair of student. I would also change the way in which TAs are utilized in those courses, for example making sure they are in all classes (lectures and labs) rather than just teaching the lab sections.\nJacobs and Inn (2003) wrote a book chapter, Using Cooperative Learning in Large Classes, that addresses this very question. Rhonda Magel published an article, Using Cooperative Learning in a Large Introductory Statistics Class, in the Journal of Statistics Education related to cooperative learning in statistics courses.\nMany universities have a Center for Teaching and Learning. This is a wonderful resource for instructors and their staff would probably welcome working with you to implement these types of pedagogies into your courses. Often they also have online resources. For example, the University of Waterloo’s Centre for Teaching Excellence has an online resource called Activities for Large Classes.\n\n\nI see that you are interested in Data Science and have been successfully funded to promote statistics education. Have you applied or been funded through NIH? Do you plan to be involved in the input on the Draft for Strategic Plan for Data Science?\nI have never personally been funded through NIH. We primarily seek funding through NSF. (BioSQuaRE was funded through the Howard Hughes Foundation.) To be honest, I hadn’t heard about this document until your question, so thank you for bringing it to my attention.\nAfter reading through the document, my take is that it seems to be addressing how data science can be used in research. My interest in data science is more on the education side of things. For example, I was on a committee through the National Academies of Science that was set up to study undergraduate data science. If you are interested, you can read our report, Envisioning the Data Science Discipline: The Undergraduate Perspective.\nWhile what happens in research and what is taught in the classroom are related, how the health sciences should approach the use of data science in their research is beyond my scope."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Andrew Zieffler",
    "section": "",
    "text": "Andrew Zieffler\nAcademic. Data lover. Statistics enthusiast.\n\n\n\n\n\n\nSelected Publications\n\\(^\\dagger\\) denotes student author\n\n\n\n2023\n\nLegacy, C.\\(^\\dagger\\), Zieffler, A., Baumer, B. S., Barr, V., & Horton, N. J. (2023). Facilitating team-based data science: Lessons learned from the DSC-WAV project. Foundations of Data Science, 5(2), 244–265. https://doi.org/10.3934/fods.2022003\nRao, V. N. V.\\(^\\dagger\\), Legacy, C., Zieffler, A., & delMas, R. (2023). A sequence of activities to build students’ reasoning about data and visualization. Teaching Statistics, 45(S1), S80–S92. http://doi.org/10.1111/test.12341\n\n\n\n\n2022\n\nLegacy, C.\\(^\\dagger\\), Zieffler, A., Fry, E., & Le, L. (2022). COMPUTES: An instrument to measure introductory statistics instructors’ emphasis on computational practices. Statistics Education Research Journal, 21(1). https://doi.org/10.52041/serj.v21i1.63\n\n\n\n\n2021\n\nHorton, N. J., Baumer, Benjamin S., Zieffler, A., & Barr, V. (2021). The Data Science Corps Wrangle-Analyze-Visualize program: Building data acumen for undergraduate students. Harvard Data Science Review, 3(1). https://doi.org/10.1162/99608f92.8233428d\nYu, C.-H., Mathiowetz, V. G., Zieffler, A., & Tomlin, G. S. (2021). Efficacy of a forearm rotation orthosis for persons with a hemiparetic arm. American Journal of Occupational Therapy, 75(6), 7506205110. https://doi.org/10.5014/ajot.2021.043455\nZieffler, A., Justice, N., delMas, R., & Huberty, M.\\(^\\dagger\\) (2021). The use of algorithmic models to develop secondary teachers’ understanding of the statistical modeling process. Journal of Statistics and Data Science Education, 29(1), 131–147. https://doi.org/10.1080/26939169.2021.1900759\n\n\n\n\n2018\n\nJustice, N., Zieffler, A., Huberty, M.\\(^\\dagger\\), & delMas, R. (2018). Every rose has it’s thorn: Secondary teachers’ reasoning about statistical models. ZDM—The International Journal on Mathematics Education, 50(7), 1253–1265. https://doi.org/10.1007/s11858-018-0953-1\nNational Academies of Sciences, Engineering, and Medicine. (2018). [Contributing Author]. Data science for undergraduates: Opportunities and options. Washington, DC: The National Academies Press. https://doi.org/10.17226/25104\nSabbag, A. G.\\(^\\dagger\\), Garfield, J., & Zieffler, A. (2018). Assessing statistical literacy and statistical reasoning: The REALI instrument. Statistics Education Research Journal, 17(2), 141–160. https://doi.org/10.52041/serj.v17i2.163\n\n\n\n\n2017\n\nGarfield, J., Zieffler, A., & Fry, E.\\(^\\dagger\\) (2017). What is statistics education? In D. Ben-Zvi, K. Makar, & J. Garfield (Eds.), The international handbook of research in statistics education (pp. 37–70). Cham, Switzerland: Springer International Publishing. https://doi.org/10.1007/978-3-319-66195-7\nJustice, N.\\(^\\dagger\\), Zieffler, A., & Garfield, J. (2017). Statistics graduate teaching assistants’ beliefs, practices, and preparation for teaching introductory statistics. Statistics Education Research Journal, 16(1), 294–319. https://doi.org/10.52041/serj.v16i1.232\nStanhope, E., Ziegler, L., Haque, T., Le, L., Vinces, M., Davis, G. K., Zieffler, A., Brodfuehrer, P., Preest, M., Belitsky, J., Umbanhowar, Jr., C., & Overvoorde, P. J. (2017). Development of a Biological Science Quantitative Reasoning Exam (BioSQuaRE). CBE–Life Sciences Education, 16(4), ar66. https://doi.org/10.1187/cbe.16-10-0301\n\n\n\n\n2015\n\nSabbag, A. G.\\(^\\dagger\\), & Zieffler, A. (2015). Assessing learning outcomes: An analysis of the GOALS-2 instrument. Statistics Education Research Journal, 14(2), 93–116. https://doi.org/10.52041/serj.v14i2.263\nUmbanhowar Jr., C, Belitsky, J. M., Brodfuehrer, P., Davis, G., Haque, T., Le, L.\\(^\\dagger\\), McFadden, C., Overvoorde, P., Preest, M., Stanhope, L., Vinces, M., Zieffler, A., & Ziegler, L.\\(^\\dagger\\) (2015). Understanding the quantitative and computational skills of incoming biology students. https://qubeshub.org/resources/806\nZieffler, A., & Fry, E. (eds.) (2015). Reasoning about uncertainty: Learning and teaching informal inferential reasoning. Minneapolis, MN: Catalyst Press.\nZieffler, A., & Huberty, M.\\(^\\dagger\\) (2015). A catalyst for change in the high school math curriculum. CHANCE, 28(3), 44–49. https://doi.org/10.1080/09332480.2015.1099365"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Andrew Zieffler",
    "section": "",
    "text": "Andrew Zieffler\nAcademic. Data lover. Statistics enthusiast.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraduation 2022 and New Students\n\n\n\nMiscellaneous\n\n\nStatistics Education\n\n\n\n\n\n\n\nAndy\n\n\nNov 21, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nA Couple Interesting Reads\n\n\n\nMiscellaneous\n\n\nScience\n\n\n\nThree fun reads: Americans Need to Get Over Their Fear of Math, Structured Procrastination, and The Devil Teaches Thermodynamics\n\n\n\nAndy\n\n\nApr 27, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nMission Statements\n\n\n\nMiscellaneous\n\n\n\nSome thoughts on mission staements\n\n\n\nAndy\n\n\nApr 26, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nBooks I Bought\n\n\n\nMiscellaneous\n\n\n\n\n\n\n\nAndy\n\n\nAug 31, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nTrivia Weekend\n\n\n\nMiscellaneous\n\n\n\n\n\n\n\nAndy\n\n\nFeb 15, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nSome TV Updates\n\n\n\nMiscellaneous\n\n\n\n\n\n\n\nAndy\n\n\nFeb 5, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nPackages and Resources for Data Viz\n\n\n\nnotes\n\n\nR\n\n\nTeaching\n\n\n\n\n\n\n\nAndy\n\n\nDec 16, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSome R Packages to Keep In Mind\n\n\n\nnotes\n\n\nR\n\n\nProductivity\n\n\n\n\n\n\n\nAndy\n\n\nDec 15, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nJournal of Statistics and Data Science Education (JSE) Reference Examples\n\n\n\nWriting\n\n\nStatistics Education\n\n\n\n\n\n\n\nAndy\n\n\nSep 14, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nA History of R\n\n\n\nR\n\n\nPresentation\n\n\n\n\n\n\n\nAndy\n\n\nJul 22, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSpelling Lesson\n\n\n\nMiscellaneous\n\n\n\n\n\n\n\nAndy\n\n\nJul 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nFour Sheets to the Wind And a One Way Ticket to France\n\n\n\nMiscellaneous\n\n\n\n\n\n\n\nAndy\n\n\nJun 30, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to My Office\n\n\n\nMiscellaneous\n\n\n\n\n\n\n\nAndy\n\n\nMar 26, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nBook Publishing in Academia\n\n\n\nMiscellaneous\n\n\nWriting\n\n\n\n\n\n\n\nAndy\n\n\nMar 1, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nBetter Research Poster\n\n\n\nResearch\n\n\nScience\n\n\nMiscellaneous\n\n\n\n\n\n\n\nAndy\n\n\nFeb 27, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nSome Plots I Made…\n\n\n\nMiscellaneous\n\n\n\n\n\n\n\nAndy\n\n\nFeb 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nAPA Style Table in RMarkdown (HTML Version)\n\n\n\nMarkdown\n\n\nR\n\n\nWriting\n\n\n\n\n\n\n\n\nAndy\n\n\nFeb 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nAPA Tables using RMarkdown: Part 5\n\n\n\nMarkdown\n\n\nR\n\n\nWriting\n\n\n\n\n\n\n\n\nAndy\n\n\nJan 18, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nAPA Tables using RMarkdown: Part 4\n\n\n\nMarkdown\n\n\nR\n\n\nWriting\n\n\n\n\n\n\n\n\nAndy\n\n\nJan 13, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nAPA Tables using RMarkdown: Part 3\n\n\n\nMarkdown\n\n\nR\n\n\nWriting\n\n\n\n\n\n\n\n\nAndy\n\n\nJan 12, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nAPA Tables using RMarkdown: Part 2\n\n\n\nMarkdown\n\n\nR\n\n\nWriting\n\n\n\n\n\n\n\n\nAndy\n\n\nJan 11, 2020\n\n\n\n\n\n\n\n\n\n\n\n\n\nAPA Tables Using RMarkdown\n\n\n\nMarkdown\n\n\nR\n\n\nWriting\n\n\n\n\n\n\n\n\nAndy\n\n\nJan 9, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nThesis Season\n\n\n\nResearch\n\n\nWriting\n\n\nMiscellaneous\n\n\n\n\n\n\n\nAndy\n\n\nDec 12, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nHelping the Earth\n\n\n\nMiscellaneous\n\n\n\n\n\n\n\nAndy\n\n\nDec 11, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nTeaching Statistics Reading/Discussion Group\n\n\n\nCATALST\n\n\nTeaching\n\n\nStatistics Education\n\n\n\n\n\n\n\nAndy\n\n\nDec 9, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nReproducibiliTEA Journal Club\n\n\n\nScience\n\n\nResearch\n\n\n\n\n\n\n\nAndy\n\n\nDec 6, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nTwo Online Articles\n\n\n\nComputing\n\n\nMiscellaneous\n\n\n\n\n\n\n\nAndy\n\n\nNov 24, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nTelevision\n\n\n\nMiscellaneous\n\n\nR\n\n\n\n\n\n\n\nAndy\n\n\nOct 22, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nIllustrating the Gender Gap in Fiction\n\n\n\nMiscellaneous\n\n\n\n\n\n\n\nAndy\n\n\nOct 10, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nDissertation Writing\n\n\n\nQuotation\n\n\nWriting\n\n\n\n\n\n\n\nAndy\n\n\nJul 24, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nWhat to do about p-values?\n\n\n\nTeaching\n\n\nStatistics Education\n\n\nR\n\n\n\n\n\n\n\nAndy\n\n\nJun 26, 2019\n\n\n\n\n\n\n\n\n\n\n\n\n2019 StatPREP Workshops\n\n\n\nComputing\n\n\nR\n\n\nStatistics Education\n\n\n\n\n\n\n\nAndy\n\n\nJun 17, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nHigher Education in Minnesota\n\n\n\nR\n\n\nMiscellaneous\n\n\n\n\n\n\n\nAndy\n\n\nJun 5, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nRichard Hamming on the Teaching of Mathematics\n\n\n\nQuotation\n\n\n\n\n\n\n\nAndy\n\n\nJun 3, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nChange: Time and Effort\n\n\n\nMiscellaneous\n\n\n\n\n\n\n\nAndy\n\n\nMay 30, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nUSCOTS 2019\n\n\n\nCATALST\n\n\nStatistics Education\n\n\nTeaching\n\n\n\n\n\n\n\nAndy\n\n\nMay 19, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nCongratulations Ethan!\n\n\n\nQME\n\n\nStatistics Education\n\n\n\n\n\n\n\nAndy\n\n\nMay 9, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nStudent Awards! So Proud!\n\n\n\nStatistics Education\n\n\nQME\n\n\n\n\n\n\n\nAndy\n\n\nMay 2, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nCV: Sale Items vs. Research Plan\n\n\n\nCV\n\n\nQuotation\n\n\n\n\n\n\n\n~\n\n\nMay 2, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nCV Tips\n\n\n\nCV\n\n\nQME\n\n\n\n\n\n\n\nAndy\n\n\nMay 1, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nComputing in the ’60s\n\n\n\n\n\n\n~\n\n\nApr 30, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nDeprecating Statistical Significance: Toward Better Science\n\n\n\nScience\n\n\nHistory\n\n\nPresentations\n\n\n\n\n\n\n\nAndy\n\n\nApr 26, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nInbox Zero\n\n\n\nMiscellaneous\n\n\nProductivity\n\n\n\n\n\n\n\nAndy\n\n\nAug 27, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nComputing Talk at SSC 2018\n\n\n\nR\n\n\nTeaching\n\n\nComputing\n\n\nPresentations\n\n\n\n\n\n\n\nAndy\n\n\nJun 3, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nR Markdown Theme Gallery\n\n\n\nR\n\n\nMarkdown\n\n\n\n\n\n\n\nAndy\n\n\nApr 12, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nSpring Weather in Minneapolis\n\n\n\nR\n\n\n\n\n\n\n\nAndy\n\n\nApr 11, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nQ&A with Becoming a Teacher of Statistics Class: Part IV\n\n\n\nTeaching\n\n\nFAQ\n\n\n\n\n\n\n\nAndy\n\n\nApr 9, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nStatistics Education Research Seminar: Teaching Statistics from a Modeling Perspective\n\n\n\nTeaching\n\n\nResearch\n\n\n\n\n\n\n\nAndy\n\n\nApr 6, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nQ&A with Becoming a Teacher of Statistics Class: Part III\n\n\n\n\n\n\nAndy\n\n\nApr 2, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nQ&A with Becoming a Teacher of Statistics Class: Part II\n\n\n\nTeaching\n\n\nFAQ\n\n\n\n\n\n\n\nAndy\n\n\nMar 30, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nQ&A with Becoming a Teacher of Statistics Class\n\n\n\nTeaching\n\n\nFAQ\n\n\n\n\n\n\n\nAndy\n\n\nMar 28, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nMinnesota State High School Boys Hockey Predictions (Post Hoc Analysis)\n\n\n\nHockey\n\n\nElo\n\n\nR\n\n\n\n\n\n\n\nAndy\n\n\nMar 15, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nMinnesota State High School Boys Hockey Predictions (Updated Quarterfinals)\n\n\n\n\n\n\n~\n\n\nMar 8, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nMinnesota State High School Boys Hockey Predictions\n\n\n\nHockey\n\n\nElo\n\n\nR\n\n\n\n\n\n\n\nAndy\n\n\nMar 3, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nCollege in the Schools\n\n\nRead about the CIS Statistics program that I oversee.\n\n\n\nAndy\n\n\nFeb 28, 2018\n\n\n\n\n\n\n\n\nNo matching items"
  }
]